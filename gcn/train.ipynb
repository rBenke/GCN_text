{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inits.py'; 'inits' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-abdecc0db610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/repo/GCN_text/gcn/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/repo/GCN_text/gcn/layers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'inits.py'; 'inits' is not a package"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.py import GCN\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get paths to available grahs metadata\n",
    "N_sample = 200000\n",
    "adj_mat, features_mat, train_labels_mat, test_labels_mat, val_labels_mat, train_mask_mat, test_mask_mat, val_mask_mat = load_data(\"txt_graph2216_21012020\", N_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465542, 465542)\n",
      "(465542, 100)\n",
      "(465542, 5)\n",
      "(465542, 5)\n",
      "(465542, 5)\n",
      "(465542, 5)\n",
      "(465542, 5)\n",
      "(465542, 5)\n",
      "number of masked elements\n",
      "train: 779\n",
      "test: 668\n",
      "val: 778\n",
      "\n",
      " positions of ones in masked labels\n",
      "[2 1 0 4 0 4 4 1 3 0 3 3 0 4 2 0 0 1 4 1 1 3 0 2 0 1 1 0 4 0 4 3 3 2 2 0 2\n",
      " 2 4 4 1 3 2 2 0 4 4 4 3 1 2 0 3 1 2 0 1 3 4 0 3 3 3 4 0 1 4 1 3 4 3 1 1 1\n",
      " 3 0 0 4 3 2 1 0 0 0 2 0 4 3 0 0 4 4 3 2 3 3 1 4 3 4 1 0 3 2 4 3 4 4 0 1 4\n",
      " 3 1 0 1 3 3 4 4 3 3 3 4 3 2 3 1 2 3 1 3 0 2 0 3 2 4 2 0 2 0 4 2 0 2 1 0 2\n",
      " 0 0 3 0 1 2 3 3 3 1 4 3 2 4 3 3 0 4 3 1 3 1 3 0 4 2 4 2 0 3 2 3 2 2 2 0 2\n",
      " 2 4 0 2 3 0 2 0 0 0 2 0 0 2 2 2 3 2 0 3 1 4 3 0 0 4 1 2 2 4 0 2 3 0 4 2 3\n",
      " 0 0 2 1 0 0 3 3 3 1 2 4 0 3 0 3 3 1 4 3 2 4 2 3 4 0 3 4 0 3 3 4 0 0 0 3 3\n",
      " 0 2 1 4 4 3 0 0 4 4 1 1 4 3 0 3 4 3 2 3 2 1 4 3 4 0 3 0 4 4 0 4 1 2 2 3 3\n",
      " 3 0 2 4 3 3 2 3 4 2 1 0 2 4 4 1 4 3 4 3 1 3 3 1 3 2 0 4 1 0 2 3 0 4 4 4 1\n",
      " 3 3 4 1 0 4 2 3 3 0 0 1 2 3 1 0 4 0 2 3 1 1 0 2 3 3 1 0 4 3 3 2 4 0 0 1 1\n",
      " 0 0 0 2 0 0 3 3 4 3 1 1 0 4 2 1 0 0 3 1 4 1 3 0 3 1 2 4 3 3 4 1 4 2 0 2 1\n",
      " 4 3 3 2 4 0 0 1 1 2 0 2 2 0 3 0 3 3 2 0 1 4 2 0 2 1 3 0 4 0 0 0 4 3 0 2 0\n",
      " 2 3 0 3 4 4 1 0 1 4 1 2 3 4 3 0 1 2 3 4 2 4 3 2 2 0 4 2 0 3 4 3 3 3 2 4 2\n",
      " 1 4 4 0 3 2 2 1 0 1 4 3 2 1 1 3 0 2 2 0 2 3 2 3 1 0 1 3 1 4 0 0 4 4 0 0 0\n",
      " 0 2 3 3 1 1 2 0 0 3 2 4 4 2 3 1 0 2 0 3 2 4 2 1 0 3 4 1 1 4 2 3 1 3 3 1 4\n",
      " 1 3 4 2 2 2 2 3 0 2 3 3 4 0 4 0 2 4 0 4 0 0 3 4 0 4 0 3 4 3 3 1 4 0 4 4 3\n",
      " 3 3 3 1 1 2 0 2 3 3 2 3 3 4 1 0 2 3 0 1 4 1 0 1 2 0 2 1 1 0 4 3 4 3 4 3 3\n",
      " 0 2 0 0 1 1 3 1 1 0 1 1 3 3 0 3 1 2 3 4 2 4 4 4 3 3 0 3 0 3 4 4 2 0 4 3 3\n",
      " 0 3 1 0 1 0 0 0 4 0 3 3 2 4 3 3 4 1 0 0 0 0 0 3 2 3 4 0 3 3 0 4 1 2 1 0 2\n",
      " 1 1 0 0 2 3 2 1 3 3 3 0 1 3 0 0 2 3 2 1 0 4 3 0 2 1 4 0 4 3 0 4 4 2 0 4 4\n",
      " 0 3 2 2 0 1 4 4 2 4 2 0 3 3 0 3 1 2 4 0 4 1 2 2 2 0 2 0 0 4 4 4 2 3 1 1 4\n",
      " 1 1]\n",
      "[0 2 3 4 1 3 4 4 0 4 2 0 4 2 1 3 3 0 0 3 2 3 2 2 4 3 0 0 2 0 0 3 3 3 3 2 3\n",
      " 2 2 1 2 4 4 2 0 3 2 4 0 3 3 4 2 3 4 3 3 4 2 2 4 3 4 0 3 4 2 4 2 3 0 4 2 2\n",
      " 3 0 1 1 1 3 0 1 0 0 0 3 3 0 2 4 4 2 3 1 3 3 2 0 1 0 2 3 0 0 2 1 2 3 1 1 4\n",
      " 4 0 0 0 1 2 3 0 0 4 1 3 3 1 1 3 2 3 2 0 3 4 1 0 4 3 0 1 4 2 3 2 1 2 3 1 3\n",
      " 0 0 3 2 1 3 0 3 2 3 3 2 1 4 1 3 4 1 0 4 1 2 4 4 4 1 0 1 1 0 0 4 2 3 1 0 2\n",
      " 3 0 1 3 0 1 4 4 0 3 1 2 2 4 1 4 3 1 4 0 2 1 3 2 4 0 0 3 4 0 3 0 0 4 3 3 1\n",
      " 2 4 1 3 4 4 2 3 0 3 3 1 2 1 3 1 4 2 2 2 4 1 1 2 2 3 3 3 3 0 2 2 3 2 1 1 3\n",
      " 2 4 1 1 3 3 0 2 4 1 2 1 3 1 1 0 3 1 0 1 1 3 2 0 3 1 0 3 4 1 1 4 3 0 3 2 2\n",
      " 2 2 4 0 0 2 4 3 2 4 4 1 2 0 3 1 2 3 4 1 1 3 4 4 3 2 3 0 0 3 4 0 1 3 2 4 4\n",
      " 1 0 1 1 0 2 2 2 2 4 3 2 3 2 3 3 4 0 3 2 1 0 4 2 0 0 3 0 0 3 3 2 2 0 1 1 3\n",
      " 0 4 0 4 1 0 4 0 1 1 0 0 3 1 3 4 4 3 1 0 3 0 3 1 3 3 3 1 0 3 4 2 4 3 1 2 0\n",
      " 2 3 3 4 3 0 1 1 2 3 3 4 1 3 1 1 3 1 4 0 1 1 2 4 0 3 1 0 1 2 1 1 0 1 1 3 3\n",
      " 2 4 1 2 2 4 4 0 0 2 3 3 2 3 2 3 4 2 0 2 3 0 3 1 2 2 1 3 4 4 3 1 3 2 1 1 2\n",
      " 4 0 4 3 1 3 0 4 0 4 4 0 3 4 0 2 0 1 3 1 4 4 2 3 1 0 3 4 2 2 4 4 4 0 4 3 1\n",
      " 1 4 2 0 2 1 2 3 3 4 2 0 0 0 3 2 2 1 1 0 3 2 2 0 1 3 2 4 2 4 4 3 2 0 1 1 4\n",
      " 3 3 4 3 2 0 0 0 1 4 3 2 2 0 4 1 3 1 0 3 1 2 2 1 3 1 3 4 0 4 4 1 3 3 2 0 2\n",
      " 2 2 0 1 0 0 4 3 4 3 0 0 3 2 3 4 4 3 2 0 4 1 0 0 2 3 2 1 2 4 2 3 0 2 2 3 2\n",
      " 3 4 3 1 0 2 1 0 3 2 2 0 0 3 3 0 3 2 1 2 1 4 2 1 2 1 2 3 3 3 3 2 3 0 0 0 1\n",
      " 2 2]\n",
      "[3 0 2 2 4 3 1 2 2 3 0 2 4 2 4 4 0 0 3 3 0 0 1 4 4 1 2 1 2 3 1 0 4 4 2 2 3\n",
      " 2 1 0 1 1 3 4 0 1 0 2 1 2 3 4 0 0 1 4 1 0 4 2 0 3 3 1 3 1 3 4 0 3 3 0 1 3\n",
      " 3 2 3 3 2 1 2 0 0 1 1 1 3 0 3 3 3 0 3 1 1 1 4 4 3 4 1 0 0 3 2 4 4 0 4 3 2\n",
      " 0 3 2 0 2 2 3 1 4 4 4 4 0 3 3 0 4 2 4 1 4 4 4 2 3 2 1 4 0 1 4 3 0 1 1 1 0\n",
      " 4 0 2 2 3 1 1 3 0 1 0 2 0 2 2 3 2 3 2 0 0 1 2 2 2 0 0 0 0 2 3 3 1 0 0 4 0\n",
      " 4 4 0 0 4 1 1 0 1 3 4 0 0 3 4 3 3 0 4 1 4 0 0 3 2 1 2 3 3 3 3 3 1 0 3 2 0\n",
      " 3 0 1 1 4 0 0 3 0 3 0 0 1 4 4 1 4 2 0 2 4 4 1 3 0 4 2 4 0 0 1 4 4 4 0 0 0\n",
      " 2 0 0 4 4 4 3 1 2 0 2 3 0 1 1 1 2 3 3 0 0 1 3 1 4 4 3 0 0 0 2 4 3 3 1 2 0\n",
      " 4 3 1 3 0 3 3 0 4 1 4 1 1 2 0 0 4 0 0 0 4 2 1 1 1 0 2 0 1 0 4 0 3 2 1 3 0\n",
      " 4 2 3 0 0 0 3 3 3 3 4 4 1 1 0 0 0 0 4 4 2 3 1 2 0 1 1 2 2 0 1 1 0 3 3 1 1\n",
      " 0 3 4 4 3 0 0 2 1 1 3 0 0 2 1 0 0 4 4 3 3 2 0 2 2 3 3 4 2 4 4 3 4 3 3 4 3\n",
      " 2 1 1 3 4 1 1 2 1 3 2 2 0 0 2 2 0 4 1 0 4 1 2 2 3 4 3 1 3 4 3 2 1 1 1 1 3\n",
      " 0 0 3 0 0 0 3 0 0 2 2 3 0 4 1 2 2 3 3 3 2 1 3 0 0 3 0 2 0 1 0 0 4 3 4 4 4\n",
      " 0 1 2 2 4 2 3 2 4 0 2 0 1 3 3 4 0 2 1 2 0 2 1 4 0 3 4 2 4 4 0 0 1 0 0 3 2\n",
      " 0 4 4 4 3 3 2 1 2 1 1 1 0 4 2 1 3 0 1 4 1 0 3 2 4 1 2 3 1 3 0 0 2 0 3 4 4\n",
      " 1 3 3 1 0 3 3 4 0 3 3 4 0 2 0 1 0 0 1 4 0 2 2 3 2 2 1 4 0 4 3 1 3 0 0 3 4\n",
      " 4 4 2 2 2 1 0 1 3 3 2 4 0 0 3 3 0 4 4 2 0 2 0 2 2 3 2 0 0 4 4 2 0 0 2 3 2\n",
      " 1 0 2 0 4 2 3 0 2 4 2 1 1 0 0 2 0 1 4 4 4 0 3 1 4 1 4 1 2 2 1 3 1 1 1 3 1\n",
      " 0 2 0 3 2 1 0 0 2 0 3 0 3 1 1 0 3 3 1 4 0 0 3 2 1 2 3 1 1 2 3 4 0 3 0 3 3\n",
      " 2 1 2 0 4 3 0 3 4 2 2 0 0 3 1 0 4 4 2 0 1 4 4 1 4 4 3 4 0 3 0 0 2 2 1 1 4\n",
      " 4 2 3 4 2 3 2 4 3 3 3 0 1 0 3 2 0 4 0 0 1 1 4 0 4 0 1 2 0 1 2 4 2 0 2 0 3\n",
      " 2]\n",
      "\n",
      " ones outside the mask\n",
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(adj_mat.shape)\n",
    "print(features_mat.shape)\n",
    "print(train_labels_mat.shape)\n",
    "print(test_labels_mat.shape)\n",
    "print(val_labels_mat.shape)\n",
    "print(train_mask_mat.shape)\n",
    "print(test_mask_mat.shape)\n",
    "print(val_mask_mat.shape)\n",
    "\n",
    "print(\"number of masked elements\")\n",
    "print(\"train: \" + str(len(np.where(train_mask_mat[:,1]==1)[0])))\n",
    "print(\"test: \" + str(len(np.where(test_mask_mat[:,1]==1)[0])))\n",
    "print(\"val: \" + str(len(np.where(val_mask_mat[:,1]==1)[0])))\n",
    "\n",
    "print(\"\\n positions of ones in masked labels\")\n",
    "print(np.where(train_labels_mat[np.array(train_mask_mat[:,1],dtype=bool)]==1)[1])\n",
    "print(np.where(test_labels_mat[np.array(test_mask_mat[:,1],dtype=bool)]==1)[1])\n",
    "print(np.where(val_labels_mat[np.array(val_mask_mat[:,1],dtype=bool)]==1)[1])\n",
    "\n",
    "print(\"\\n ones outside the mask\")\n",
    "print(np.where(train_labels_mat[np.logical_not(np.array(train_mask_mat[:,1],dtype=bool))]==1))\n",
    "print(np.where(test_labels_mat[np.logical_not(np.array(test_mask_mat[:,1],dtype=bool))]==1))\n",
    "print(np.where(val_labels_mat[np.logical_not(np.array(val_mask_mat[:,1],dtype=bool))]==1))\n",
    "\n",
    "#sum_global_nodes = np.sum(adj_mat.toarray()[:,np.array(train_mask_mat[:,1],dtype=bool)], axis = 1) + np.sum(adj_mat.toarray()[:,np.array(test_mask_mat[:,1],dtype=bool)], axis = 1) + np.sum(adj_mat.toarray()[:,np.array(val_mask_mat[:,1],dtype=bool)], axis = 1)\n",
    "#print(adj_mat.shape[1] - sum(sum_global_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robertb/python/GCN_text_classif/gcn/utils.py:57: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    }
   ],
   "source": [
    "#delete all flags before declaration new one\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('mode', '', 'kernel') # for line by line mode\n",
    "tf.app.flags.DEFINE_string('port', '', 'kernel') # for line by line mode\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel') # for jupyter notebook\n",
    "\n",
    "flags.DEFINE_float('learning_rate', 0.03, 'Initial learning rate.') #0.01\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.') #200\n",
    "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.') #16\n",
    "flags.DEFINE_float('dropout', 0.6, 'Dropout rate (1 - keep probability).') #0.5\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 20, 'Tolerance for early stopping (# of epochs).') #10\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features_mat)\n",
    "support = [preprocess_adj(adj_mat)]\n",
    "num_supports = 1\n",
    "model_func = GCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/robertb/python/GCN_text_classif/gcn/models.py:95: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/python/GCN_text_classif/gcn/models.py:40: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/inits.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/layers.py:82: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/layers.py:26: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/layers.py:33: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/layers.py:170: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/robertb/python/GCN_text_classif/gcn/models.py:51: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/python/GCN_text_classif/gcn/models.py:51: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, train_labels_mat.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 30.33150 train_acc= 0.22721 val_loss= 39.41757 val_acc= 0.72237 time= 10.71388\n",
      "Epoch: 0002 train_loss= 16.81936 train_acc= 0.20924 val_loss= 37.68326 val_acc= 0.70823 time= 10.01661\n",
      "Epoch: 0003 train_loss= 26.68737 train_acc= 0.22593 val_loss= 36.66272 val_acc= 0.69280 time= 10.01657\n",
      "Epoch: 0004 train_loss= 28.54598 train_acc= 0.21823 val_loss= 35.95023 val_acc= 0.66838 time= 10.07508\n",
      "Epoch: 0005 train_loss= 20.57083 train_acc= 0.22721 val_loss= 35.04095 val_acc= 0.64910 time= 10.03505\n",
      "Epoch: 0006 train_loss= 23.62959 train_acc= 0.21566 val_loss= 34.01050 val_acc= 0.62982 time= 10.04319\n",
      "Epoch: 0007 train_loss= 25.53231 train_acc= 0.19384 val_loss= 32.79931 val_acc= 0.62725 time= 10.14809\n",
      "Epoch: 0008 train_loss= 23.18822 train_acc= 0.23363 val_loss= 31.68569 val_acc= 0.61954 time= 9.98431\n",
      "Epoch: 0009 train_loss= 21.49719 train_acc= 0.21053 val_loss= 30.61210 val_acc= 0.61440 time= 10.07334\n",
      "Epoch: 0010 train_loss= 33.64248 train_acc= 0.19384 val_loss= 29.52277 val_acc= 0.60797 time= 10.03616\n",
      "Epoch: 0011 train_loss= 23.22820 train_acc= 0.21694 val_loss= 28.42147 val_acc= 0.59769 time= 10.01634\n",
      "Epoch: 0012 train_loss= 22.84588 train_acc= 0.21438 val_loss= 27.26761 val_acc= 0.59126 time= 10.00059\n",
      "Epoch: 0013 train_loss= 28.15431 train_acc= 0.23877 val_loss= 26.04314 val_acc= 0.59126 time= 10.09325\n",
      "Epoch: 0014 train_loss= 23.95174 train_acc= 0.19641 val_loss= 24.88573 val_acc= 0.58869 time= 10.00339\n",
      "Epoch: 0015 train_loss= 19.33304 train_acc= 0.20026 val_loss= 23.81691 val_acc= 0.58226 time= 10.01113\n",
      "Epoch: 0016 train_loss= 23.35598 train_acc= 0.20924 val_loss= 22.91148 val_acc= 0.57326 time= 10.04036\n",
      "Epoch: 0017 train_loss= 21.56052 train_acc= 0.22336 val_loss= 23.14679 val_acc= 0.54884 time= 10.03529\n",
      "Epoch: 0018 train_loss= 25.67305 train_acc= 0.19641 val_loss= 23.50206 val_acc= 0.54242 time= 10.02281\n",
      "Epoch: 0019 train_loss= 25.09290 train_acc= 0.22593 val_loss= 23.89862 val_acc= 0.53470 time= 10.00485\n",
      "Epoch: 0020 train_loss= 27.39264 train_acc= 0.19641 val_loss= 24.37520 val_acc= 0.51542 time= 10.07591\n",
      "Epoch: 0021 train_loss= 19.82828 train_acc= 0.23363 val_loss= 24.81748 val_acc= 0.50129 time= 10.06912\n",
      "Epoch: 0022 train_loss= 24.76236 train_acc= 0.20668 val_loss= 25.23985 val_acc= 0.49486 time= 10.04222\n",
      "Epoch: 0023 train_loss= 20.91928 train_acc= 0.20924 val_loss= 25.52662 val_acc= 0.48329 time= 10.52230\n",
      "Epoch: 0024 train_loss= 24.69373 train_acc= 0.20668 val_loss= 25.85886 val_acc= 0.47686 time= 10.31705\n",
      "Epoch: 0025 train_loss= 19.53985 train_acc= 0.20539 val_loss= 26.13939 val_acc= 0.47429 time= 10.14904\n",
      "Epoch: 0026 train_loss= 18.48641 train_acc= 0.18742 val_loss= 26.42439 val_acc= 0.47044 time= 10.14827\n",
      "Epoch: 0027 train_loss= 15.10655 train_acc= 0.20282 val_loss= 26.62718 val_acc= 0.46530 time= 10.03803\n",
      "Epoch: 0028 train_loss= 19.13650 train_acc= 0.21438 val_loss= 26.81292 val_acc= 0.46144 time= 10.03286\n",
      "Epoch: 0029 train_loss= 20.53760 train_acc= 0.20924 val_loss= 27.09163 val_acc= 0.46272 time= 10.05236\n",
      "Epoch: 0030 train_loss= 16.88383 train_acc= 0.21438 val_loss= 27.36971 val_acc= 0.46144 time= 10.02645\n",
      "Epoch: 0031 train_loss= 18.97995 train_acc= 0.20154 val_loss= 27.59387 val_acc= 0.45758 time= 10.00752\n",
      "Epoch: 0032 train_loss= 26.49120 train_acc= 0.21053 val_loss= 27.97974 val_acc= 0.46144 time= 10.09838\n",
      "Epoch: 0033 train_loss= 16.85279 train_acc= 0.21566 val_loss= 28.28302 val_acc= 0.45630 time= 10.05917\n",
      "Epoch: 0034 train_loss= 16.06248 train_acc= 0.23107 val_loss= 28.61886 val_acc= 0.45630 time= 10.03841\n",
      "Epoch: 0035 train_loss= 25.61025 train_acc= 0.22721 val_loss= 29.02945 val_acc= 0.45887 time= 10.03393\n",
      "Epoch: 0036 train_loss= 18.96479 train_acc= 0.23235 val_loss= 29.44853 val_acc= 0.46272 time= 9.99450\n",
      "Epoch: 0037 train_loss= 21.08729 train_acc= 0.21566 val_loss= 29.87613 val_acc= 0.46401 time= 10.04864\n",
      "Epoch: 0038 train_loss= 19.66954 train_acc= 0.23363 val_loss= 30.27986 val_acc= 0.46401 time= 10.14446\n",
      "Epoch: 0039 train_loss= 15.63927 train_acc= 0.20282 val_loss= 30.65070 val_acc= 0.46530 time= 10.00677\n",
      "Epoch: 0040 train_loss= 23.74373 train_acc= 0.20539 val_loss= 30.95825 val_acc= 0.46658 time= 10.10820\n",
      "Epoch: 0041 train_loss= 22.06090 train_acc= 0.20796 val_loss= 31.24516 val_acc= 0.47172 time= 10.04138\n",
      "Epoch: 0042 train_loss= 20.84033 train_acc= 0.19897 val_loss= 31.50195 val_acc= 0.46915 time= 10.05857\n",
      "Epoch: 0043 train_loss= 20.00509 train_acc= 0.22208 val_loss= 31.72740 val_acc= 0.46915 time= 10.00118\n",
      "Epoch: 0044 train_loss= 21.09463 train_acc= 0.20539 val_loss= 31.95000 val_acc= 0.46787 time= 10.04361\n",
      "Epoch: 0045 train_loss= 13.06258 train_acc= 0.23877 val_loss= 32.24564 val_acc= 0.46787 time= 10.14852\n",
      "Epoch: 0046 train_loss= 16.39295 train_acc= 0.19384 val_loss= 32.44986 val_acc= 0.46272 time= 10.02312\n",
      "Epoch: 0047 train_loss= 20.66700 train_acc= 0.22593 val_loss= 32.61364 val_acc= 0.45887 time= 10.04543\n",
      "Epoch: 0048 train_loss= 26.06840 train_acc= 0.22208 val_loss= 32.74863 val_acc= 0.45244 time= 9.99281\n",
      "Epoch: 0049 train_loss= 21.36515 train_acc= 0.22978 val_loss= 32.78788 val_acc= 0.43959 time= 10.00398\n",
      "Epoch: 0050 train_loss= 25.37960 train_acc= 0.19512 val_loss= 32.87888 val_acc= 0.43316 time= 10.08238\n",
      "Epoch: 0051 train_loss= 18.17984 train_acc= 0.20154 val_loss= 32.91633 val_acc= 0.42416 time= 10.14312\n",
      "Epoch: 0052 train_loss= 13.80497 train_acc= 0.22465 val_loss= 32.88339 val_acc= 0.42159 time= 10.03973\n",
      "Epoch: 0053 train_loss= 18.85738 train_acc= 0.20282 val_loss= 32.85091 val_acc= 0.41388 time= 10.69932\n",
      "Epoch: 0054 train_loss= 26.69802 train_acc= 0.20924 val_loss= 32.85963 val_acc= 0.40488 time= 10.22530\n",
      "Epoch: 0055 train_loss= 20.86179 train_acc= 0.19769 val_loss= 32.95164 val_acc= 0.39332 time= 10.06773\n",
      "Epoch: 0056 train_loss= 24.97447 train_acc= 0.20796 val_loss= 32.96809 val_acc= 0.38175 time= 10.04134\n",
      "Epoch: 0057 train_loss= 18.95733 train_acc= 0.20796 val_loss= 32.98352 val_acc= 0.37275 time= 10.25158\n",
      "Epoch: 0058 train_loss= 13.67663 train_acc= 0.22721 val_loss= 33.06282 val_acc= 0.37018 time= 10.08877\n",
      "Epoch: 0059 train_loss= 16.25856 train_acc= 0.19127 val_loss= 33.12662 val_acc= 0.36632 time= 10.05403\n",
      "Epoch: 0060 train_loss= 21.85145 train_acc= 0.21309 val_loss= 33.15312 val_acc= 0.35733 time= 10.03620\n",
      "Epoch: 0061 train_loss= 15.79061 train_acc= 0.22593 val_loss= 33.24054 val_acc= 0.35604 time= 10.01944\n",
      "Epoch: 0062 train_loss= 16.88350 train_acc= 0.21309 val_loss= 33.33949 val_acc= 0.36118 time= 10.07626\n",
      "Epoch: 0063 train_loss= 22.61569 train_acc= 0.20924 val_loss= 33.37252 val_acc= 0.35861 time= 10.09590\n",
      "Epoch: 0064 train_loss= 17.18325 train_acc= 0.22593 val_loss= 33.44285 val_acc= 0.36118 time= 10.04443\n",
      "Epoch: 0065 train_loss= 18.56265 train_acc= 0.22850 val_loss= 33.45361 val_acc= 0.36118 time= 10.02231\n",
      "Epoch: 0066 train_loss= 22.77436 train_acc= 0.19769 val_loss= 33.35721 val_acc= 0.35604 time= 10.03176\n",
      "Epoch: 0067 train_loss= 13.76147 train_acc= 0.25546 val_loss= 33.25648 val_acc= 0.35476 time= 10.04496\n",
      "Epoch: 0068 train_loss= 16.28947 train_acc= 0.21309 val_loss= 33.16203 val_acc= 0.35347 time= 10.00350\n",
      "Epoch: 0069 train_loss= 14.18797 train_acc= 0.21438 val_loss= 33.04854 val_acc= 0.35090 time= 10.06564\n",
      "Epoch: 0070 train_loss= 22.71703 train_acc= 0.22080 val_loss= 32.84707 val_acc= 0.34833 time= 10.10605\n",
      "Epoch: 0071 train_loss= 20.62721 train_acc= 0.21951 val_loss= 32.67652 val_acc= 0.35090 time= 10.01873\n",
      "Epoch: 0072 train_loss= 15.47788 train_acc= 0.21309 val_loss= 32.42221 val_acc= 0.35219 time= 10.01072\n",
      "Epoch: 0073 train_loss= 19.90364 train_acc= 0.20282 val_loss= 32.20514 val_acc= 0.35347 time= 10.05196\n",
      "Epoch: 0074 train_loss= 19.08827 train_acc= 0.19255 val_loss= 31.94729 val_acc= 0.34961 time= 10.01344\n",
      "Epoch: 0075 train_loss= 16.06900 train_acc= 0.20282 val_loss= 31.65778 val_acc= 0.34833 time= 10.00655\n",
      "Epoch: 0076 train_loss= 14.79319 train_acc= 0.21823 val_loss= 31.42137 val_acc= 0.34319 time= 10.22597\n",
      "Epoch: 0077 train_loss= 23.02143 train_acc= 0.21181 val_loss= 31.26193 val_acc= 0.33933 time= 10.03018\n",
      "Epoch: 0078 train_loss= 18.22747 train_acc= 0.20668 val_loss= 31.15307 val_acc= 0.34062 time= 10.03681\n",
      "Epoch: 0079 train_loss= 19.87485 train_acc= 0.20924 val_loss= 31.08811 val_acc= 0.33805 time= 10.04766\n",
      "Epoch: 0080 train_loss= 21.20543 train_acc= 0.22208 val_loss= 31.04863 val_acc= 0.33676 time= 10.06339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0081 train_loss= 19.02888 train_acc= 0.22080 val_loss= 30.96159 val_acc= 0.34190 time= 10.09306\n",
      "Epoch: 0082 train_loss= 20.47910 train_acc= 0.22721 val_loss= 30.85707 val_acc= 0.34319 time= 10.16634\n",
      "Epoch: 0083 train_loss= 12.13223 train_acc= 0.21823 val_loss= 30.75383 val_acc= 0.34190 time= 10.56029\n",
      "Epoch: 0084 train_loss= 19.33172 train_acc= 0.20924 val_loss= 30.67305 val_acc= 0.34319 time= 10.25206\n",
      "Epoch: 0085 train_loss= 13.63630 train_acc= 0.19255 val_loss= 30.61614 val_acc= 0.34319 time= 10.09942\n",
      "Epoch: 0086 train_loss= 18.40340 train_acc= 0.21053 val_loss= 30.58290 val_acc= 0.34319 time= 10.02798\n",
      "Epoch: 0087 train_loss= 16.76731 train_acc= 0.18870 val_loss= 30.56355 val_acc= 0.34190 time= 10.00977\n",
      "Epoch: 0088 train_loss= 15.22830 train_acc= 0.22465 val_loss= 30.48252 val_acc= 0.33933 time= 10.10460\n",
      "Epoch: 0089 train_loss= 21.75092 train_acc= 0.21053 val_loss= 30.35401 val_acc= 0.33548 time= 10.02415\n",
      "Epoch: 0090 train_loss= 16.74750 train_acc= 0.21823 val_loss= 30.27246 val_acc= 0.33548 time= 10.02171\n",
      "Epoch: 0091 train_loss= 13.79950 train_acc= 0.23107 val_loss= 30.16435 val_acc= 0.33933 time= 10.02065\n",
      "Epoch: 0092 train_loss= 14.29775 train_acc= 0.24904 val_loss= 30.03250 val_acc= 0.33548 time= 10.03710\n",
      "Epoch: 0093 train_loss= 17.00531 train_acc= 0.21566 val_loss= 29.89946 val_acc= 0.33548 time= 10.06236\n",
      "Epoch: 0094 train_loss= 15.35412 train_acc= 0.24262 val_loss= 29.74567 val_acc= 0.33548 time= 10.05380\n",
      "Epoch: 0095 train_loss= 17.62586 train_acc= 0.21053 val_loss= 29.55219 val_acc= 0.33676 time= 10.07720\n",
      "Epoch: 0096 train_loss= 18.07541 train_acc= 0.21566 val_loss= 29.32309 val_acc= 0.33805 time= 10.05093\n",
      "Epoch: 0097 train_loss= 12.57620 train_acc= 0.20796 val_loss= 29.14723 val_acc= 0.34190 time= 10.00868\n",
      "Epoch: 0098 train_loss= 19.25006 train_acc= 0.18742 val_loss= 28.97467 val_acc= 0.34062 time= 10.05899\n",
      "Epoch: 0099 train_loss= 11.96042 train_acc= 0.22208 val_loss= 28.88815 val_acc= 0.34576 time= 10.02219\n",
      "Epoch: 0100 train_loss= 20.17628 train_acc= 0.19255 val_loss= 28.81657 val_acc= 0.34961 time= 10.02837\n",
      "Epoch: 0101 train_loss= 14.44303 train_acc= 0.21181 val_loss= 28.85328 val_acc= 0.35219 time= 10.11784\n",
      "Epoch: 0102 train_loss= 17.16253 train_acc= 0.22850 val_loss= 28.86549 val_acc= 0.35347 time= 10.01888\n",
      "Epoch: 0103 train_loss= 15.27627 train_acc= 0.23748 val_loss= 28.85413 val_acc= 0.35861 time= 10.02569\n",
      "Epoch: 0104 train_loss= 15.35476 train_acc= 0.20411 val_loss= 28.76229 val_acc= 0.35861 time= 10.05479\n",
      "Epoch: 0105 train_loss= 16.12392 train_acc= 0.20668 val_loss= 28.72330 val_acc= 0.35733 time= 10.03948\n",
      "Epoch: 0106 train_loss= 15.19277 train_acc= 0.22721 val_loss= 28.64223 val_acc= 0.36118 time= 10.02599\n",
      "Epoch: 0107 train_loss= 12.98249 train_acc= 0.21694 val_loss= 28.56274 val_acc= 0.36375 time= 10.16768\n",
      "Epoch: 0108 train_loss= 13.55622 train_acc= 0.21951 val_loss= 28.49736 val_acc= 0.36889 time= 10.02071\n",
      "Epoch: 0109 train_loss= 16.34513 train_acc= 0.21951 val_loss= 28.41840 val_acc= 0.36761 time= 10.00988\n",
      "Epoch: 0110 train_loss= 9.86151 train_acc= 0.24390 val_loss= 28.34021 val_acc= 0.36632 time= 10.02844\n",
      "Epoch: 0111 train_loss= 23.53774 train_acc= 0.21309 val_loss= 28.17878 val_acc= 0.36632 time= 10.07909\n",
      "Epoch: 0112 train_loss= 20.97197 train_acc= 0.21951 val_loss= 27.99385 val_acc= 0.36889 time= 10.03947\n",
      "Epoch: 0113 train_loss= 17.13240 train_acc= 0.20026 val_loss= 27.78705 val_acc= 0.37018 time= 10.61937\n",
      "Epoch: 0114 train_loss= 21.14526 train_acc= 0.20668 val_loss= 27.52470 val_acc= 0.36632 time= 10.13617\n",
      "Epoch: 0115 train_loss= 13.48030 train_acc= 0.22593 val_loss= 27.26147 val_acc= 0.36504 time= 10.18225\n",
      "Epoch: 0116 train_loss= 14.88432 train_acc= 0.21823 val_loss= 27.05886 val_acc= 0.36118 time= 10.06822\n",
      "Epoch: 0117 train_loss= 18.62966 train_acc= 0.20411 val_loss= 26.86804 val_acc= 0.35476 time= 10.08753\n",
      "Epoch: 0118 train_loss= 13.54227 train_acc= 0.22336 val_loss= 26.69934 val_acc= 0.34833 time= 10.07209\n",
      "Epoch: 0119 train_loss= 14.77479 train_acc= 0.21823 val_loss= 26.56824 val_acc= 0.34447 time= 10.05496\n",
      "Epoch: 0120 train_loss= 14.88729 train_acc= 0.21566 val_loss= 26.47731 val_acc= 0.34062 time= 10.41638\n",
      "Epoch: 0121 train_loss= 17.50662 train_acc= 0.21438 val_loss= 26.41264 val_acc= 0.33290 time= 10.43650\n",
      "Epoch: 0122 train_loss= 14.87835 train_acc= 0.21694 val_loss= 26.29306 val_acc= 0.33290 time= 10.00982\n",
      "Epoch: 0123 train_loss= 13.42509 train_acc= 0.24262 val_loss= 26.15578 val_acc= 0.32648 time= 10.02981\n",
      "Epoch: 0124 train_loss= 18.73548 train_acc= 0.22080 val_loss= 25.89095 val_acc= 0.32648 time= 10.03618\n",
      "Epoch: 0125 train_loss= 15.12293 train_acc= 0.21823 val_loss= 25.67790 val_acc= 0.32262 time= 10.00762\n",
      "Epoch: 0126 train_loss= 10.21852 train_acc= 0.20924 val_loss= 25.46547 val_acc= 0.32134 time= 10.10588\n",
      "Epoch: 0127 train_loss= 12.42346 train_acc= 0.20796 val_loss= 25.31334 val_acc= 0.31877 time= 10.10924\n",
      "Epoch: 0128 train_loss= 17.17189 train_acc= 0.23620 val_loss= 25.16397 val_acc= 0.32134 time= 10.14555\n",
      "Epoch: 0129 train_loss= 14.91900 train_acc= 0.23492 val_loss= 25.02213 val_acc= 0.31877 time= 10.11958\n",
      "Epoch: 0130 train_loss= 11.71852 train_acc= 0.22208 val_loss= 24.89743 val_acc= 0.32648 time= 10.08828\n",
      "Epoch: 0131 train_loss= 13.34215 train_acc= 0.23363 val_loss= 24.80370 val_acc= 0.32776 time= 10.04652\n",
      "Epoch: 0132 train_loss= 14.15359 train_acc= 0.21053 val_loss= 24.67985 val_acc= 0.32905 time= 10.19161\n",
      "Epoch: 0133 train_loss= 12.42009 train_acc= 0.22593 val_loss= 24.54455 val_acc= 0.33290 time= 10.07243\n",
      "Epoch: 0134 train_loss= 13.34191 train_acc= 0.20154 val_loss= 24.39533 val_acc= 0.32519 time= 10.07717\n",
      "Epoch: 0135 train_loss= 14.10464 train_acc= 0.22721 val_loss= 24.24030 val_acc= 0.32134 time= 10.09782\n",
      "Epoch: 0136 train_loss= 11.40422 train_acc= 0.22850 val_loss= 24.06708 val_acc= 0.32005 time= 10.07432\n",
      "Epoch: 0137 train_loss= 14.68306 train_acc= 0.23235 val_loss= 23.97888 val_acc= 0.31491 time= 10.07459\n",
      "Epoch: 0138 train_loss= 15.41291 train_acc= 0.23620 val_loss= 23.86271 val_acc= 0.30848 time= 10.14388\n",
      "Epoch: 0139 train_loss= 14.21029 train_acc= 0.20154 val_loss= 23.75949 val_acc= 0.30463 time= 10.03789\n",
      "Epoch: 0140 train_loss= 16.13368 train_acc= 0.22593 val_loss= 23.70741 val_acc= 0.30463 time= 10.08401\n",
      "Epoch: 0141 train_loss= 11.70360 train_acc= 0.22465 val_loss= 23.66511 val_acc= 0.30463 time= 10.08605\n",
      "Epoch: 0142 train_loss= 16.03068 train_acc= 0.22336 val_loss= 23.62171 val_acc= 0.30463 time= 10.02516\n",
      "Epoch: 0143 train_loss= 10.16848 train_acc= 0.21053 val_loss= 23.58752 val_acc= 0.30206 time= 10.61310\n",
      "Epoch: 0144 train_loss= 15.75734 train_acc= 0.21694 val_loss= 23.58349 val_acc= 0.29820 time= 10.26367\n",
      "Epoch: 0145 train_loss= 15.79313 train_acc= 0.20154 val_loss= 23.56407 val_acc= 0.29692 time= 10.06652\n",
      "Epoch: 0146 train_loss= 11.81378 train_acc= 0.22593 val_loss= 23.50588 val_acc= 0.29692 time= 10.03820\n",
      "Epoch: 0147 train_loss= 9.78328 train_acc= 0.25032 val_loss= 23.42124 val_acc= 0.29820 time= 10.09005\n",
      "Epoch: 0148 train_loss= 11.70792 train_acc= 0.19769 val_loss= 23.36078 val_acc= 0.30077 time= 10.02435\n",
      "Epoch: 0149 train_loss= 11.08524 train_acc= 0.23620 val_loss= 23.27391 val_acc= 0.30077 time= 10.07103\n",
      "Epoch: 0150 train_loss= 14.36255 train_acc= 0.22593 val_loss= 23.11367 val_acc= 0.30463 time= 10.07280\n",
      "Epoch: 0151 train_loss= 12.28698 train_acc= 0.22850 val_loss= 22.95193 val_acc= 0.30848 time= 10.13738\n",
      "Epoch: 0152 train_loss= 11.69849 train_acc= 0.24134 val_loss= 22.79456 val_acc= 0.31362 time= 10.12795\n",
      "Epoch: 0153 train_loss= 16.26058 train_acc= 0.23107 val_loss= 22.64399 val_acc= 0.31877 time= 10.05526\n",
      "Epoch: 0154 train_loss= 14.98880 train_acc= 0.24519 val_loss= 22.50880 val_acc= 0.32648 time= 10.03804\n",
      "Epoch: 0155 train_loss= 14.44229 train_acc= 0.19641 val_loss= 22.37570 val_acc= 0.32905 time= 10.05406\n",
      "Epoch: 0156 train_loss= 9.22316 train_acc= 0.22850 val_loss= 22.26644 val_acc= 0.32905 time= 10.04756\n",
      "Epoch: 0157 train_loss= 10.91508 train_acc= 0.20924 val_loss= 22.19081 val_acc= 0.33033 time= 10.23881\n",
      "Epoch: 0158 train_loss= 12.63656 train_acc= 0.23235 val_loss= 22.12085 val_acc= 0.33162 time= 10.04064\n",
      "Epoch: 0159 train_loss= 15.57572 train_acc= 0.23363 val_loss= 22.05674 val_acc= 0.33162 time= 10.03500\n",
      "Epoch: 0160 train_loss= 15.37927 train_acc= 0.23107 val_loss= 21.98686 val_acc= 0.33290 time= 10.05357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0161 train_loss= 10.67792 train_acc= 0.24647 val_loss= 21.90744 val_acc= 0.33162 time= 10.01205\n",
      "Epoch: 0162 train_loss= 15.27615 train_acc= 0.24005 val_loss= 21.80357 val_acc= 0.33548 time= 10.04854\n",
      "Epoch: 0163 train_loss= 11.57002 train_acc= 0.20924 val_loss= 21.73407 val_acc= 0.33290 time= 10.13722\n",
      "Epoch: 0164 train_loss= 9.30929 train_acc= 0.24647 val_loss= 21.67608 val_acc= 0.33033 time= 10.09720\n",
      "Epoch: 0165 train_loss= 15.22788 train_acc= 0.22721 val_loss= 21.63949 val_acc= 0.32391 time= 10.04852\n",
      "Epoch: 0166 train_loss= 13.88878 train_acc= 0.21053 val_loss= 21.59272 val_acc= 0.32134 time= 10.00591\n",
      "Epoch: 0167 train_loss= 8.65047 train_acc= 0.22080 val_loss= 21.55916 val_acc= 0.32262 time= 10.02684\n",
      "Epoch: 0168 train_loss= 17.35087 train_acc= 0.21694 val_loss= 21.51568 val_acc= 0.32391 time= 9.99444\n",
      "Epoch: 0169 train_loss= 11.91391 train_acc= 0.21181 val_loss= 21.51903 val_acc= 0.32519 time= 10.26238\n",
      "Epoch: 0170 train_loss= 10.18093 train_acc= 0.24775 val_loss= 21.51010 val_acc= 0.32519 time= 10.09935\n",
      "Epoch: 0171 train_loss= 14.22613 train_acc= 0.22080 val_loss= 21.52193 val_acc= 0.32134 time= 10.05346\n",
      "Epoch: 0172 train_loss= 10.01592 train_acc= 0.24519 val_loss= 21.51331 val_acc= 0.31491 time= 10.03670\n",
      "Epoch: 0173 train_loss= 11.83237 train_acc= 0.22336 val_loss= 21.53469 val_acc= 0.30848 time= 10.88150\n",
      "Epoch: 0174 train_loss= 11.28229 train_acc= 0.21823 val_loss= 21.50652 val_acc= 0.30334 time= 10.09477\n",
      "Epoch: 0175 train_loss= 8.41964 train_acc= 0.22080 val_loss= 21.45482 val_acc= 0.29692 time= 10.07324\n",
      "Epoch: 0176 train_loss= 11.83759 train_acc= 0.21309 val_loss= 21.40510 val_acc= 0.29692 time= 10.23350\n",
      "Epoch: 0177 train_loss= 12.10268 train_acc= 0.23748 val_loss= 21.37078 val_acc= 0.29049 time= 10.02527\n",
      "Epoch: 0178 train_loss= 12.73852 train_acc= 0.23235 val_loss= 21.36208 val_acc= 0.28535 time= 10.02164\n",
      "Epoch: 0179 train_loss= 8.86394 train_acc= 0.23107 val_loss= 21.33218 val_acc= 0.28406 time= 10.06773\n",
      "Epoch: 0180 train_loss= 9.18308 train_acc= 0.21309 val_loss= 21.31129 val_acc= 0.28406 time= 10.01254\n",
      "Epoch: 0181 train_loss= 16.62104 train_acc= 0.22465 val_loss= 21.30559 val_acc= 0.28406 time= 10.05915\n",
      "Epoch: 0182 train_loss= 10.34829 train_acc= 0.25160 val_loss= 21.26537 val_acc= 0.28406 time= 10.18491\n",
      "Epoch: 0183 train_loss= 10.43856 train_acc= 0.23363 val_loss= 21.23868 val_acc= 0.28535 time= 10.08796\n",
      "Epoch: 0184 train_loss= 10.17181 train_acc= 0.23235 val_loss= 21.24804 val_acc= 0.28406 time= 10.05499\n",
      "Epoch: 0185 train_loss= 11.00554 train_acc= 0.18742 val_loss= 21.23927 val_acc= 0.28149 time= 10.02377\n",
      "Epoch: 0186 train_loss= 14.45132 train_acc= 0.24262 val_loss= 21.23185 val_acc= 0.28278 time= 10.05296\n",
      "Epoch: 0187 train_loss= 9.19504 train_acc= 0.25802 val_loss= 21.21529 val_acc= 0.28535 time= 10.08141\n",
      "Epoch: 0188 train_loss= 9.48880 train_acc= 0.20411 val_loss= 21.19933 val_acc= 0.28406 time= 10.24461\n",
      "Epoch: 0189 train_loss= 14.39093 train_acc= 0.19769 val_loss= 21.21194 val_acc= 0.28406 time= 10.03684\n",
      "Epoch: 0190 train_loss= 13.09236 train_acc= 0.24262 val_loss= 21.20038 val_acc= 0.28278 time= 10.04845\n",
      "Epoch: 0191 train_loss= 11.39286 train_acc= 0.22850 val_loss= 21.18510 val_acc= 0.28278 time= 10.00304\n",
      "Epoch: 0192 train_loss= 8.49208 train_acc= 0.22593 val_loss= 21.16502 val_acc= 0.28663 time= 10.05098\n",
      "Epoch: 0193 train_loss= 11.22661 train_acc= 0.18614 val_loss= 21.12634 val_acc= 0.28535 time= 10.01326\n",
      "Epoch: 0194 train_loss= 8.03755 train_acc= 0.21566 val_loss= 21.06721 val_acc= 0.28149 time= 10.10929\n",
      "Epoch: 0195 train_loss= 10.97972 train_acc= 0.21053 val_loss= 21.00082 val_acc= 0.28149 time= 10.05867\n",
      "Epoch: 0196 train_loss= 9.89676 train_acc= 0.21566 val_loss= 20.94323 val_acc= 0.27892 time= 10.05203\n",
      "Epoch: 0197 train_loss= 8.90624 train_acc= 0.23235 val_loss= 20.88552 val_acc= 0.27506 time= 10.03912\n",
      "Epoch: 0198 train_loss= 14.23276 train_acc= 0.22850 val_loss= 20.81860 val_acc= 0.26864 time= 10.05721\n",
      "Epoch: 0199 train_loss= 12.63840 train_acc= 0.22978 val_loss= 20.75334 val_acc= 0.26350 time= 10.07235\n",
      "Epoch: 0200 train_loss= 10.65180 train_acc= 0.24904 val_loss= 20.69456 val_acc= 0.25835 time= 10.04731\n",
      "Epoch: 0201 train_loss= 9.20395 train_acc= 0.23492 val_loss= 20.62001 val_acc= 0.25450 time= 10.15635\n",
      "Epoch: 0202 train_loss= 12.69575 train_acc= 0.23877 val_loss= 20.53116 val_acc= 0.25064 time= 10.03735\n",
      "Epoch: 0203 train_loss= 10.21630 train_acc= 0.21438 val_loss= 20.40803 val_acc= 0.25064 time= 10.88857\n",
      "Epoch: 0204 train_loss= 8.12434 train_acc= 0.21438 val_loss= 20.31204 val_acc= 0.24807 time= 10.06971\n",
      "Epoch: 0205 train_loss= 9.82102 train_acc= 0.23748 val_loss= 20.21585 val_acc= 0.25064 time= 10.07346\n",
      "Epoch: 0206 train_loss= 10.06243 train_acc= 0.19897 val_loss= 20.13146 val_acc= 0.24936 time= 10.04789\n",
      "Epoch: 0207 train_loss= 8.92599 train_acc= 0.22336 val_loss= 20.06336 val_acc= 0.24550 time= 10.17333\n",
      "Epoch: 0208 train_loss= 11.36775 train_acc= 0.22721 val_loss= 20.01283 val_acc= 0.24679 time= 10.04279\n",
      "Epoch: 0209 train_loss= 8.40534 train_acc= 0.21823 val_loss= 19.96980 val_acc= 0.24293 time= 10.00799\n",
      "Epoch: 0210 train_loss= 11.82512 train_acc= 0.21566 val_loss= 19.90026 val_acc= 0.25450 time= 10.03186\n",
      "Epoch: 0211 train_loss= 9.19944 train_acc= 0.24519 val_loss= 19.83183 val_acc= 0.26093 time= 10.05684\n",
      "Epoch: 0212 train_loss= 9.59580 train_acc= 0.23363 val_loss= 19.77369 val_acc= 0.26478 time= 10.07825\n",
      "Epoch: 0213 train_loss= 11.77322 train_acc= 0.22850 val_loss= 19.70456 val_acc= 0.26735 time= 10.13677\n",
      "Epoch: 0214 train_loss= 8.23608 train_acc= 0.21694 val_loss= 19.63178 val_acc= 0.26992 time= 10.02641\n",
      "Epoch: 0215 train_loss= 10.93770 train_acc= 0.22850 val_loss= 19.54103 val_acc= 0.27121 time= 10.06599\n",
      "Epoch: 0216 train_loss= 8.53551 train_acc= 0.21053 val_loss= 19.44455 val_acc= 0.27378 time= 10.01723\n",
      "Epoch: 0217 train_loss= 8.37811 train_acc= 0.23620 val_loss= 19.35812 val_acc= 0.27892 time= 10.03538\n",
      "Epoch: 0218 train_loss= 12.65876 train_acc= 0.20796 val_loss= 19.25509 val_acc= 0.28663 time= 10.02112\n",
      "Epoch: 0219 train_loss= 8.63991 train_acc= 0.21951 val_loss= 19.17130 val_acc= 0.29049 time= 10.18267\n",
      "Epoch: 0220 train_loss= 10.04348 train_acc= 0.19897 val_loss= 19.09536 val_acc= 0.29820 time= 10.03861\n",
      "Epoch: 0221 train_loss= 13.07038 train_acc= 0.24262 val_loss= 19.00856 val_acc= 0.30206 time= 10.01927\n",
      "Epoch: 0222 train_loss= 9.71654 train_acc= 0.22336 val_loss= 18.93170 val_acc= 0.31491 time= 10.03439\n",
      "Epoch: 0223 train_loss= 8.92857 train_acc= 0.23235 val_loss= 18.84569 val_acc= 0.32648 time= 10.10064\n",
      "Epoch: 0224 train_loss= 7.13227 train_acc= 0.22721 val_loss= 18.74002 val_acc= 0.34062 time= 10.05877\n",
      "Epoch: 0225 train_loss= 10.80717 train_acc= 0.23620 val_loss= 18.63518 val_acc= 0.34319 time= 10.04830\n",
      "Epoch: 0226 train_loss= 8.98600 train_acc= 0.23363 val_loss= 18.52230 val_acc= 0.34833 time= 10.06680\n",
      "Epoch: 0227 train_loss= 9.17124 train_acc= 0.22208 val_loss= 18.42159 val_acc= 0.35090 time= 10.05916\n",
      "Epoch: 0228 train_loss= 6.92709 train_acc= 0.20924 val_loss= 18.33807 val_acc= 0.35219 time= 10.02602\n",
      "Epoch: 0229 train_loss= 10.02904 train_acc= 0.22080 val_loss= 18.25411 val_acc= 0.35090 time= 10.03453\n",
      "Epoch: 0230 train_loss= 8.76827 train_acc= 0.24005 val_loss= 18.16479 val_acc= 0.35476 time= 10.02647\n",
      "Epoch: 0231 train_loss= 9.70280 train_acc= 0.22721 val_loss= 18.10431 val_acc= 0.35733 time= 10.09016\n",
      "Epoch: 0232 train_loss= 13.47476 train_acc= 0.22208 val_loss= 18.02650 val_acc= 0.36375 time= 10.10169\n",
      "Epoch: 0233 train_loss= 9.45678 train_acc= 0.21694 val_loss= 17.95393 val_acc= 0.36889 time= 10.79672\n",
      "Epoch: 0234 train_loss= 8.97330 train_acc= 0.21053 val_loss= 17.88391 val_acc= 0.37661 time= 10.04496\n",
      "Epoch: 0235 train_loss= 10.21977 train_acc= 0.20282 val_loss= 17.81598 val_acc= 0.37404 time= 10.09983\n",
      "Epoch: 0236 train_loss= 8.21457 train_acc= 0.23748 val_loss= 17.76132 val_acc= 0.37789 time= 10.04218\n",
      "Epoch: 0237 train_loss= 8.08186 train_acc= 0.24775 val_loss= 17.70998 val_acc= 0.37789 time= 10.10902\n",
      "Epoch: 0238 train_loss= 8.42550 train_acc= 0.23363 val_loss= 17.66340 val_acc= 0.37789 time= 10.24683\n",
      "Epoch: 0239 train_loss= 8.30478 train_acc= 0.22593 val_loss= 17.60434 val_acc= 0.37661 time= 10.02570\n",
      "Epoch: 0240 train_loss= 6.41674 train_acc= 0.26187 val_loss= 17.55067 val_acc= 0.37789 time= 10.07216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0241 train_loss= 9.41452 train_acc= 0.23363 val_loss= 17.50740 val_acc= 0.38303 time= 10.10352\n",
      "Epoch: 0242 train_loss= 7.88681 train_acc= 0.24775 val_loss= 17.48282 val_acc= 0.38303 time= 10.10715\n",
      "Epoch: 0243 train_loss= 9.22976 train_acc= 0.21566 val_loss= 17.43946 val_acc= 0.38432 time= 10.08175\n",
      "Epoch: 0244 train_loss= 9.06951 train_acc= 0.23620 val_loss= 17.41250 val_acc= 0.38560 time= 10.11469\n",
      "Epoch: 0245 train_loss= 8.34277 train_acc= 0.21309 val_loss= 17.38198 val_acc= 0.38560 time= 10.01817\n",
      "Epoch: 0246 train_loss= 10.15687 train_acc= 0.24519 val_loss= 17.37591 val_acc= 0.39075 time= 10.01461\n",
      "Epoch: 0247 train_loss= 7.10354 train_acc= 0.22978 val_loss= 17.37362 val_acc= 0.39075 time= 10.08671\n",
      "Epoch: 0248 train_loss= 9.65806 train_acc= 0.25802 val_loss= 17.35454 val_acc= 0.39203 time= 10.04643\n",
      "Epoch: 0249 train_loss= 5.31145 train_acc= 0.23235 val_loss= 17.33066 val_acc= 0.39075 time= 10.02977\n",
      "Epoch: 0250 train_loss= 7.19428 train_acc= 0.26829 val_loss= 17.30233 val_acc= 0.38946 time= 10.09886\n",
      "Epoch: 0251 train_loss= 8.39866 train_acc= 0.25546 val_loss= 17.28163 val_acc= 0.38046 time= 10.12728\n",
      "Epoch: 0252 train_loss= 7.64272 train_acc= 0.23363 val_loss= 17.27137 val_acc= 0.37532 time= 10.07701\n",
      "Epoch: 0253 train_loss= 7.87135 train_acc= 0.23363 val_loss= 17.26196 val_acc= 0.37018 time= 10.05085\n",
      "Epoch: 0254 train_loss= 10.27627 train_acc= 0.24519 val_loss= 17.25071 val_acc= 0.36632 time= 10.08563\n",
      "Epoch: 0255 train_loss= 8.27254 train_acc= 0.25289 val_loss= 17.22401 val_acc= 0.35990 time= 10.06807\n",
      "Epoch: 0256 train_loss= 10.62110 train_acc= 0.21694 val_loss= 17.20232 val_acc= 0.35347 time= 10.07482\n",
      "Epoch: 0257 train_loss= 9.76221 train_acc= 0.22850 val_loss= 17.14116 val_acc= 0.34190 time= 10.13735\n",
      "Epoch: 0258 train_loss= 7.05395 train_acc= 0.20924 val_loss= 17.05355 val_acc= 0.33548 time= 10.10740\n",
      "Epoch: 0259 train_loss= 10.89289 train_acc= 0.23363 val_loss= 16.95670 val_acc= 0.33290 time= 10.26596\n",
      "Epoch: 0260 train_loss= 10.56625 train_acc= 0.22850 val_loss= 16.85572 val_acc= 0.33162 time= 10.11366\n",
      "Epoch: 0261 train_loss= 8.69494 train_acc= 0.23877 val_loss= 16.75721 val_acc= 0.32905 time= 10.08019\n",
      "Epoch: 0262 train_loss= 13.02204 train_acc= 0.21951 val_loss= 16.65714 val_acc= 0.32519 time= 10.84225\n",
      "Epoch: 0263 train_loss= 9.65751 train_acc= 0.23492 val_loss= 16.56132 val_acc= 0.31877 time= 10.24778\n",
      "Epoch: 0264 train_loss= 9.39984 train_acc= 0.21694 val_loss= 16.47090 val_acc= 0.31748 time= 10.01819\n",
      "Epoch: 0265 train_loss= 5.86712 train_acc= 0.21951 val_loss= 16.36800 val_acc= 0.31620 time= 10.02866\n",
      "Epoch: 0266 train_loss= 8.34680 train_acc= 0.21694 val_loss= 16.25902 val_acc= 0.31105 time= 10.12161\n",
      "Epoch: 0267 train_loss= 8.07157 train_acc= 0.24134 val_loss= 16.15190 val_acc= 0.30206 time= 10.02749\n",
      "Epoch: 0268 train_loss= 9.16756 train_acc= 0.23235 val_loss= 16.05602 val_acc= 0.29563 time= 10.00459\n",
      "Epoch: 0269 train_loss= 6.95076 train_acc= 0.21823 val_loss= 15.95269 val_acc= 0.29177 time= 10.24884\n",
      "Epoch: 0270 train_loss= 8.37675 train_acc= 0.24262 val_loss= 15.88020 val_acc= 0.28663 time= 10.06745\n",
      "Epoch: 0271 train_loss= 7.96884 train_acc= 0.19384 val_loss= 15.81167 val_acc= 0.28149 time= 10.13313\n",
      "Epoch: 0272 train_loss= 9.01558 train_acc= 0.24904 val_loss= 15.75001 val_acc= 0.27892 time= 10.11731\n",
      "Epoch: 0273 train_loss= 7.24666 train_acc= 0.24262 val_loss= 15.69253 val_acc= 0.27892 time= 10.08443\n",
      "Epoch: 0274 train_loss= 10.94867 train_acc= 0.20796 val_loss= 15.62467 val_acc= 0.27763 time= 10.09235\n",
      "Epoch: 0275 train_loss= 10.40398 train_acc= 0.22593 val_loss= 15.56089 val_acc= 0.27892 time= 10.15920\n",
      "Epoch: 0276 train_loss= 9.64091 train_acc= 0.22721 val_loss= 15.48774 val_acc= 0.28021 time= 10.07689\n",
      "Epoch: 0277 train_loss= 7.65327 train_acc= 0.25032 val_loss= 15.42872 val_acc= 0.28278 time= 10.09626\n",
      "Epoch: 0278 train_loss= 10.91750 train_acc= 0.21694 val_loss= 15.38442 val_acc= 0.28278 time= 10.12561\n",
      "Epoch: 0279 train_loss= 9.38813 train_acc= 0.21951 val_loss= 15.35308 val_acc= 0.28149 time= 10.09934\n",
      "Epoch: 0280 train_loss= 9.11429 train_acc= 0.20924 val_loss= 15.31653 val_acc= 0.27763 time= 10.00574\n",
      "Epoch: 0281 train_loss= 7.87298 train_acc= 0.23877 val_loss= 15.28687 val_acc= 0.27635 time= 10.01027\n",
      "Epoch: 0282 train_loss= 7.97610 train_acc= 0.24262 val_loss= 15.27033 val_acc= 0.27378 time= 10.13199\n",
      "Epoch: 0283 train_loss= 8.14473 train_acc= 0.23107 val_loss= 15.23406 val_acc= 0.27378 time= 10.12696\n",
      "Epoch: 0284 train_loss= 7.56353 train_acc= 0.21053 val_loss= 15.19657 val_acc= 0.27635 time= 10.08851\n",
      "Epoch: 0285 train_loss= 8.13876 train_acc= 0.20668 val_loss= 15.16624 val_acc= 0.28021 time= 10.03012\n",
      "Epoch: 0286 train_loss= 7.81318 train_acc= 0.21823 val_loss= 15.12693 val_acc= 0.28021 time= 10.03293\n",
      "Epoch: 0287 train_loss= 7.58261 train_acc= 0.24519 val_loss= 15.07802 val_acc= 0.27892 time= 10.01765\n",
      "Epoch: 0288 train_loss= 7.52802 train_acc= 0.23235 val_loss= 15.02507 val_acc= 0.28535 time= 10.14629\n",
      "Epoch: 0289 train_loss= 8.35372 train_acc= 0.21309 val_loss= 14.96292 val_acc= 0.28663 time= 10.08479\n",
      "Epoch: 0290 train_loss= 8.58132 train_acc= 0.22978 val_loss= 14.90427 val_acc= 0.29049 time= 10.08173\n",
      "Epoch: 0291 train_loss= 7.19594 train_acc= 0.23107 val_loss= 14.84786 val_acc= 0.29306 time= 10.04200\n",
      "Epoch: 0292 train_loss= 8.36800 train_acc= 0.21694 val_loss= 14.79908 val_acc= 0.29563 time= 10.67151\n",
      "Epoch: 0293 train_loss= 7.91056 train_acc= 0.24904 val_loss= 14.75396 val_acc= 0.30206 time= 10.07816\n",
      "Epoch: 0294 train_loss= 6.97300 train_acc= 0.23877 val_loss= 14.69054 val_acc= 0.30463 time= 10.13256\n",
      "Epoch: 0295 train_loss= 7.80616 train_acc= 0.22465 val_loss= 14.62246 val_acc= 0.30977 time= 10.08785\n",
      "Epoch: 0296 train_loss= 9.78010 train_acc= 0.24134 val_loss= 14.56380 val_acc= 0.31234 time= 10.06010\n",
      "Epoch: 0297 train_loss= 8.70177 train_acc= 0.22721 val_loss= 14.50655 val_acc= 0.31362 time= 10.07083\n",
      "Epoch: 0298 train_loss= 7.06174 train_acc= 0.27214 val_loss= 14.44552 val_acc= 0.31362 time= 10.02027\n",
      "Epoch: 0299 train_loss= 7.11037 train_acc= 0.21309 val_loss= 14.38805 val_acc= 0.31362 time= 10.08134\n",
      "Epoch: 0300 train_loss= 8.87769 train_acc= 0.23492 val_loss= 14.31733 val_acc= 0.31620 time= 10.20660\n",
      "Epoch: 0301 train_loss= 8.31536 train_acc= 0.24262 val_loss= 14.24724 val_acc= 0.32262 time= 10.08162\n",
      "Epoch: 0302 train_loss= 8.59209 train_acc= 0.23235 val_loss= 14.17895 val_acc= 0.33290 time= 10.10706\n",
      "Epoch: 0303 train_loss= 6.81804 train_acc= 0.21951 val_loss= 14.12463 val_acc= 0.34319 time= 10.06822\n",
      "Epoch: 0304 train_loss= 10.63665 train_acc= 0.23107 val_loss= 14.07776 val_acc= 0.34961 time= 10.01356\n",
      "Epoch: 0305 train_loss= 7.29729 train_acc= 0.24262 val_loss= 14.03937 val_acc= 0.35219 time= 10.08650\n",
      "Epoch: 0306 train_loss= 9.12944 train_acc= 0.24005 val_loss= 13.99664 val_acc= 0.35604 time= 10.07122\n",
      "Epoch: 0307 train_loss= 7.63810 train_acc= 0.24005 val_loss= 13.96362 val_acc= 0.35990 time= 10.20601\n",
      "Epoch: 0308 train_loss= 7.67427 train_acc= 0.22850 val_loss= 13.93329 val_acc= 0.35733 time= 10.02130\n",
      "Epoch: 0309 train_loss= 8.72642 train_acc= 0.21438 val_loss= 13.90866 val_acc= 0.35347 time= 10.00663\n",
      "Epoch: 0310 train_loss= 6.70390 train_acc= 0.25802 val_loss= 13.88190 val_acc= 0.35090 time= 10.03818\n",
      "Epoch: 0311 train_loss= 9.64160 train_acc= 0.23107 val_loss= 13.85544 val_acc= 0.34961 time= 10.05552\n",
      "Epoch: 0312 train_loss= 5.79224 train_acc= 0.22336 val_loss= 13.83453 val_acc= 0.34319 time= 10.04127\n",
      "Epoch: 0313 train_loss= 8.72936 train_acc= 0.23620 val_loss= 13.80194 val_acc= 0.33933 time= 10.15759\n",
      "Epoch: 0314 train_loss= 7.61589 train_acc= 0.23748 val_loss= 13.77172 val_acc= 0.33676 time= 10.05967\n",
      "Epoch: 0315 train_loss= 8.02522 train_acc= 0.24519 val_loss= 13.73997 val_acc= 0.33676 time= 10.03921\n",
      "Epoch: 0316 train_loss= 6.65438 train_acc= 0.24134 val_loss= 13.71743 val_acc= 0.33290 time= 10.06167\n",
      "Epoch: 0317 train_loss= 9.31952 train_acc= 0.23235 val_loss= 13.68520 val_acc= 0.32905 time= 10.06930\n",
      "Epoch: 0318 train_loss= 6.51281 train_acc= 0.22721 val_loss= 13.65164 val_acc= 0.32776 time= 10.05210\n",
      "Epoch: 0319 train_loss= 7.03030 train_acc= 0.24647 val_loss= 13.61412 val_acc= 0.32776 time= 10.16461\n",
      "Epoch: 0320 train_loss= 8.37927 train_acc= 0.25674 val_loss= 13.57012 val_acc= 0.33162 time= 10.06447\n",
      "Epoch: 0321 train_loss= 7.04790 train_acc= 0.23748 val_loss= 13.52330 val_acc= 0.32905 time= 10.03698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0322 train_loss= 6.17212 train_acc= 0.21951 val_loss= 13.49052 val_acc= 0.32905 time= 11.03098\n",
      "Epoch: 0323 train_loss= 7.59732 train_acc= 0.22080 val_loss= 13.46160 val_acc= 0.33033 time= 10.10971\n",
      "Epoch: 0324 train_loss= 5.29685 train_acc= 0.25417 val_loss= 13.43024 val_acc= 0.32905 time= 10.03883\n",
      "Epoch: 0325 train_loss= 6.13692 train_acc= 0.26059 val_loss= 13.39267 val_acc= 0.33162 time= 10.15017\n",
      "Epoch: 0326 train_loss= 8.13469 train_acc= 0.23235 val_loss= 13.37054 val_acc= 0.33290 time= 10.13595\n",
      "Epoch: 0327 train_loss= 6.43751 train_acc= 0.22208 val_loss= 13.34554 val_acc= 0.33290 time= 10.05798\n",
      "Epoch: 0328 train_loss= 6.70601 train_acc= 0.23363 val_loss= 13.32617 val_acc= 0.33162 time= 10.02800\n",
      "Epoch: 0329 train_loss= 7.49256 train_acc= 0.22850 val_loss= 13.30678 val_acc= 0.33290 time= 10.05106\n",
      "Epoch: 0330 train_loss= 7.13865 train_acc= 0.24134 val_loss= 13.27764 val_acc= 0.33290 time= 10.10111\n",
      "Epoch: 0331 train_loss= 5.86257 train_acc= 0.21181 val_loss= 13.25172 val_acc= 0.33676 time= 10.13007\n",
      "Epoch: 0332 train_loss= 4.76577 train_acc= 0.22978 val_loss= 13.22613 val_acc= 0.34319 time= 10.06900\n",
      "Epoch: 0333 train_loss= 8.59277 train_acc= 0.24262 val_loss= 13.19324 val_acc= 0.34447 time= 10.06102\n",
      "Epoch: 0334 train_loss= 5.31630 train_acc= 0.24005 val_loss= 13.16349 val_acc= 0.34704 time= 10.00980\n",
      "Epoch: 0335 train_loss= 7.30576 train_acc= 0.25674 val_loss= 13.11940 val_acc= 0.35219 time= 10.02354\n",
      "Epoch: 0336 train_loss= 6.15770 train_acc= 0.24904 val_loss= 13.07887 val_acc= 0.35861 time= 10.01355\n",
      "Epoch: 0337 train_loss= 7.62687 train_acc= 0.24390 val_loss= 13.04369 val_acc= 0.36247 time= 10.05037\n",
      "Epoch: 0338 train_loss= 7.38149 train_acc= 0.23492 val_loss= 13.02072 val_acc= 0.36761 time= 10.11332\n",
      "Epoch: 0339 train_loss= 9.33733 train_acc= 0.25546 val_loss= 12.99618 val_acc= 0.37532 time= 10.04066\n",
      "Epoch: 0340 train_loss= 5.50662 train_acc= 0.24390 val_loss= 12.97874 val_acc= 0.38432 time= 10.01500\n",
      "Epoch: 0341 train_loss= 7.83959 train_acc= 0.19384 val_loss= 12.96435 val_acc= 0.38946 time= 10.04895\n",
      "Epoch: 0342 train_loss= 6.12621 train_acc= 0.22080 val_loss= 12.95227 val_acc= 0.39974 time= 10.12262\n",
      "Epoch: 0343 train_loss= 7.55939 train_acc= 0.22721 val_loss= 12.93118 val_acc= 0.40874 time= 10.08062\n",
      "Epoch: 0344 train_loss= 6.11000 train_acc= 0.23492 val_loss= 12.90671 val_acc= 0.41388 time= 10.13862\n",
      "Epoch: 0345 train_loss= 6.27287 train_acc= 0.21951 val_loss= 12.88316 val_acc= 0.42031 time= 10.07596\n",
      "Epoch: 0346 train_loss= 5.69686 train_acc= 0.24519 val_loss= 12.85906 val_acc= 0.41902 time= 10.02523\n",
      "Epoch: 0347 train_loss= 6.73533 train_acc= 0.24262 val_loss= 12.83298 val_acc= 0.41902 time= 10.01428\n",
      "Epoch: 0348 train_loss= 6.54876 train_acc= 0.23620 val_loss= 12.80348 val_acc= 0.41774 time= 10.03934\n",
      "Epoch: 0349 train_loss= 6.15798 train_acc= 0.23363 val_loss= 12.77608 val_acc= 0.41131 time= 10.04705\n",
      "Epoch: 0350 train_loss= 5.93362 train_acc= 0.23877 val_loss= 12.75424 val_acc= 0.40103 time= 10.20159\n",
      "Epoch: 0351 train_loss= 10.25350 train_acc= 0.24519 val_loss= 12.73658 val_acc= 0.39332 time= 10.07419\n",
      "Epoch: 0352 train_loss= 5.26297 train_acc= 0.25417 val_loss= 12.71840 val_acc= 0.38560 time= 10.68464\n",
      "Epoch: 0353 train_loss= 7.83028 train_acc= 0.22850 val_loss= 12.69568 val_acc= 0.37532 time= 10.08068\n",
      "Epoch: 0354 train_loss= 6.53375 train_acc= 0.24775 val_loss= 12.66910 val_acc= 0.37018 time= 10.07474\n",
      "Epoch: 0355 train_loss= 4.70359 train_acc= 0.24519 val_loss= 12.64119 val_acc= 0.36761 time= 10.08526\n",
      "Epoch: 0356 train_loss= 5.37650 train_acc= 0.21823 val_loss= 12.60859 val_acc= 0.36247 time= 10.14638\n",
      "Epoch: 0357 train_loss= 6.14752 train_acc= 0.23363 val_loss= 12.57464 val_acc= 0.35990 time= 10.06004\n",
      "Epoch: 0358 train_loss= 5.28620 train_acc= 0.21823 val_loss= 12.53472 val_acc= 0.35733 time= 10.05877\n",
      "Epoch: 0359 train_loss= 6.31301 train_acc= 0.20668 val_loss= 12.49720 val_acc= 0.35219 time= 10.02614\n",
      "Epoch: 0360 train_loss= 6.99781 train_acc= 0.24134 val_loss= 12.46728 val_acc= 0.34576 time= 10.04429\n",
      "Epoch: 0361 train_loss= 6.38862 train_acc= 0.25931 val_loss= 12.44222 val_acc= 0.34190 time= 10.08250\n",
      "Epoch: 0362 train_loss= 5.75431 train_acc= 0.17458 val_loss= 12.42024 val_acc= 0.34062 time= 10.05362\n",
      "Epoch: 0363 train_loss= 4.64697 train_acc= 0.24005 val_loss= 12.40211 val_acc= 0.33933 time= 10.15040\n",
      "Epoch: 0364 train_loss= 5.65860 train_acc= 0.22721 val_loss= 12.38809 val_acc= 0.33805 time= 10.03564\n",
      "Epoch: 0365 train_loss= 5.53342 train_acc= 0.22721 val_loss= 12.36597 val_acc= 0.33676 time= 10.03891\n",
      "Epoch: 0366 train_loss= 5.09516 train_acc= 0.24262 val_loss= 12.34925 val_acc= 0.33548 time= 10.07848\n",
      "Epoch: 0367 train_loss= 6.05375 train_acc= 0.27856 val_loss= 12.33470 val_acc= 0.33033 time= 10.06562\n",
      "Epoch: 0368 train_loss= 6.60725 train_acc= 0.22850 val_loss= 12.31707 val_acc= 0.32519 time= 10.03305\n",
      "Epoch: 0369 train_loss= 8.23994 train_acc= 0.23748 val_loss= 12.30003 val_acc= 0.32262 time= 10.09431\n",
      "Epoch: 0370 train_loss= 5.23756 train_acc= 0.24262 val_loss= 12.27527 val_acc= 0.31877 time= 10.08285\n",
      "Epoch: 0371 train_loss= 5.26519 train_acc= 0.23235 val_loss= 12.24758 val_acc= 0.32005 time= 10.04155\n",
      "Epoch: 0372 train_loss= 5.52103 train_acc= 0.25931 val_loss= 12.21421 val_acc= 0.31877 time= 10.01572\n",
      "Epoch: 0373 train_loss= 7.87217 train_acc= 0.22336 val_loss= 12.18323 val_acc= 0.31748 time= 10.06985\n",
      "Epoch: 0374 train_loss= 4.45279 train_acc= 0.19897 val_loss= 12.15304 val_acc= 0.31748 time= 10.10390\n",
      "Epoch: 0375 train_loss= 6.80766 train_acc= 0.23748 val_loss= 12.12440 val_acc= 0.31491 time= 10.15326\n",
      "Epoch: 0376 train_loss= 6.85412 train_acc= 0.22336 val_loss= 12.09409 val_acc= 0.31105 time= 10.08571\n",
      "Epoch: 0377 train_loss= 6.21572 train_acc= 0.24005 val_loss= 12.06138 val_acc= 0.30977 time= 10.07689\n",
      "Epoch: 0378 train_loss= 6.33764 train_acc= 0.24519 val_loss= 12.02737 val_acc= 0.30977 time= 10.17740\n",
      "Epoch: 0379 train_loss= 6.60135 train_acc= 0.25546 val_loss= 11.98948 val_acc= 0.30463 time= 10.17148\n",
      "Epoch: 0380 train_loss= 6.08154 train_acc= 0.24775 val_loss= 11.95996 val_acc= 0.29820 time= 10.07545\n",
      "Epoch: 0381 train_loss= 5.78411 train_acc= 0.22080 val_loss= 11.92806 val_acc= 0.29692 time= 10.17656\n",
      "Epoch: 0382 train_loss= 5.60609 train_acc= 0.24134 val_loss= 11.89704 val_acc= 0.29563 time= 10.82941\n",
      "Epoch: 0383 train_loss= 5.40311 train_acc= 0.22721 val_loss= 11.86949 val_acc= 0.29692 time= 10.07154\n",
      "Epoch: 0384 train_loss= 5.54492 train_acc= 0.20924 val_loss= 11.84149 val_acc= 0.29820 time= 10.02525\n",
      "Epoch: 0385 train_loss= 5.93247 train_acc= 0.22080 val_loss= 11.80776 val_acc= 0.29820 time= 10.03277\n",
      "Epoch: 0386 train_loss= 4.78726 train_acc= 0.22721 val_loss= 11.77055 val_acc= 0.30206 time= 10.04456\n",
      "Epoch: 0387 train_loss= 4.58073 train_acc= 0.23877 val_loss= 11.73419 val_acc= 0.30591 time= 10.01687\n",
      "Epoch: 0388 train_loss= 6.67394 train_acc= 0.24647 val_loss= 11.70651 val_acc= 0.31105 time= 10.13074\n",
      "Epoch: 0389 train_loss= 4.35757 train_acc= 0.25032 val_loss= 11.67863 val_acc= 0.31234 time= 10.07435\n",
      "Epoch: 0390 train_loss= 7.44851 train_acc= 0.23748 val_loss= 11.64963 val_acc= 0.31234 time= 10.08917\n",
      "Epoch: 0391 train_loss= 5.52130 train_acc= 0.23877 val_loss= 11.62424 val_acc= 0.31234 time= 10.07476\n",
      "Epoch: 0392 train_loss= 6.43377 train_acc= 0.21566 val_loss= 11.60449 val_acc= 0.31234 time= 10.03567\n",
      "Epoch: 0393 train_loss= 5.31171 train_acc= 0.23748 val_loss= 11.58132 val_acc= 0.31620 time= 10.04649\n",
      "Epoch: 0394 train_loss= 5.46262 train_acc= 0.23235 val_loss= 11.56491 val_acc= 0.31620 time= 10.14691\n",
      "Epoch: 0395 train_loss= 5.55538 train_acc= 0.23492 val_loss= 11.54854 val_acc= 0.31362 time= 10.01801\n",
      "Epoch: 0396 train_loss= 6.19145 train_acc= 0.23877 val_loss= 11.53948 val_acc= 0.31362 time= 10.07904\n",
      "Epoch: 0397 train_loss= 6.41035 train_acc= 0.22721 val_loss= 11.53350 val_acc= 0.31234 time= 10.12864\n",
      "Epoch: 0398 train_loss= 6.35650 train_acc= 0.23363 val_loss= 11.52561 val_acc= 0.30848 time= 10.03840\n",
      "Epoch: 0399 train_loss= 6.40346 train_acc= 0.26701 val_loss= 11.51745 val_acc= 0.30720 time= 10.04622\n",
      "Epoch: 0400 train_loss= 4.13644 train_acc= 0.23492 val_loss= 11.49677 val_acc= 0.30591 time= 10.14069\n",
      "Epoch: 0401 train_loss= 5.40341 train_acc= 0.27086 val_loss= 11.47291 val_acc= 0.30591 time= 10.13087\n",
      "Epoch: 0402 train_loss= 6.15724 train_acc= 0.23620 val_loss= 11.44371 val_acc= 0.30848 time= 10.07968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0403 train_loss= 6.01022 train_acc= 0.22336 val_loss= 11.41377 val_acc= 0.30848 time= 10.07595\n",
      "Epoch: 0404 train_loss= 4.85069 train_acc= 0.22721 val_loss= 11.38272 val_acc= 0.30848 time= 10.06347\n",
      "Epoch: 0405 train_loss= 4.20779 train_acc= 0.22850 val_loss= 11.35916 val_acc= 0.30977 time= 10.05415\n",
      "Epoch: 0406 train_loss= 6.03339 train_acc= 0.21181 val_loss= 11.32777 val_acc= 0.31105 time= 10.18140\n",
      "Epoch: 0407 train_loss= 5.40622 train_acc= 0.23363 val_loss= 11.29851 val_acc= 0.31234 time= 10.04509\n",
      "Epoch: 0408 train_loss= 6.66147 train_acc= 0.22593 val_loss= 11.26594 val_acc= 0.31491 time= 10.03931\n",
      "Epoch: 0409 train_loss= 5.67104 train_acc= 0.23363 val_loss= 11.23388 val_acc= 0.31748 time= 10.08884\n",
      "Epoch: 0410 train_loss= 5.56684 train_acc= 0.26573 val_loss= 11.20254 val_acc= 0.32134 time= 10.03471\n",
      "Epoch: 0411 train_loss= 5.40521 train_acc= 0.24134 val_loss= 11.17568 val_acc= 0.32134 time= 10.14120\n",
      "Epoch: 0412 train_loss= 6.21449 train_acc= 0.23363 val_loss= 11.14515 val_acc= 0.32391 time= 10.71662\n",
      "Epoch: 0413 train_loss= 5.51210 train_acc= 0.22593 val_loss= 11.11550 val_acc= 0.32648 time= 10.16297\n",
      "Epoch: 0414 train_loss= 4.22623 train_acc= 0.23363 val_loss= 11.09374 val_acc= 0.32648 time= 10.10323\n",
      "Epoch: 0415 train_loss= 5.41559 train_acc= 0.21694 val_loss= 11.07019 val_acc= 0.32262 time= 10.12054\n",
      "Epoch: 0416 train_loss= 5.60211 train_acc= 0.23492 val_loss= 11.05108 val_acc= 0.32262 time= 10.17364\n",
      "Epoch: 0417 train_loss= 4.97578 train_acc= 0.24647 val_loss= 11.03159 val_acc= 0.32648 time= 10.09435\n",
      "Epoch: 0418 train_loss= 4.86875 train_acc= 0.21053 val_loss= 11.01812 val_acc= 0.32648 time= 10.02360\n",
      "Epoch: 0419 train_loss= 4.35829 train_acc= 0.25289 val_loss= 11.00655 val_acc= 0.32648 time= 10.18958\n",
      "Epoch: 0420 train_loss= 5.32315 train_acc= 0.23235 val_loss= 10.99037 val_acc= 0.32391 time= 10.03345\n",
      "Epoch: 0421 train_loss= 4.52975 train_acc= 0.24390 val_loss= 10.97391 val_acc= 0.32005 time= 10.06912\n",
      "Epoch: 0422 train_loss= 4.90689 train_acc= 0.20668 val_loss= 10.95658 val_acc= 0.32134 time= 10.03176\n",
      "Epoch: 0423 train_loss= 5.28684 train_acc= 0.20668 val_loss= 10.93844 val_acc= 0.32134 time= 10.02293\n",
      "Epoch: 0424 train_loss= 4.28500 train_acc= 0.23748 val_loss= 10.92389 val_acc= 0.32134 time= 10.03923\n",
      "Epoch: 0425 train_loss= 7.06228 train_acc= 0.21181 val_loss= 10.91173 val_acc= 0.32391 time= 10.14626\n",
      "Epoch: 0426 train_loss= 5.98583 train_acc= 0.27471 val_loss= 10.90215 val_acc= 0.32519 time= 10.03405\n",
      "Epoch: 0427 train_loss= 6.96823 train_acc= 0.21823 val_loss= 10.88726 val_acc= 0.32519 time= 10.03614\n",
      "Epoch: 0428 train_loss= 5.10565 train_acc= 0.24647 val_loss= 10.86943 val_acc= 0.32519 time= 10.02481\n",
      "Epoch: 0429 train_loss= 5.90258 train_acc= 0.22721 val_loss= 10.85613 val_acc= 0.32648 time= 10.04821\n",
      "Epoch: 0430 train_loss= 5.51188 train_acc= 0.22978 val_loss= 10.84422 val_acc= 0.32776 time= 10.07859\n",
      "Epoch: 0431 train_loss= 4.25455 train_acc= 0.24390 val_loss= 10.83308 val_acc= 0.33419 time= 10.26779\n",
      "Epoch: 0432 train_loss= 4.20304 train_acc= 0.23492 val_loss= 10.82005 val_acc= 0.33933 time= 10.03961\n",
      "Epoch: 0433 train_loss= 4.64272 train_acc= 0.21823 val_loss= 10.80641 val_acc= 0.34190 time= 10.01386\n",
      "Epoch: 0434 train_loss= 5.37891 train_acc= 0.20668 val_loss= 10.79483 val_acc= 0.34319 time= 10.01307\n",
      "Epoch: 0435 train_loss= 5.47572 train_acc= 0.23748 val_loss= 10.77320 val_acc= 0.34704 time= 10.02247\n",
      "Epoch: 0436 train_loss= 5.20293 train_acc= 0.26958 val_loss= 10.75387 val_acc= 0.34961 time= 10.03415\n",
      "Epoch: 0437 train_loss= 4.83337 train_acc= 0.20924 val_loss= 10.73511 val_acc= 0.34961 time= 10.08298\n",
      "Epoch: 0438 train_loss= 4.01449 train_acc= 0.23363 val_loss= 10.72576 val_acc= 0.35090 time= 10.20504\n",
      "Epoch: 0439 train_loss= 4.12927 train_acc= 0.24390 val_loss= 10.71739 val_acc= 0.36118 time= 10.02158\n",
      "Epoch: 0440 train_loss= 4.68428 train_acc= 0.23877 val_loss= 10.71317 val_acc= 0.36761 time= 10.03032\n",
      "Epoch: 0441 train_loss= 4.73011 train_acc= 0.25546 val_loss= 10.70335 val_acc= 0.37147 time= 10.67602\n",
      "Epoch: 0442 train_loss= 4.21337 train_acc= 0.22978 val_loss= 10.69097 val_acc= 0.37532 time= 10.31139\n",
      "Epoch: 0443 train_loss= 4.85876 train_acc= 0.24519 val_loss= 10.67989 val_acc= 0.37918 time= 10.04622\n",
      "Epoch: 0444 train_loss= 5.11424 train_acc= 0.22721 val_loss= 10.67112 val_acc= 0.39075 time= 10.12170\n",
      "Epoch: 0445 train_loss= 5.38102 train_acc= 0.25674 val_loss= 10.66769 val_acc= 0.39589 time= 10.08643\n",
      "Epoch: 0446 train_loss= 5.91957 train_acc= 0.22465 val_loss= 10.66275 val_acc= 0.40360 time= 10.05167\n",
      "Epoch: 0447 train_loss= 3.43184 train_acc= 0.22208 val_loss= 10.65944 val_acc= 0.40360 time= 10.06516\n",
      "Epoch: 0448 train_loss= 4.22411 train_acc= 0.21823 val_loss= 10.65550 val_acc= 0.40360 time= 10.03467\n",
      "Epoch: 0449 train_loss= 5.40874 train_acc= 0.22336 val_loss= 10.65182 val_acc= 0.40231 time= 10.08740\n",
      "Epoch: 0450 train_loss= 4.69964 train_acc= 0.24134 val_loss= 10.65149 val_acc= 0.39846 time= 10.16027\n",
      "Epoch: 0451 train_loss= 4.55160 train_acc= 0.26187 val_loss= 10.64834 val_acc= 0.39460 time= 10.04566\n",
      "Epoch: 0452 train_loss= 4.45751 train_acc= 0.23620 val_loss= 10.64540 val_acc= 0.38560 time= 10.02426\n",
      "Epoch: 0453 train_loss= 3.93581 train_acc= 0.26573 val_loss= 10.64315 val_acc= 0.37918 time= 10.04857\n",
      "Epoch: 0454 train_loss= 6.44636 train_acc= 0.22465 val_loss= 10.64132 val_acc= 0.36889 time= 10.02756\n",
      "Epoch: 0455 train_loss= 4.27213 train_acc= 0.25417 val_loss= 10.63999 val_acc= 0.35990 time= 10.03652\n",
      "Epoch: 0456 train_loss= 5.40503 train_acc= 0.24519 val_loss= 10.62614 val_acc= 0.34704 time= 10.11477\n",
      "Epoch: 0457 train_loss= 4.46516 train_acc= 0.23620 val_loss= 10.61532 val_acc= 0.33676 time= 10.03811\n",
      "Epoch: 0458 train_loss= 3.55256 train_acc= 0.24262 val_loss= 10.60810 val_acc= 0.33290 time= 10.02182\n",
      "Epoch: 0459 train_loss= 4.62571 train_acc= 0.24262 val_loss= 10.60653 val_acc= 0.32519 time= 10.01648\n",
      "Epoch: 0460 train_loss= 4.27098 train_acc= 0.24390 val_loss= 10.60611 val_acc= 0.32134 time= 10.03828\n",
      "Epoch: 0461 train_loss= 6.15690 train_acc= 0.24904 val_loss= 10.60454 val_acc= 0.31877 time= 10.12062\n",
      "Epoch: 0462 train_loss= 4.19139 train_acc= 0.23107 val_loss= 10.60058 val_acc= 0.31748 time= 10.12368\n",
      "Epoch: 0463 train_loss= 3.93412 train_acc= 0.24390 val_loss= 10.60742 val_acc= 0.31748 time= 10.07294\n",
      "Epoch: 0464 train_loss= 4.14746 train_acc= 0.20924 val_loss= 10.60690 val_acc= 0.31620 time= 10.04919\n",
      "Epoch: 0465 train_loss= 3.96203 train_acc= 0.23620 val_loss= 10.60049 val_acc= 0.31491 time= 10.05117\n",
      "Epoch: 0466 train_loss= 4.82175 train_acc= 0.25160 val_loss= 10.59405 val_acc= 0.31234 time= 10.03307\n",
      "Epoch: 0467 train_loss= 3.68897 train_acc= 0.23877 val_loss= 10.58979 val_acc= 0.30977 time= 10.12133\n",
      "Epoch: 0468 train_loss= 3.28694 train_acc= 0.22721 val_loss= 10.58392 val_acc= 0.30720 time= 10.06652\n",
      "Epoch: 0469 train_loss= 4.56549 train_acc= 0.25032 val_loss= 10.58588 val_acc= 0.30334 time= 10.20818\n",
      "Epoch: 0470 train_loss= 4.52509 train_acc= 0.25802 val_loss= 10.58456 val_acc= 0.30077 time= 10.09730\n",
      "Epoch: 0471 train_loss= 5.87045 train_acc= 0.22593 val_loss= 10.57997 val_acc= 0.29820 time= 10.72912\n",
      "Epoch: 0472 train_loss= 3.70348 train_acc= 0.23877 val_loss= 10.57308 val_acc= 0.29692 time= 10.06148\n",
      "Epoch: 0473 train_loss= 4.14334 train_acc= 0.23877 val_loss= 10.56600 val_acc= 0.29306 time= 10.10799\n",
      "Epoch: 0474 train_loss= 3.61564 train_acc= 0.23620 val_loss= 10.55983 val_acc= 0.29306 time= 10.06131\n",
      "Epoch: 0475 train_loss= 3.88154 train_acc= 0.22593 val_loss= 10.54970 val_acc= 0.29306 time= 10.14829\n",
      "Epoch: 0476 train_loss= 4.05148 train_acc= 0.23363 val_loss= 10.53117 val_acc= 0.29306 time= 10.05130\n",
      "Epoch: 0477 train_loss= 6.00998 train_acc= 0.23235 val_loss= 10.51075 val_acc= 0.29563 time= 10.73804\n",
      "Epoch: 0478 train_loss= 4.69290 train_acc= 0.22465 val_loss= 10.48179 val_acc= 0.29563 time= 10.05938\n",
      "Epoch: 0479 train_loss= 4.83597 train_acc= 0.22080 val_loss= 10.45366 val_acc= 0.30206 time= 10.02528\n",
      "Epoch: 0480 train_loss= 4.60865 train_acc= 0.24519 val_loss= 10.42578 val_acc= 0.30206 time= 10.04441\n",
      "Epoch: 0481 train_loss= 4.56736 train_acc= 0.24647 val_loss= 10.39826 val_acc= 0.30591 time= 10.14238\n",
      "Epoch: 0482 train_loss= 3.30902 train_acc= 0.23877 val_loss= 10.37228 val_acc= 0.31105 time= 10.20368\n",
      "Epoch: 0483 train_loss= 5.64714 train_acc= 0.24134 val_loss= 10.34711 val_acc= 0.31620 time= 10.56754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0484 train_loss= 3.84775 train_acc= 0.24775 val_loss= 10.32338 val_acc= 0.32005 time= 10.77939\n",
      "Epoch: 0485 train_loss= 4.40721 train_acc= 0.25417 val_loss= 10.30548 val_acc= 0.32648 time= 11.00741\n",
      "Epoch: 0486 train_loss= 3.64997 train_acc= 0.24390 val_loss= 10.28928 val_acc= 0.33033 time= 11.82871\n",
      "Epoch: 0487 train_loss= 4.14216 train_acc= 0.22593 val_loss= 10.27365 val_acc= 0.33290 time= 11.15288\n",
      "Epoch: 0488 train_loss= 4.58071 train_acc= 0.24904 val_loss= 10.25434 val_acc= 0.33548 time= 10.26356\n",
      "Epoch: 0489 train_loss= 3.87467 train_acc= 0.21053 val_loss= 10.23793 val_acc= 0.33548 time= 10.51335\n",
      "Epoch: 0490 train_loss= 4.23529 train_acc= 0.25417 val_loss= 10.21700 val_acc= 0.33548 time= 10.16824\n",
      "Epoch: 0491 train_loss= 4.78875 train_acc= 0.25802 val_loss= 10.19701 val_acc= 0.33548 time= 10.09219\n",
      "Epoch: 0492 train_loss= 4.67677 train_acc= 0.25160 val_loss= 10.16925 val_acc= 0.33548 time= 10.29424\n",
      "Epoch: 0493 train_loss= 3.94735 train_acc= 0.25546 val_loss= 10.14841 val_acc= 0.34447 time= 10.13864\n",
      "Epoch: 0494 train_loss= 4.11445 train_acc= 0.23877 val_loss= 10.13542 val_acc= 0.35347 time= 10.05969\n",
      "Epoch: 0495 train_loss= 4.43872 train_acc= 0.22593 val_loss= 10.12708 val_acc= 0.36247 time= 10.05989\n",
      "Epoch: 0496 train_loss= 3.42647 train_acc= 0.23877 val_loss= 10.11900 val_acc= 0.36761 time= 10.16523\n",
      "Epoch: 0497 train_loss= 3.53351 train_acc= 0.23492 val_loss= 10.11234 val_acc= 0.37018 time= 10.27010\n",
      "Epoch: 0498 train_loss= 5.05337 train_acc= 0.22850 val_loss= 10.10428 val_acc= 0.37147 time= 11.07734\n",
      "Epoch: 0499 train_loss= 4.14295 train_acc= 0.24005 val_loss= 10.09884 val_acc= 0.37404 time= 10.53184\n",
      "Epoch: 0500 train_loss= 3.22836 train_acc= 0.27343 val_loss= 10.09442 val_acc= 0.37275 time= 10.79511\n",
      "Epoch: 0501 train_loss= 3.76837 train_acc= 0.25546 val_loss= 10.08963 val_acc= 0.37275 time= 10.20842\n",
      "Epoch: 0502 train_loss= 3.84944 train_acc= 0.26059 val_loss= 10.08426 val_acc= 0.37275 time= 10.05620\n",
      "Epoch: 0503 train_loss= 5.30758 train_acc= 0.24775 val_loss= 10.07495 val_acc= 0.37275 time= 10.09187\n",
      "Epoch: 0504 train_loss= 4.27139 train_acc= 0.24390 val_loss= 10.06383 val_acc= 0.37275 time= 10.10638\n",
      "Epoch: 0505 train_loss= 4.87229 train_acc= 0.24647 val_loss= 10.05087 val_acc= 0.37147 time= 10.23768\n",
      "Epoch: 0506 train_loss= 4.16568 train_acc= 0.26059 val_loss= 10.03538 val_acc= 0.36247 time= 11.15880\n",
      "Epoch: 0507 train_loss= 3.64284 train_acc= 0.26573 val_loss= 10.01992 val_acc= 0.35219 time= 10.37403\n",
      "Epoch: 0508 train_loss= 3.16896 train_acc= 0.23363 val_loss= 10.00531 val_acc= 0.34833 time= 10.38646\n",
      "Epoch: 0509 train_loss= 3.04740 train_acc= 0.25802 val_loss= 9.99299 val_acc= 0.34447 time= 10.48899\n",
      "Epoch: 0510 train_loss= 4.55999 train_acc= 0.24904 val_loss= 9.98373 val_acc= 0.33676 time= 10.24086\n",
      "Epoch: 0511 train_loss= 3.93047 train_acc= 0.24134 val_loss= 9.97533 val_acc= 0.33676 time= 10.25573\n",
      "Epoch: 0512 train_loss= 3.48054 train_acc= 0.26829 val_loss= 9.96380 val_acc= 0.33676 time= 11.10463\n",
      "Epoch: 0513 train_loss= 4.05240 train_acc= 0.23235 val_loss= 9.95520 val_acc= 0.33676 time= 11.94828\n",
      "Epoch: 0514 train_loss= 3.57192 train_acc= 0.26573 val_loss= 9.94597 val_acc= 0.33676 time= 11.88558\n",
      "Epoch: 0515 train_loss= 3.52883 train_acc= 0.23107 val_loss= 9.94233 val_acc= 0.33676 time= 10.66294\n",
      "Epoch: 0516 train_loss= 3.74124 train_acc= 0.23620 val_loss= 9.93638 val_acc= 0.33676 time= 10.75460\n",
      "Epoch: 0517 train_loss= 3.22908 train_acc= 0.22593 val_loss= 9.93280 val_acc= 0.33676 time= 10.68127\n",
      "Epoch: 0518 train_loss= 3.05692 train_acc= 0.26059 val_loss= 9.92277 val_acc= 0.33805 time= 10.55858\n",
      "Epoch: 0519 train_loss= 4.00979 train_acc= 0.24134 val_loss= 9.91819 val_acc= 0.33805 time= 10.64940\n",
      "Epoch: 0520 train_loss= 3.13147 train_acc= 0.24519 val_loss= 9.91352 val_acc= 0.33676 time= 10.64439\n",
      "Epoch: 0521 train_loss= 3.08428 train_acc= 0.24390 val_loss= 9.90836 val_acc= 0.33676 time= 10.60708\n",
      "Epoch: 0522 train_loss= 2.97357 train_acc= 0.24390 val_loss= 9.90179 val_acc= 0.33676 time= 10.64815\n",
      "Epoch: 0523 train_loss= 4.63995 train_acc= 0.24904 val_loss= 9.89376 val_acc= 0.33548 time= 10.69927\n",
      "Epoch: 0524 train_loss= 4.11640 train_acc= 0.24262 val_loss= 9.88604 val_acc= 0.33162 time= 10.94739\n",
      "Epoch: 0525 train_loss= 3.84637 train_acc= 0.25546 val_loss= 9.87941 val_acc= 0.33419 time= 10.78827\n",
      "Epoch: 0526 train_loss= 3.75011 train_acc= 0.23235 val_loss= 9.87082 val_acc= 0.33033 time= 10.53722\n",
      "Epoch: 0527 train_loss= 3.53714 train_acc= 0.25289 val_loss= 9.86292 val_acc= 0.33033 time= 10.67333\n",
      "Epoch: 0528 train_loss= 3.91223 train_acc= 0.25160 val_loss= 9.86139 val_acc= 0.33033 time= 10.66597\n",
      "Epoch: 0529 train_loss= 3.46286 train_acc= 0.21566 val_loss= 9.85845 val_acc= 0.32134 time= 11.53005\n",
      "Epoch: 0530 train_loss= 3.29892 train_acc= 0.22978 val_loss= 9.85222 val_acc= 0.31491 time= 10.58538\n",
      "Epoch: 0531 train_loss= 4.32130 train_acc= 0.26958 val_loss= 9.85196 val_acc= 0.31234 time= 10.58812\n",
      "Epoch: 0532 train_loss= 3.34578 train_acc= 0.24262 val_loss= 9.84979 val_acc= 0.30463 time= 10.56710\n",
      "Epoch: 0533 train_loss= 3.93137 train_acc= 0.26187 val_loss= 9.84416 val_acc= 0.29820 time= 10.70524\n",
      "Epoch: 0534 train_loss= 4.55427 train_acc= 0.22465 val_loss= 9.83700 val_acc= 0.29049 time= 10.50682\n",
      "Epoch: 0535 train_loss= 4.00432 train_acc= 0.25417 val_loss= 9.82921 val_acc= 0.28663 time= 10.67872\n",
      "Epoch: 0536 train_loss= 3.57024 train_acc= 0.26059 val_loss= 9.82241 val_acc= 0.28406 time= 10.51512\n",
      "Epoch: 0537 train_loss= 3.42689 train_acc= 0.22336 val_loss= 9.81380 val_acc= 0.28535 time= 10.53860\n",
      "Epoch: 0538 train_loss= 3.29424 train_acc= 0.26316 val_loss= 9.80073 val_acc= 0.27763 time= 10.74163\n",
      "Epoch: 0539 train_loss= 4.51036 train_acc= 0.26573 val_loss= 9.79212 val_acc= 0.27378 time= 10.47752\n",
      "Epoch: 0540 train_loss= 3.04283 train_acc= 0.27343 val_loss= 9.78518 val_acc= 0.26350 time= 10.61406\n",
      "Epoch: 0541 train_loss= 3.73693 train_acc= 0.26573 val_loss= 9.77591 val_acc= 0.25707 time= 10.60231\n",
      "Epoch: 0542 train_loss= 3.79313 train_acc= 0.25289 val_loss= 9.77157 val_acc= 0.25450 time= 10.65222\n",
      "Epoch: 0543 train_loss= 3.41921 train_acc= 0.26059 val_loss= 9.76502 val_acc= 0.24936 time= 10.64332\n",
      "Epoch: 0544 train_loss= 3.34359 train_acc= 0.25032 val_loss= 9.75892 val_acc= 0.24036 time= 10.64498\n",
      "Epoch: 0545 train_loss= 2.81046 train_acc= 0.22208 val_loss= 9.75547 val_acc= 0.23265 time= 10.61931\n",
      "Epoch: 0546 train_loss= 3.35824 train_acc= 0.24005 val_loss= 9.75255 val_acc= 0.23008 time= 10.49593\n",
      "Epoch: 0547 train_loss= 3.63558 train_acc= 0.24775 val_loss= 9.75011 val_acc= 0.22494 time= 10.63205\n",
      "Epoch: 0548 train_loss= 3.50956 train_acc= 0.23235 val_loss= 9.74955 val_acc= 0.22237 time= 10.55869\n",
      "Epoch: 0549 train_loss= 3.22726 train_acc= 0.24262 val_loss= 9.75341 val_acc= 0.21979 time= 10.55331\n",
      "Epoch: 0550 train_loss= 3.67297 train_acc= 0.25032 val_loss= 9.75765 val_acc= 0.21722 time= 11.25600\n",
      "Epoch: 0551 train_loss= 3.69154 train_acc= 0.23492 val_loss= 9.76003 val_acc= 0.21594 time= 10.55438\n",
      "Epoch: 0552 train_loss= 3.20097 train_acc= 0.25674 val_loss= 9.76034 val_acc= 0.21722 time= 10.78659\n",
      "Epoch: 0553 train_loss= 4.04202 train_acc= 0.23492 val_loss= 9.75430 val_acc= 0.21851 time= 10.94497\n",
      "Epoch: 0554 train_loss= 3.15665 train_acc= 0.24005 val_loss= 9.75225 val_acc= 0.21722 time= 10.50565\n",
      "Epoch: 0555 train_loss= 3.77854 train_acc= 0.27343 val_loss= 9.74999 val_acc= 0.21722 time= 10.52998\n",
      "Epoch: 0556 train_loss= 3.79056 train_acc= 0.23235 val_loss= 9.74211 val_acc= 0.21337 time= 10.81989\n",
      "Epoch: 0557 train_loss= 3.56934 train_acc= 0.20411 val_loss= 9.73643 val_acc= 0.21337 time= 11.37033\n",
      "Epoch: 0558 train_loss= 3.28311 train_acc= 0.25289 val_loss= 9.73035 val_acc= 0.21080 time= 10.75780\n",
      "Epoch: 0559 train_loss= 3.33851 train_acc= 0.22080 val_loss= 9.71842 val_acc= 0.21337 time= 11.33510\n",
      "Epoch: 0560 train_loss= 3.77788 train_acc= 0.22336 val_loss= 9.70555 val_acc= 0.21722 time= 11.07585\n",
      "Epoch: 0561 train_loss= 2.87012 train_acc= 0.25546 val_loss= 9.69513 val_acc= 0.22108 time= 10.51212\n",
      "Epoch: 0562 train_loss= 2.52270 train_acc= 0.24904 val_loss= 9.68587 val_acc= 0.22751 time= 10.46812\n",
      "Epoch: 0563 train_loss= 3.12465 train_acc= 0.24390 val_loss= 9.67277 val_acc= 0.23265 time= 10.60780\n",
      "Epoch: 0564 train_loss= 2.93893 train_acc= 0.22850 val_loss= 9.66216 val_acc= 0.24036 time= 10.60215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0565 train_loss= 3.51722 train_acc= 0.22593 val_loss= 9.64543 val_acc= 0.24807 time= 10.50119\n",
      "Epoch: 0566 train_loss= 3.39440 train_acc= 0.24519 val_loss= 9.62980 val_acc= 0.25835 time= 10.40553\n",
      "Epoch: 0567 train_loss= 4.49396 train_acc= 0.25802 val_loss= 9.61600 val_acc= 0.25964 time= 10.50941\n",
      "Epoch: 0568 train_loss= 3.16609 train_acc= 0.24134 val_loss= 9.59875 val_acc= 0.25964 time= 10.54693\n",
      "Epoch: 0569 train_loss= 3.30214 train_acc= 0.26059 val_loss= 9.58635 val_acc= 0.25964 time= 10.57117\n",
      "Epoch: 0570 train_loss= 2.34057 train_acc= 0.25160 val_loss= 9.57317 val_acc= 0.26864 time= 10.67543\n",
      "Epoch: 0571 train_loss= 3.65072 train_acc= 0.25289 val_loss= 9.55945 val_acc= 0.27506 time= 10.59113\n",
      "Epoch: 0572 train_loss= 3.43234 train_acc= 0.25802 val_loss= 9.54695 val_acc= 0.28149 time= 10.43271\n",
      "Epoch: 0573 train_loss= 3.18221 train_acc= 0.25289 val_loss= 9.53510 val_acc= 0.28792 time= 10.62591\n",
      "Epoch: 0574 train_loss= 3.47672 train_acc= 0.25674 val_loss= 9.52400 val_acc= 0.29306 time= 10.57079\n",
      "Epoch: 0575 train_loss= 3.13899 train_acc= 0.24519 val_loss= 9.50843 val_acc= 0.29949 time= 10.75415\n",
      "Epoch: 0576 train_loss= 3.09988 train_acc= 0.23620 val_loss= 9.49405 val_acc= 0.29949 time= 10.67043\n",
      "Epoch: 0577 train_loss= 3.32787 train_acc= 0.26701 val_loss= 9.48089 val_acc= 0.29820 time= 10.43825\n",
      "Epoch: 0578 train_loss= 3.03337 train_acc= 0.25032 val_loss= 9.46829 val_acc= 0.29306 time= 10.70128\n",
      "Epoch: 0579 train_loss= 3.30656 train_acc= 0.22465 val_loss= 9.45712 val_acc= 0.29306 time= 18.61721\n",
      "Epoch: 0580 train_loss= 3.32720 train_acc= 0.25546 val_loss= 9.44763 val_acc= 0.29692 time= 13.54628\n",
      "Epoch: 0581 train_loss= 3.20457 train_acc= 0.26573 val_loss= 9.43514 val_acc= 0.30077 time= 11.49240\n",
      "Epoch: 0582 train_loss= 3.03024 train_acc= 0.26701 val_loss= 9.42232 val_acc= 0.30977 time= 11.30742\n",
      "Epoch: 0583 train_loss= 2.44957 train_acc= 0.22336 val_loss= 9.41326 val_acc= 0.31748 time= 12.88400\n",
      "Epoch: 0584 train_loss= 3.36860 train_acc= 0.25674 val_loss= 9.40522 val_acc= 0.32519 time= 12.93104\n",
      "Epoch: 0585 train_loss= 3.23752 train_acc= 0.25160 val_loss= 9.39997 val_acc= 0.33033 time= 11.74115\n",
      "Epoch: 0586 train_loss= 3.59996 train_acc= 0.24519 val_loss= 9.39762 val_acc= 0.33676 time= 11.82527\n",
      "Epoch: 0587 train_loss= 2.81527 train_acc= 0.26701 val_loss= 9.40109 val_acc= 0.35476 time= 10.80575\n",
      "Epoch: 0588 train_loss= 3.09044 train_acc= 0.25032 val_loss= 9.40642 val_acc= 0.37018 time= 10.59866\n",
      "Epoch: 0589 train_loss= 3.91787 train_acc= 0.22978 val_loss= 9.41234 val_acc= 0.38303 time= 10.57284\n",
      "Epoch: 0590 train_loss= 3.07278 train_acc= 0.25931 val_loss= 9.41340 val_acc= 0.39717 time= 10.63644\n",
      "Epoch: 0591 train_loss= 3.31851 train_acc= 0.23748 val_loss= 9.41070 val_acc= 0.39974 time= 10.58748\n",
      "Epoch: 0592 train_loss= 2.56718 train_acc= 0.24134 val_loss= 9.40897 val_acc= 0.39974 time= 10.59741\n",
      "Epoch: 0593 train_loss= 3.09073 train_acc= 0.25289 val_loss= 9.40384 val_acc= 0.39589 time= 10.65271\n",
      "Epoch: 0594 train_loss= 3.07618 train_acc= 0.26059 val_loss= 9.40101 val_acc= 0.39203 time= 10.63528\n",
      "Epoch: 0595 train_loss= 3.28916 train_acc= 0.24134 val_loss= 9.39612 val_acc= 0.38689 time= 10.57425\n",
      "Epoch: 0596 train_loss= 3.02682 train_acc= 0.25546 val_loss= 9.39227 val_acc= 0.38432 time= 10.68518\n",
      "Epoch: 0597 train_loss= 2.49853 train_acc= 0.25417 val_loss= 9.38678 val_acc= 0.37789 time= 10.55892\n",
      "Epoch: 0598 train_loss= 3.81745 train_acc= 0.24134 val_loss= 9.37975 val_acc= 0.37275 time= 10.68590\n",
      "Epoch: 0599 train_loss= 3.01729 train_acc= 0.24904 val_loss= 9.37723 val_acc= 0.36632 time= 10.53083\n",
      "Epoch: 0600 train_loss= 2.79604 train_acc= 0.24647 val_loss= 9.37749 val_acc= 0.35733 time= 10.54652\n",
      "Epoch: 0601 train_loss= 3.22256 train_acc= 0.24262 val_loss= 9.37912 val_acc= 0.34190 time= 10.57012\n",
      "Epoch: 0602 train_loss= 2.34682 train_acc= 0.26958 val_loss= 9.38365 val_acc= 0.33548 time= 10.58928\n",
      "Epoch: 0603 train_loss= 3.20053 train_acc= 0.23748 val_loss= 9.38716 val_acc= 0.33162 time= 10.65924\n",
      "Epoch: 0604 train_loss= 2.65638 train_acc= 0.25160 val_loss= 9.38940 val_acc= 0.32905 time= 10.61594\n",
      "Epoch: 0605 train_loss= 3.24215 train_acc= 0.22978 val_loss= 9.39225 val_acc= 0.32134 time= 10.66555\n",
      "Epoch: 0606 train_loss= 2.78220 train_acc= 0.25674 val_loss= 9.39091 val_acc= 0.31491 time= 10.64136\n",
      "Epoch: 0607 train_loss= 2.82204 train_acc= 0.24904 val_loss= 9.38950 val_acc= 0.31234 time= 10.63092\n",
      "Epoch: 0608 train_loss= 2.73985 train_acc= 0.25546 val_loss= 9.38902 val_acc= 0.30848 time= 10.70755\n",
      "Epoch: 0609 train_loss= 3.55847 train_acc= 0.26187 val_loss= 9.38794 val_acc= 0.30848 time= 11.07929\n",
      "Epoch: 0610 train_loss= 2.56124 train_acc= 0.23107 val_loss= 9.38680 val_acc= 0.30591 time= 11.34686\n",
      "Epoch: 0611 train_loss= 2.57760 train_acc= 0.26187 val_loss= 9.38562 val_acc= 0.30206 time= 10.53768\n",
      "Epoch: 0612 train_loss= 3.07423 train_acc= 0.25674 val_loss= 9.38539 val_acc= 0.29692 time= 11.29237\n",
      "Epoch: 0613 train_loss= 3.05677 train_acc= 0.25802 val_loss= 9.37988 val_acc= 0.28792 time= 10.54704\n",
      "Epoch: 0614 train_loss= 2.77731 train_acc= 0.24775 val_loss= 9.37649 val_acc= 0.28663 time= 10.42769\n",
      "Epoch: 0615 train_loss= 2.65295 train_acc= 0.24262 val_loss= 9.37337 val_acc= 0.29177 time= 10.60616\n",
      "Epoch: 0616 train_loss= 2.77531 train_acc= 0.26059 val_loss= 9.36709 val_acc= 0.29177 time= 10.76027\n",
      "Epoch: 0617 train_loss= 2.61697 train_acc= 0.23877 val_loss= 9.36394 val_acc= 0.29692 time= 10.48477\n",
      "Epoch: 0618 train_loss= 2.86135 train_acc= 0.23492 val_loss= 9.35243 val_acc= 0.30334 time= 10.44733\n",
      "Epoch: 0619 train_loss= 2.66000 train_acc= 0.23363 val_loss= 9.34016 val_acc= 0.30334 time= 10.63329\n",
      "Epoch: 0620 train_loss= 3.11919 train_acc= 0.24775 val_loss= 9.32814 val_acc= 0.30077 time= 11.02742\n",
      "Epoch: 0621 train_loss= 2.76876 train_acc= 0.24262 val_loss= 9.31829 val_acc= 0.30334 time= 10.38136\n",
      "Epoch: 0622 train_loss= 2.96974 train_acc= 0.21823 val_loss= 9.31049 val_acc= 0.30463 time= 10.17374\n",
      "Epoch: 0623 train_loss= 2.59655 train_acc= 0.24775 val_loss= 9.30329 val_acc= 0.31105 time= 10.10728\n",
      "Epoch: 0624 train_loss= 2.95765 train_acc= 0.26187 val_loss= 9.29650 val_acc= 0.31491 time= 10.18720\n",
      "Epoch: 0625 train_loss= 3.94990 train_acc= 0.25160 val_loss= 9.29167 val_acc= 0.31877 time= 10.11773\n",
      "Epoch: 0626 train_loss= 2.94123 train_acc= 0.26316 val_loss= 9.28843 val_acc= 0.32519 time= 10.32433\n",
      "Epoch: 0627 train_loss= 3.76304 train_acc= 0.24262 val_loss= 9.28410 val_acc= 0.33162 time= 10.19045\n",
      "Epoch: 0628 train_loss= 2.46883 train_acc= 0.25417 val_loss= 9.28214 val_acc= 0.33933 time= 10.44702\n",
      "Epoch: 0629 train_loss= 2.58022 train_acc= 0.22850 val_loss= 9.27938 val_acc= 0.34190 time= 10.16978\n",
      "Epoch: 0630 train_loss= 2.67015 train_acc= 0.23877 val_loss= 9.27778 val_acc= 0.34961 time= 10.44217\n",
      "Epoch: 0631 train_loss= 2.70072 train_acc= 0.25160 val_loss= 9.27942 val_acc= 0.35861 time= 11.35857\n",
      "Epoch: 0632 train_loss= 2.75206 train_acc= 0.25417 val_loss= 9.27960 val_acc= 0.35990 time= 10.26731\n",
      "Epoch: 0633 train_loss= 2.58817 train_acc= 0.28370 val_loss= 9.27758 val_acc= 0.36118 time= 10.32741\n",
      "Epoch: 0634 train_loss= 2.76113 train_acc= 0.22850 val_loss= 9.27627 val_acc= 0.35861 time= 10.55998\n",
      "Epoch: 0635 train_loss= 2.89226 train_acc= 0.27599 val_loss= 9.27314 val_acc= 0.35476 time= 10.36777\n",
      "Epoch: 0636 train_loss= 3.13314 train_acc= 0.24262 val_loss= 9.26826 val_acc= 0.35090 time= 10.29507\n",
      "Epoch: 0637 train_loss= 2.44207 train_acc= 0.25289 val_loss= 9.26419 val_acc= 0.35090 time= 10.27515\n",
      "Epoch: 0638 train_loss= 2.43960 train_acc= 0.22721 val_loss= 9.26661 val_acc= 0.34833 time= 10.79361\n",
      "Epoch: 0639 train_loss= 2.73125 train_acc= 0.24519 val_loss= 9.26858 val_acc= 0.34833 time= 10.63916\n",
      "Epoch: 0640 train_loss= 2.87531 train_acc= 0.24005 val_loss= 9.26793 val_acc= 0.34704 time= 10.90669\n",
      "Epoch: 0641 train_loss= 2.73001 train_acc= 0.27856 val_loss= 9.26595 val_acc= 0.34961 time= 10.77887\n",
      "Epoch: 0642 train_loss= 2.89689 train_acc= 0.24005 val_loss= 9.26272 val_acc= 0.34833 time= 10.53168\n",
      "Epoch: 0643 train_loss= 2.44808 train_acc= 0.24005 val_loss= 9.25964 val_acc= 0.34704 time= 11.25899\n",
      "Epoch: 0644 train_loss= 2.21748 train_acc= 0.25289 val_loss= 9.25437 val_acc= 0.34447 time= 10.31796\n",
      "Epoch: 0645 train_loss= 2.60070 train_acc= 0.24904 val_loss= 9.24668 val_acc= 0.33676 time= 10.57234\n",
      "Epoch: 0646 train_loss= 3.15623 train_acc= 0.25289 val_loss= 9.23679 val_acc= 0.32776 time= 11.67039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0647 train_loss= 3.26738 train_acc= 0.24005 val_loss= 9.22441 val_acc= 0.32005 time= 10.97398\n",
      "Epoch: 0648 train_loss= 2.41929 train_acc= 0.26059 val_loss= 9.21350 val_acc= 0.30720 time= 10.72055\n",
      "Epoch: 0649 train_loss= 2.95086 train_acc= 0.25931 val_loss= 9.20301 val_acc= 0.29563 time= 10.30037\n",
      "Epoch: 0650 train_loss= 2.41496 train_acc= 0.24390 val_loss= 9.19212 val_acc= 0.28406 time= 10.21115\n",
      "Epoch: 0651 train_loss= 2.65400 train_acc= 0.23492 val_loss= 9.18317 val_acc= 0.27378 time= 10.33571\n",
      "Epoch: 0652 train_loss= 2.34554 train_acc= 0.25546 val_loss= 9.17469 val_acc= 0.26864 time= 10.17491\n",
      "Epoch: 0653 train_loss= 2.39342 train_acc= 0.23363 val_loss= 9.16848 val_acc= 0.26607 time= 10.16987\n",
      "Epoch: 0654 train_loss= 2.60410 train_acc= 0.26187 val_loss= 9.16244 val_acc= 0.26864 time= 10.15347\n",
      "Epoch: 0655 train_loss= 2.58680 train_acc= 0.24647 val_loss= 9.15459 val_acc= 0.26992 time= 10.13976\n",
      "Epoch: 0656 train_loss= 2.75808 train_acc= 0.24262 val_loss= 9.14923 val_acc= 0.27506 time= 10.17266\n",
      "Epoch: 0657 train_loss= 2.73405 train_acc= 0.26059 val_loss= 9.14086 val_acc= 0.27763 time= 10.29117\n",
      "Epoch: 0658 train_loss= 2.85982 train_acc= 0.24262 val_loss= 9.13586 val_acc= 0.27763 time= 10.21945\n",
      "Epoch: 0659 train_loss= 3.03825 train_acc= 0.26059 val_loss= 9.13200 val_acc= 0.28920 time= 10.16475\n",
      "Epoch: 0660 train_loss= 3.13732 train_acc= 0.25160 val_loss= 9.12389 val_acc= 0.29049 time= 10.17452\n",
      "Epoch: 0661 train_loss= 2.86307 train_acc= 0.24134 val_loss= 9.11720 val_acc= 0.29563 time= 10.16668\n",
      "Epoch: 0662 train_loss= 2.84327 train_acc= 0.26059 val_loss= 9.11521 val_acc= 0.30334 time= 10.18586\n",
      "Epoch: 0663 train_loss= 2.29331 train_acc= 0.25032 val_loss= 9.10722 val_acc= 0.29949 time= 10.18445\n",
      "Epoch: 0664 train_loss= 2.92129 train_acc= 0.25546 val_loss= 9.09575 val_acc= 0.29434 time= 10.26545\n",
      "Epoch: 0665 train_loss= 2.21966 train_acc= 0.23492 val_loss= 9.08569 val_acc= 0.29306 time= 10.15655\n",
      "Epoch: 0666 train_loss= 2.45845 train_acc= 0.22593 val_loss= 9.07396 val_acc= 0.28792 time= 10.17514\n",
      "Epoch: 0667 train_loss= 2.65762 train_acc= 0.24390 val_loss= 9.06208 val_acc= 0.28920 time= 10.33686\n",
      "Epoch: 0668 train_loss= 2.76949 train_acc= 0.25417 val_loss= 9.05342 val_acc= 0.28792 time= 10.20589\n",
      "Epoch: 0669 train_loss= 2.95321 train_acc= 0.22593 val_loss= 9.04684 val_acc= 0.28535 time= 10.17966\n",
      "Epoch: 0670 train_loss= 2.54748 train_acc= 0.23363 val_loss= 9.04101 val_acc= 0.28278 time= 10.78517\n",
      "Epoch: 0671 train_loss= 2.77355 train_acc= 0.22080 val_loss= 9.03446 val_acc= 0.28535 time= 10.17822\n",
      "Epoch: 0672 train_loss= 2.65824 train_acc= 0.25674 val_loss= 9.03045 val_acc= 0.29563 time= 10.16873\n",
      "Epoch: 0673 train_loss= 2.56318 train_acc= 0.24134 val_loss= 9.02634 val_acc= 0.30977 time= 10.18672\n",
      "Epoch: 0674 train_loss= 2.17833 train_acc= 0.22978 val_loss= 9.02358 val_acc= 0.33033 time= 10.16535\n",
      "Epoch: 0675 train_loss= 2.27535 train_acc= 0.26701 val_loss= 9.01822 val_acc= 0.33419 time= 10.18519\n",
      "Epoch: 0676 train_loss= 2.79661 train_acc= 0.27728 val_loss= 9.01602 val_acc= 0.34319 time= 10.26835\n",
      "Epoch: 0677 train_loss= 2.20274 train_acc= 0.23620 val_loss= 9.01391 val_acc= 0.35476 time= 10.18987\n",
      "Epoch: 0678 train_loss= 2.26291 train_acc= 0.22978 val_loss= 9.01478 val_acc= 0.38689 time= 10.16687\n",
      "Epoch: 0679 train_loss= 2.20460 train_acc= 0.24775 val_loss= 9.01958 val_acc= 0.41003 time= 10.14771\n",
      "Epoch: 0680 train_loss= 2.29853 train_acc= 0.25032 val_loss= 9.02424 val_acc= 0.43573 time= 10.16799\n",
      "Epoch: 0681 train_loss= 2.23061 train_acc= 0.25931 val_loss= 9.02779 val_acc= 0.45758 time= 10.17108\n",
      "Epoch: 0682 train_loss= 2.48150 train_acc= 0.23748 val_loss= 9.02897 val_acc= 0.48586 time= 10.27399\n",
      "Epoch: 0683 train_loss= 2.60242 train_acc= 0.24904 val_loss= 9.03304 val_acc= 0.50900 time= 10.18807\n",
      "Epoch: 0684 train_loss= 2.42668 train_acc= 0.24775 val_loss= 9.03373 val_acc= 0.51671 time= 10.18050\n",
      "Epoch: 0685 train_loss= 2.10385 train_acc= 0.25674 val_loss= 9.03331 val_acc= 0.51671 time= 10.16092\n",
      "Epoch: 0686 train_loss= 2.49455 train_acc= 0.27214 val_loss= 9.02880 val_acc= 0.51928 time= 10.19089\n",
      "Epoch: 0687 train_loss= 2.85215 train_acc= 0.24904 val_loss= 9.02696 val_acc= 0.51799 time= 10.18095\n",
      "Epoch: 0688 train_loss= 2.40955 train_acc= 0.25160 val_loss= 9.02370 val_acc= 0.51542 time= 10.34604\n",
      "Epoch: 0689 train_loss= 2.49222 train_acc= 0.26958 val_loss= 9.01949 val_acc= 0.51414 time= 10.16621\n",
      "Epoch: 0690 train_loss= 2.34933 train_acc= 0.25674 val_loss= 9.01793 val_acc= 0.51414 time= 10.14862\n",
      "Epoch: 0691 train_loss= 2.60007 train_acc= 0.24519 val_loss= 9.01541 val_acc= 0.50386 time= 10.15463\n",
      "Epoch: 0692 train_loss= 2.32989 train_acc= 0.24904 val_loss= 9.01341 val_acc= 0.49357 time= 10.16793\n",
      "Epoch: 0693 train_loss= 2.29115 train_acc= 0.25931 val_loss= 9.01382 val_acc= 0.48586 time= 10.17578\n",
      "Epoch: 0694 train_loss= 2.42711 train_acc= 0.27086 val_loss= 9.01396 val_acc= 0.47044 time= 10.27507\n",
      "Epoch: 0695 train_loss= 2.09307 train_acc= 0.24647 val_loss= 9.01073 val_acc= 0.46144 time= 10.25599\n",
      "Epoch: 0696 train_loss= 2.88075 train_acc= 0.24647 val_loss= 9.00348 val_acc= 0.43702 time= 10.15232\n",
      "Epoch: 0697 train_loss= 2.32311 train_acc= 0.24647 val_loss= 8.99794 val_acc= 0.42288 time= 10.36434\n",
      "Epoch: 0698 train_loss= 2.24180 train_acc= 0.26187 val_loss= 8.99413 val_acc= 0.41388 time= 10.28601\n",
      "Epoch: 0699 train_loss= 3.27101 train_acc= 0.23492 val_loss= 8.98756 val_acc= 0.40617 time= 10.95603\n",
      "Epoch: 0700 train_loss= 2.31709 train_acc= 0.26316 val_loss= 8.97692 val_acc= 0.39589 time= 10.18660\n",
      "Epoch: 0701 train_loss= 2.63951 train_acc= 0.22978 val_loss= 8.96846 val_acc= 0.38560 time= 10.35011\n",
      "Epoch: 0702 train_loss= 2.25090 train_acc= 0.25931 val_loss= 8.96088 val_acc= 0.37918 time= 10.17490\n",
      "Epoch: 0703 train_loss= 2.38814 train_acc= 0.26059 val_loss= 8.95215 val_acc= 0.37918 time= 10.16212\n",
      "Epoch: 0704 train_loss= 2.15904 train_acc= 0.24262 val_loss= 8.94150 val_acc= 0.37404 time= 10.17272\n",
      "Epoch: 0705 train_loss= 2.02878 train_acc= 0.28626 val_loss= 8.93038 val_acc= 0.36247 time= 10.16234\n",
      "Epoch: 0706 train_loss= 2.39282 train_acc= 0.26573 val_loss= 8.92068 val_acc= 0.34961 time= 10.17351\n",
      "Epoch: 0707 train_loss= 2.19822 train_acc= 0.24390 val_loss= 8.91155 val_acc= 0.34319 time= 10.32265\n",
      "Epoch: 0708 train_loss= 2.09913 train_acc= 0.26701 val_loss= 8.90415 val_acc= 0.33290 time= 10.14331\n",
      "Epoch: 0709 train_loss= 2.10307 train_acc= 0.27086 val_loss= 8.89753 val_acc= 0.32005 time= 10.16018\n",
      "Epoch: 0710 train_loss= 2.25656 train_acc= 0.24904 val_loss= 8.89202 val_acc= 0.30720 time= 10.17400\n",
      "Epoch: 0711 train_loss= 2.10358 train_acc= 0.27599 val_loss= 8.88464 val_acc= 0.28021 time= 10.14289\n",
      "Epoch: 0712 train_loss= 2.32636 train_acc= 0.23877 val_loss= 8.87580 val_acc= 0.26607 time= 10.18821\n",
      "Epoch: 0713 train_loss= 2.51859 train_acc= 0.25032 val_loss= 8.86707 val_acc= 0.25064 time= 10.24956\n",
      "Epoch: 0714 train_loss= 2.29922 train_acc= 0.27343 val_loss= 8.85928 val_acc= 0.23008 time= 10.18350\n",
      "Epoch: 0715 train_loss= 2.36095 train_acc= 0.24775 val_loss= 8.85151 val_acc= 0.21594 time= 10.29742\n",
      "Epoch: 0716 train_loss= 2.23992 train_acc= 0.25417 val_loss= 8.84426 val_acc= 0.20694 time= 12.65261\n",
      "Epoch: 0717 train_loss= 1.95337 train_acc= 0.24005 val_loss= 8.83538 val_acc= 0.20180 time= 13.09033\n",
      "Epoch: 0718 train_loss= 2.74300 train_acc= 0.24775 val_loss= 8.82385 val_acc= 0.19794 time= 11.66670\n",
      "Epoch: 0719 train_loss= 1.97586 train_acc= 0.24390 val_loss= 8.81460 val_acc= 0.18380 time= 15.72100\n",
      "Epoch: 0720 train_loss= 2.37957 train_acc= 0.26701 val_loss= 8.80802 val_acc= 0.17866 time= 11.00755\n",
      "Epoch: 0721 train_loss= 2.12466 train_acc= 0.25032 val_loss= 8.80158 val_acc= 0.18252 time= 11.77334\n",
      "Epoch: 0722 train_loss= 2.18253 train_acc= 0.25802 val_loss= 8.79668 val_acc= 0.18252 time= 11.09823\n",
      "Epoch: 0723 train_loss= 2.37827 train_acc= 0.24262 val_loss= 8.79356 val_acc= 0.18123 time= 11.89769\n",
      "Epoch: 0724 train_loss= 2.18917 train_acc= 0.25160 val_loss= 8.78995 val_acc= 0.18123 time= 11.08811\n",
      "Epoch: 0725 train_loss= 2.05849 train_acc= 0.23107 val_loss= 8.78713 val_acc= 0.17995 time= 10.73942\n",
      "Epoch: 0726 train_loss= 2.69022 train_acc= 0.24005 val_loss= 8.78529 val_acc= 0.19152 time= 10.97514\n",
      "Epoch: 0727 train_loss= 2.10837 train_acc= 0.25032 val_loss= 8.78360 val_acc= 0.19794 time= 11.62350\n",
      "Epoch: 0728 train_loss= 2.23579 train_acc= 0.26829 val_loss= 8.78198 val_acc= 0.20951 time= 11.68272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0729 train_loss= 2.56217 train_acc= 0.27471 val_loss= 8.78076 val_acc= 0.21979 time= 11.14653\n",
      "Epoch: 0730 train_loss= 2.54398 train_acc= 0.26059 val_loss= 8.77712 val_acc= 0.22751 time= 11.13443\n",
      "Epoch: 0731 train_loss= 2.39769 train_acc= 0.26059 val_loss= 8.77310 val_acc= 0.22494 time= 10.79256\n",
      "Epoch: 0732 train_loss= 2.05324 train_acc= 0.24519 val_loss= 8.77044 val_acc= 0.22494 time= 11.53950\n",
      "Epoch: 0733 train_loss= 2.04174 train_acc= 0.23620 val_loss= 8.76650 val_acc= 0.23393 time= 12.09543\n",
      "Epoch: 0734 train_loss= 2.50242 train_acc= 0.25417 val_loss= 8.76272 val_acc= 0.23907 time= 12.42438\n",
      "Epoch: 0735 train_loss= 1.94980 train_acc= 0.24134 val_loss= 8.76144 val_acc= 0.26350 time= 14.43249\n",
      "Epoch: 0736 train_loss= 2.59656 train_acc= 0.22978 val_loss= 8.76331 val_acc= 0.28406 time= 10.75614\n",
      "Epoch: 0737 train_loss= 2.27432 train_acc= 0.27728 val_loss= 8.76645 val_acc= 0.29820 time= 10.53642\n",
      "Epoch: 0738 train_loss= 2.05001 train_acc= 0.26444 val_loss= 8.76616 val_acc= 0.30720 time= 10.77888\n",
      "Epoch: 0739 train_loss= 2.04565 train_acc= 0.24519 val_loss= 8.76561 val_acc= 0.32905 time= 10.33685\n",
      "Epoch: 0740 train_loss= 2.19297 train_acc= 0.23363 val_loss= 8.76259 val_acc= 0.33805 time= 10.33773\n",
      "Epoch: 0741 train_loss= 2.26351 train_acc= 0.24005 val_loss= 8.75745 val_acc= 0.34319 time= 10.22030\n",
      "Epoch: 0742 train_loss= 2.24449 train_acc= 0.24775 val_loss= 8.75302 val_acc= 0.35476 time= 10.28089\n",
      "Epoch: 0743 train_loss= 1.92195 train_acc= 0.27214 val_loss= 8.74661 val_acc= 0.35219 time= 10.33343\n",
      "Epoch: 0744 train_loss= 1.99340 train_acc= 0.24647 val_loss= 8.73763 val_acc= 0.35090 time= 10.29520\n",
      "Epoch: 0745 train_loss= 2.29319 train_acc= 0.23748 val_loss= 8.72974 val_acc= 0.34447 time= 10.88849\n",
      "Epoch: 0746 train_loss= 2.06240 train_acc= 0.24262 val_loss= 8.72149 val_acc= 0.33290 time= 11.04541\n",
      "Epoch: 0747 train_loss= 2.12767 train_acc= 0.23620 val_loss= 8.71674 val_acc= 0.33162 time= 11.03187\n",
      "Epoch: 0748 train_loss= 2.30895 train_acc= 0.25289 val_loss= 8.71184 val_acc= 0.33290 time= 12.18789\n",
      "Epoch: 0749 train_loss= 1.97916 train_acc= 0.27728 val_loss= 8.70794 val_acc= 0.33290 time= 11.34329\n",
      "Epoch: 0750 train_loss= 2.18564 train_acc= 0.25931 val_loss= 8.69866 val_acc= 0.32262 time= 10.50748\n",
      "Epoch: 0751 train_loss= 1.94628 train_acc= 0.26958 val_loss= 8.68519 val_acc= 0.30720 time= 10.56370\n",
      "Epoch: 0752 train_loss= 2.00472 train_acc= 0.26573 val_loss= 8.67899 val_acc= 0.29306 time= 10.45730\n",
      "Epoch: 0753 train_loss= 2.18071 train_acc= 0.26187 val_loss= 8.67301 val_acc= 0.28663 time= 11.20341\n",
      "Epoch: 0754 train_loss= 2.34895 train_acc= 0.29268 val_loss= 8.66646 val_acc= 0.27121 time= 11.33842\n",
      "Epoch: 0755 train_loss= 2.12538 train_acc= 0.25417 val_loss= 8.65992 val_acc= 0.25193 time= 11.09023\n",
      "Epoch: 0756 train_loss= 1.93121 train_acc= 0.26187 val_loss= 8.65339 val_acc= 0.23650 time= 10.54766\n",
      "Epoch: 0757 train_loss= 1.99790 train_acc= 0.25160 val_loss= 8.64904 val_acc= 0.23008 time= 10.62615\n",
      "Epoch: 0758 train_loss= 1.88938 train_acc= 0.24262 val_loss= 8.64548 val_acc= 0.21851 time= 10.31702\n",
      "Epoch: 0759 train_loss= 1.98044 train_acc= 0.25417 val_loss= 8.64224 val_acc= 0.20823 time= 10.45188\n",
      "Epoch: 0760 train_loss= 2.61654 train_acc= 0.24134 val_loss= 8.63728 val_acc= 0.19409 time= 10.25126\n",
      "Epoch: 0761 train_loss= 2.17691 train_acc= 0.25289 val_loss= 8.63206 val_acc= 0.18380 time= 10.49199\n",
      "Epoch: 0762 train_loss= 2.11630 train_acc= 0.26059 val_loss= 8.62699 val_acc= 0.17352 time= 10.35988\n",
      "Epoch: 0763 train_loss= 2.26152 train_acc= 0.26444 val_loss= 8.62217 val_acc= 0.16452 time= 10.47292\n",
      "Epoch: 0764 train_loss= 1.84084 train_acc= 0.25032 val_loss= 8.61808 val_acc= 0.15039 time= 12.20521\n",
      "Epoch: 0765 train_loss= 2.10535 train_acc= 0.25674 val_loss= 8.60913 val_acc= 0.14653 time= 11.13660\n",
      "Epoch: 0766 train_loss= 1.94472 train_acc= 0.25032 val_loss= 8.59983 val_acc= 0.14524 time= 10.55911\n",
      "Epoch: 0767 train_loss= 2.20922 train_acc= 0.22850 val_loss= 8.59119 val_acc= 0.13753 time= 10.48456\n",
      "Epoch: 0768 train_loss= 1.97333 train_acc= 0.24775 val_loss= 8.58260 val_acc= 0.13753 time= 10.62107\n",
      "Epoch: 0769 train_loss= 1.88655 train_acc= 0.26059 val_loss= 8.57211 val_acc= 0.13753 time= 10.63708\n",
      "Epoch: 0770 train_loss= 2.41223 train_acc= 0.27599 val_loss= 8.55940 val_acc= 0.13368 time= 10.42073\n",
      "Epoch: 0771 train_loss= 2.24558 train_acc= 0.23492 val_loss= 8.54957 val_acc= 0.12596 time= 10.67346\n",
      "Epoch: 0772 train_loss= 1.97757 train_acc= 0.23748 val_loss= 8.54540 val_acc= 0.11825 time= 10.71305\n",
      "Epoch: 0773 train_loss= 2.00407 train_acc= 0.21566 val_loss= 8.54442 val_acc= 0.11568 time= 10.82940\n",
      "Epoch: 0774 train_loss= 2.24899 train_acc= 0.24519 val_loss= 8.54546 val_acc= 0.11825 time= 11.55554\n",
      "Epoch: 0775 train_loss= 1.95865 train_acc= 0.26701 val_loss= 8.54547 val_acc= 0.12082 time= 11.52434\n",
      "Epoch: 0776 train_loss= 2.25880 train_acc= 0.23107 val_loss= 8.54675 val_acc= 0.13111 time= 10.90788\n",
      "Epoch: 0777 train_loss= 1.83464 train_acc= 0.26829 val_loss= 8.54851 val_acc= 0.13882 time= 11.16958\n",
      "Epoch: 0778 train_loss= 2.09086 train_acc= 0.24390 val_loss= 8.55134 val_acc= 0.15681 time= 10.80236\n",
      "Epoch: 0779 train_loss= 2.21818 train_acc= 0.27985 val_loss= 8.55621 val_acc= 0.16838 time= 11.07280\n",
      "Epoch: 0780 train_loss= 2.12745 train_acc= 0.25802 val_loss= 8.56029 val_acc= 0.18638 time= 10.66274\n",
      "Epoch: 0781 train_loss= 2.03113 train_acc= 0.26573 val_loss= 8.56375 val_acc= 0.20308 time= 11.09602\n",
      "Epoch: 0782 train_loss= 2.04596 train_acc= 0.26059 val_loss= 8.56975 val_acc= 0.23522 time= 10.99775\n",
      "Epoch: 0783 train_loss= 2.20020 train_acc= 0.26701 val_loss= 8.57843 val_acc= 0.24807 time= 10.41899\n",
      "Epoch: 0784 train_loss= 1.79466 train_acc= 0.24134 val_loss= 8.58687 val_acc= 0.26864 time= 10.81421\n",
      "Epoch: 0785 train_loss= 1.90069 train_acc= 0.26829 val_loss= 8.58977 val_acc= 0.30334 time= 10.43254\n",
      "Epoch: 0786 train_loss= 2.34001 train_acc= 0.27214 val_loss= 8.59150 val_acc= 0.32391 time= 10.47845\n",
      "Epoch: 0787 train_loss= 2.03175 train_acc= 0.27471 val_loss= 8.58802 val_acc= 0.35219 time= 10.89197\n",
      "Epoch: 0788 train_loss= 1.93397 train_acc= 0.26187 val_loss= 8.58211 val_acc= 0.35604 time= 10.99030\n",
      "Epoch: 0789 train_loss= 1.98261 train_acc= 0.27599 val_loss= 8.57015 val_acc= 0.38175 time= 10.62265\n",
      "Epoch: 0790 train_loss= 2.23886 train_acc= 0.26701 val_loss= 8.55923 val_acc= 0.41388 time= 11.17947\n",
      "Epoch: 0791 train_loss= 1.90992 train_acc= 0.26187 val_loss= 8.55049 val_acc= 0.44087 time= 10.95735\n",
      "Epoch: 0792 train_loss= 1.95988 train_acc= 0.26829 val_loss= 8.54555 val_acc= 0.45630 time= 10.56176\n",
      "Epoch: 0793 train_loss= 1.86692 train_acc= 0.25160 val_loss= 8.53640 val_acc= 0.46787 time= 10.36704\n",
      "Epoch: 0794 train_loss= 2.46586 train_acc= 0.25417 val_loss= 8.53071 val_acc= 0.45758 time= 10.49417\n",
      "Epoch: 0795 train_loss= 2.28887 train_acc= 0.25417 val_loss= 8.52840 val_acc= 0.42159 time= 11.40275\n",
      "Epoch: 0796 train_loss= 1.89129 train_acc= 0.27086 val_loss= 8.52476 val_acc= 0.38175 time= 11.54148\n",
      "Epoch: 0797 train_loss= 2.37609 train_acc= 0.25032 val_loss= 8.52080 val_acc= 0.35604 time= 10.46217\n",
      "Epoch: 0798 train_loss= 2.05707 train_acc= 0.23877 val_loss= 8.51511 val_acc= 0.31620 time= 11.24068\n",
      "Epoch: 0799 train_loss= 1.99328 train_acc= 0.25931 val_loss= 8.51054 val_acc= 0.28920 time= 10.46657\n",
      "Epoch: 0800 train_loss= 1.90595 train_acc= 0.27856 val_loss= 8.50840 val_acc= 0.27378 time= 11.09615\n",
      "Epoch: 0801 train_loss= 2.11721 train_acc= 0.25674 val_loss= 8.50322 val_acc= 0.24422 time= 11.75191\n",
      "Epoch: 0802 train_loss= 1.89413 train_acc= 0.24390 val_loss= 8.49890 val_acc= 0.23136 time= 12.02321\n",
      "Epoch: 0803 train_loss= 2.05971 train_acc= 0.23877 val_loss= 8.49706 val_acc= 0.20180 time= 11.02414\n",
      "Epoch: 0804 train_loss= 1.85196 train_acc= 0.26701 val_loss= 8.49463 val_acc= 0.16967 time= 10.87346\n",
      "Epoch: 0805 train_loss= 2.07240 train_acc= 0.24005 val_loss= 8.49588 val_acc= 0.15167 time= 12.20098\n",
      "Epoch: 0806 train_loss= 1.90891 train_acc= 0.26573 val_loss= 8.49700 val_acc= 0.13625 time= 11.28016\n",
      "Epoch: 0807 train_loss= 1.91729 train_acc= 0.27471 val_loss= 8.49608 val_acc= 0.12339 time= 10.58741\n",
      "Epoch: 0808 train_loss= 2.01738 train_acc= 0.25417 val_loss= 8.49209 val_acc= 0.12211 time= 10.99525\n",
      "Epoch: 0809 train_loss= 1.82279 train_acc= 0.24134 val_loss= 8.48664 val_acc= 0.11825 time= 12.38005\n",
      "Epoch: 0810 train_loss= 2.02727 train_acc= 0.25289 val_loss= 8.47606 val_acc= 0.11954 time= 10.97703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0811 train_loss= 1.99702 train_acc= 0.25417 val_loss= 8.46261 val_acc= 0.12211 time= 10.96758\n",
      "Epoch: 0812 train_loss= 1.85770 train_acc= 0.24134 val_loss= 8.45027 val_acc= 0.12853 time= 13.13021\n",
      "Epoch: 0813 train_loss= 1.91447 train_acc= 0.23363 val_loss= 8.44250 val_acc= 0.13753 time= 12.64116\n",
      "Epoch: 0814 train_loss= 2.12333 train_acc= 0.27728 val_loss= 8.43652 val_acc= 0.14139 time= 12.36061\n",
      "Epoch: 0815 train_loss= 1.85483 train_acc= 0.24134 val_loss= 8.43528 val_acc= 0.15553 time= 12.69058\n",
      "Epoch: 0816 train_loss= 2.10569 train_acc= 0.26829 val_loss= 8.43604 val_acc= 0.16838 time= 12.12582\n",
      "Epoch: 0817 train_loss= 1.78377 train_acc= 0.22721 val_loss= 8.43808 val_acc= 0.17095 time= 11.50631\n",
      "Epoch: 0818 train_loss= 1.81786 train_acc= 0.27599 val_loss= 8.44149 val_acc= 0.16967 time= 11.23131\n",
      "Epoch: 0819 train_loss= 1.93324 train_acc= 0.26187 val_loss= 8.44541 val_acc= 0.17738 time= 10.79179\n",
      "Epoch: 0820 train_loss= 2.09987 train_acc= 0.26059 val_loss= 8.44425 val_acc= 0.17995 time= 10.72902\n",
      "Epoch: 0821 train_loss= 2.07581 train_acc= 0.25802 val_loss= 8.44292 val_acc= 0.18509 time= 10.56498\n",
      "Epoch: 0822 train_loss= 2.15068 train_acc= 0.21694 val_loss= 8.44188 val_acc= 0.17738 time= 10.40442\n",
      "Epoch: 0823 train_loss= 1.87912 train_acc= 0.25160 val_loss= 8.43634 val_acc= 0.16067 time= 10.29425\n",
      "Epoch: 0824 train_loss= 2.05504 train_acc= 0.22850 val_loss= 8.43196 val_acc= 0.14653 time= 10.34814\n",
      "Epoch: 0825 train_loss= 1.96655 train_acc= 0.24005 val_loss= 8.42523 val_acc= 0.14396 time= 10.26728\n",
      "Epoch: 0826 train_loss= 1.94911 train_acc= 0.23235 val_loss= 8.41929 val_acc= 0.14396 time= 10.34280\n",
      "Epoch: 0827 train_loss= 1.96852 train_acc= 0.26444 val_loss= 8.41465 val_acc= 0.15681 time= 10.75270\n",
      "Epoch: 0828 train_loss= 1.94252 train_acc= 0.27214 val_loss= 8.41102 val_acc= 0.17095 time= 11.43717\n",
      "Epoch: 0829 train_loss= 1.86237 train_acc= 0.25802 val_loss= 8.40825 val_acc= 0.17995 time= 11.46718\n",
      "Epoch: 0830 train_loss= 1.90024 train_acc= 0.26701 val_loss= 8.40658 val_acc= 0.18895 time= 11.87101\n",
      "Epoch: 0831 train_loss= 1.88192 train_acc= 0.25931 val_loss= 8.40560 val_acc= 0.19152 time= 11.95455\n",
      "Epoch: 0832 train_loss= 1.82180 train_acc= 0.26701 val_loss= 8.40352 val_acc= 0.19280 time= 10.99618\n",
      "Epoch: 0833 train_loss= 1.94928 train_acc= 0.23620 val_loss= 8.40100 val_acc= 0.21208 time= 12.10486\n",
      "Epoch: 0834 train_loss= 2.19515 train_acc= 0.25802 val_loss= 8.39802 val_acc= 0.23008 time= 11.26980\n",
      "Epoch: 0835 train_loss= 1.90229 train_acc= 0.27599 val_loss= 8.39524 val_acc= 0.26478 time= 10.87907\n",
      "Epoch: 0836 train_loss= 2.04327 train_acc= 0.26958 val_loss= 8.39323 val_acc= 0.28278 time= 12.39381\n",
      "Epoch: 0837 train_loss= 1.87587 train_acc= 0.25546 val_loss= 8.39189 val_acc= 0.29177 time= 11.72504\n",
      "Epoch: 0838 train_loss= 1.81692 train_acc= 0.24390 val_loss= 8.39056 val_acc= 0.31362 time= 12.10107\n",
      "Epoch: 0839 train_loss= 1.79879 train_acc= 0.27086 val_loss= 8.38838 val_acc= 0.33162 time= 10.78873\n",
      "Epoch: 0840 train_loss= 1.97785 train_acc= 0.27985 val_loss= 8.38578 val_acc= 0.33290 time= 10.29513\n",
      "Epoch: 0841 train_loss= 1.93908 train_acc= 0.25417 val_loss= 8.38337 val_acc= 0.33419 time= 10.32913\n",
      "Epoch: 0842 train_loss= 2.12271 train_acc= 0.23748 val_loss= 8.38230 val_acc= 0.33676 time= 10.39374\n",
      "Epoch: 0843 train_loss= 2.23118 train_acc= 0.25289 val_loss= 8.38143 val_acc= 0.34062 time= 11.97789\n",
      "Epoch: 0844 train_loss= 1.81808 train_acc= 0.26316 val_loss= 8.38059 val_acc= 0.33290 time= 11.78892\n",
      "Epoch: 0845 train_loss= 1.88532 train_acc= 0.24134 val_loss= 8.37857 val_acc= 0.29692 time= 10.68195\n",
      "Epoch: 0846 train_loss= 1.81349 train_acc= 0.25160 val_loss= 8.37653 val_acc= 0.26735 time= 10.33590\n",
      "Epoch: 0847 train_loss= 1.89548 train_acc= 0.26059 val_loss= 8.37547 val_acc= 0.28021 time= 11.13995\n",
      "Epoch: 0848 train_loss= 1.81564 train_acc= 0.25032 val_loss= 8.37468 val_acc= 0.30720 time= 10.85905\n",
      "Epoch: 0849 train_loss= 2.23744 train_acc= 0.26958 val_loss= 8.37451 val_acc= 0.32391 time= 11.25795\n",
      "Epoch: 0850 train_loss= 1.93381 train_acc= 0.26958 val_loss= 8.37291 val_acc= 0.32391 time= 10.86147\n",
      "Epoch: 0851 train_loss= 1.78296 train_acc= 0.27599 val_loss= 8.37170 val_acc= 0.33162 time= 10.60434\n",
      "Epoch: 0852 train_loss= 1.87195 train_acc= 0.24262 val_loss= 8.36948 val_acc= 0.35219 time= 10.24253\n",
      "Epoch: 0853 train_loss= 1.75454 train_acc= 0.22978 val_loss= 8.36867 val_acc= 0.35733 time= 10.54007\n",
      "Epoch: 0854 train_loss= 1.80091 train_acc= 0.26187 val_loss= 8.36877 val_acc= 0.36889 time= 10.95957\n",
      "Epoch: 0855 train_loss= 1.80927 train_acc= 0.25546 val_loss= 8.36837 val_acc= 0.37147 time= 10.50039\n",
      "Epoch: 0856 train_loss= 1.78411 train_acc= 0.28755 val_loss= 8.36866 val_acc= 0.37018 time= 10.49949\n",
      "Epoch: 0857 train_loss= 1.74806 train_acc= 0.26573 val_loss= 8.36809 val_acc= 0.36632 time= 10.78347\n",
      "Epoch: 0858 train_loss= 1.82031 train_acc= 0.25032 val_loss= 8.36625 val_acc= 0.36118 time= 10.62082\n",
      "Epoch: 0859 train_loss= 1.91873 train_acc= 0.25931 val_loss= 8.36309 val_acc= 0.34319 time= 10.59337\n",
      "Epoch: 0860 train_loss= 1.85829 train_acc= 0.23492 val_loss= 8.35827 val_acc= 0.32134 time= 10.74663\n",
      "Epoch: 0861 train_loss= 1.78821 train_acc= 0.29782 val_loss= 8.35420 val_acc= 0.28149 time= 10.72517\n",
      "Epoch: 0862 train_loss= 1.85532 train_acc= 0.27471 val_loss= 8.34962 val_acc= 0.25707 time= 10.66266\n",
      "Epoch: 0863 train_loss= 1.75123 train_acc= 0.26187 val_loss= 8.34518 val_acc= 0.24036 time= 10.63985\n",
      "Epoch: 0864 train_loss= 2.03585 train_acc= 0.25674 val_loss= 8.34272 val_acc= 0.23779 time= 11.45934\n",
      "Epoch: 0865 train_loss= 1.90511 train_acc= 0.24519 val_loss= 8.34062 val_acc= 0.22751 time= 12.17425\n",
      "Epoch: 0866 train_loss= 1.70854 train_acc= 0.24775 val_loss= 8.34037 val_acc= 0.22751 time= 11.02903\n",
      "Epoch: 0867 train_loss= 1.79360 train_acc= 0.22850 val_loss= 8.33882 val_acc= 0.23650 time= 11.10019\n",
      "Epoch: 0868 train_loss= 1.73305 train_acc= 0.23235 val_loss= 8.33934 val_acc= 0.25064 time= 11.27666\n",
      "Epoch: 0869 train_loss= 1.89029 train_acc= 0.26829 val_loss= 8.34121 val_acc= 0.27378 time= 12.28638\n",
      "Epoch: 0870 train_loss= 1.89281 train_acc= 0.25289 val_loss= 8.34495 val_acc= 0.31620 time= 12.09010\n",
      "Epoch: 0871 train_loss= 1.78747 train_acc= 0.26187 val_loss= 8.34715 val_acc= 0.34062 time= 11.14841\n",
      "Epoch: 0872 train_loss= 1.75113 train_acc= 0.25160 val_loss= 8.35039 val_acc= 0.37275 time= 11.61107\n",
      "Epoch: 0873 train_loss= 1.79857 train_acc= 0.26059 val_loss= 8.35221 val_acc= 0.39075 time= 11.16775\n",
      "Epoch: 0874 train_loss= 1.81711 train_acc= 0.25802 val_loss= 8.35469 val_acc= 0.40874 time= 10.54327\n",
      "Epoch: 0875 train_loss= 1.71625 train_acc= 0.23363 val_loss= 8.35728 val_acc= 0.41517 time= 10.31250\n",
      "Epoch: 0876 train_loss= 1.96159 train_acc= 0.27086 val_loss= 8.35577 val_acc= 0.41517 time= 10.30388\n",
      "Epoch: 0877 train_loss= 1.89703 train_acc= 0.25289 val_loss= 8.35223 val_acc= 0.39717 time= 10.26486\n",
      "Epoch: 0878 train_loss= 1.96222 train_acc= 0.25417 val_loss= 8.34772 val_acc= 0.37147 time= 10.37840\n",
      "Epoch: 0879 train_loss= 1.72536 train_acc= 0.23492 val_loss= 8.34228 val_acc= 0.33162 time= 10.74230\n",
      "Epoch: 0880 train_loss= 1.78668 train_acc= 0.25417 val_loss= 8.33378 val_acc= 0.29177 time= 10.47411\n",
      "Epoch: 0881 train_loss= 1.78693 train_acc= 0.23877 val_loss= 8.32543 val_acc= 0.26607 time= 10.24485\n",
      "Epoch: 0882 train_loss= 1.79625 train_acc= 0.29397 val_loss= 8.31734 val_acc= 0.26478 time= 10.26760\n",
      "Epoch: 0883 train_loss= 1.82073 train_acc= 0.25546 val_loss= 8.30943 val_acc= 0.25835 time= 10.26910\n",
      "Epoch: 0884 train_loss= 1.79753 train_acc= 0.26444 val_loss= 8.30211 val_acc= 0.26607 time= 10.46799\n",
      "Epoch: 0885 train_loss= 1.74950 train_acc= 0.23363 val_loss= 8.29636 val_acc= 0.26992 time= 10.30150\n",
      "Epoch: 0886 train_loss= 1.83646 train_acc= 0.23492 val_loss= 8.29235 val_acc= 0.27506 time= 10.30375\n",
      "Epoch: 0887 train_loss= 1.76505 train_acc= 0.25802 val_loss= 8.28944 val_acc= 0.29434 time= 10.28981\n",
      "Epoch: 0888 train_loss= 1.70378 train_acc= 0.24775 val_loss= 8.28913 val_acc= 0.32519 time= 10.62414\n",
      "Epoch: 0889 train_loss= 1.82319 train_acc= 0.27728 val_loss= 8.29040 val_acc= 0.34961 time= 10.35775\n",
      "Epoch: 0890 train_loss= 1.80104 train_acc= 0.25160 val_loss= 8.29101 val_acc= 0.37018 time= 10.34903\n",
      "Epoch: 0891 train_loss= 1.81609 train_acc= 0.24005 val_loss= 8.29047 val_acc= 0.37789 time= 10.28250\n",
      "Epoch: 0892 train_loss= 1.94523 train_acc= 0.24904 val_loss= 8.28859 val_acc= 0.37661 time= 10.85256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0893 train_loss= 1.85324 train_acc= 0.26316 val_loss= 8.28560 val_acc= 0.34447 time= 10.28629\n",
      "Epoch: 0894 train_loss= 1.77404 train_acc= 0.27214 val_loss= 8.28188 val_acc= 0.30206 time= 10.29486\n",
      "Epoch: 0895 train_loss= 1.63431 train_acc= 0.25931 val_loss= 8.27924 val_acc= 0.26735 time= 10.28190\n",
      "Epoch: 0896 train_loss= 1.78944 train_acc= 0.27086 val_loss= 8.27581 val_acc= 0.23650 time= 10.45537\n",
      "Epoch: 0897 train_loss= 1.76621 train_acc= 0.26701 val_loss= 8.27262 val_acc= 0.21080 time= 10.41459\n",
      "Epoch: 0898 train_loss= 1.76225 train_acc= 0.22978 val_loss= 8.26880 val_acc= 0.19023 time= 10.32348\n",
      "Epoch: 0899 train_loss= 1.79644 train_acc= 0.23620 val_loss= 8.26660 val_acc= 0.17609 time= 10.29028\n",
      "Epoch: 0900 train_loss= 1.68245 train_acc= 0.26187 val_loss= 8.26348 val_acc= 0.17352 time= 10.39060\n",
      "Epoch: 0901 train_loss= 1.77838 train_acc= 0.26316 val_loss= 8.26068 val_acc= 0.16710 time= 10.43485\n",
      "Epoch: 0902 train_loss= 1.75080 train_acc= 0.25032 val_loss= 8.25761 val_acc= 0.17224 time= 10.52771\n",
      "Epoch: 0903 train_loss= 1.68272 train_acc= 0.26444 val_loss= 8.25493 val_acc= 0.18766 time= 10.35426\n",
      "Epoch: 0904 train_loss= 1.74737 train_acc= 0.23363 val_loss= 8.25337 val_acc= 0.22365 time= 10.28433\n",
      "Epoch: 0905 train_loss= 1.69032 train_acc= 0.25802 val_loss= 8.25203 val_acc= 0.24679 time= 10.50237\n",
      "Epoch: 0906 train_loss= 1.76141 train_acc= 0.26059 val_loss= 8.25119 val_acc= 0.29692 time= 10.30275\n",
      "Epoch: 0907 train_loss= 1.76553 train_acc= 0.22978 val_loss= 8.24942 val_acc= 0.32391 time= 10.25771\n",
      "Epoch: 0908 train_loss= 1.97237 train_acc= 0.24262 val_loss= 8.24832 val_acc= 0.37404 time= 10.34118\n",
      "Epoch: 0909 train_loss= 1.75042 train_acc= 0.27086 val_loss= 8.24713 val_acc= 0.42802 time= 10.28305\n",
      "Epoch: 0910 train_loss= 1.69187 train_acc= 0.24390 val_loss= 8.24677 val_acc= 0.48072 time= 10.26925\n",
      "Epoch: 0911 train_loss= 1.73703 train_acc= 0.25802 val_loss= 8.24619 val_acc= 0.50643 time= 10.29108\n",
      "Epoch: 0912 train_loss= 1.69090 train_acc= 0.26444 val_loss= 8.24563 val_acc= 0.55527 time= 10.24252\n",
      "Epoch: 0913 train_loss= 1.75402 train_acc= 0.26829 val_loss= 8.24535 val_acc= 0.60668 time= 10.28222\n",
      "Epoch: 0914 train_loss= 1.84660 train_acc= 0.26316 val_loss= 8.24500 val_acc= 0.62982 time= 10.41187\n",
      "Epoch: 0915 train_loss= 1.80206 train_acc= 0.26829 val_loss= 8.24257 val_acc= 0.62082 time= 10.41377\n",
      "Epoch: 0916 train_loss= 1.70046 train_acc= 0.27599 val_loss= 8.24139 val_acc= 0.59512 time= 10.28928\n",
      "Epoch: 0917 train_loss= 1.69996 train_acc= 0.25931 val_loss= 8.24222 val_acc= 0.52956 time= 10.46302\n",
      "Epoch: 0918 train_loss= 1.74499 train_acc= 0.26059 val_loss= 8.24360 val_acc= 0.48586 time= 10.38645\n",
      "Epoch: 0919 train_loss= 1.77292 train_acc= 0.24262 val_loss= 8.24434 val_acc= 0.46015 time= 10.33951\n",
      "Epoch: 0920 train_loss= 1.84459 train_acc= 0.27985 val_loss= 8.24541 val_acc= 0.42802 time= 10.43829\n",
      "Epoch: 0921 train_loss= 1.73549 train_acc= 0.22721 val_loss= 8.24650 val_acc= 0.42031 time= 10.98744\n",
      "Epoch: 0922 train_loss= 1.74099 train_acc= 0.27086 val_loss= 8.24670 val_acc= 0.44602 time= 10.26001\n",
      "Epoch: 0923 train_loss= 1.73078 train_acc= 0.27214 val_loss= 8.24412 val_acc= 0.48843 time= 10.28378\n",
      "Epoch: 0924 train_loss= 1.73053 train_acc= 0.24647 val_loss= 8.23841 val_acc= 0.53728 time= 10.25129\n",
      "Epoch: 0925 train_loss= 1.67232 train_acc= 0.27471 val_loss= 8.23178 val_acc= 0.60540 time= 10.28099\n",
      "Epoch: 0926 train_loss= 1.69940 train_acc= 0.25417 val_loss= 8.22814 val_acc= 0.66838 time= 10.40877\n",
      "Epoch: 0927 train_loss= 1.78452 train_acc= 0.25160 val_loss= 8.22443 val_acc= 0.70437 time= 10.30134\n",
      "Epoch: 0928 train_loss= 1.71719 train_acc= 0.25289 val_loss= 8.21965 val_acc= 0.73650 time= 10.66068\n",
      "Epoch: 0929 train_loss= 1.68030 train_acc= 0.26059 val_loss= 8.21576 val_acc= 0.74550 time= 10.46435\n",
      "Epoch: 0930 train_loss= 1.77044 train_acc= 0.24005 val_loss= 8.21230 val_acc= 0.73779 time= 10.54441\n",
      "Epoch: 0931 train_loss= 1.69517 train_acc= 0.23492 val_loss= 8.20870 val_acc= 0.72494 time= 10.28451\n",
      "Epoch: 0932 train_loss= 1.69230 train_acc= 0.25931 val_loss= 8.20504 val_acc= 0.67352 time= 10.36984\n",
      "Epoch: 0933 train_loss= 1.85330 train_acc= 0.26316 val_loss= 8.20330 val_acc= 0.59255 time= 10.31148\n",
      "Epoch: 0934 train_loss= 1.76919 train_acc= 0.28113 val_loss= 8.20353 val_acc= 0.53213 time= 10.31187\n",
      "Epoch: 0935 train_loss= 1.70097 train_acc= 0.26573 val_loss= 8.20499 val_acc= 0.45887 time= 10.31087\n",
      "Epoch: 0936 train_loss= 1.77048 train_acc= 0.27086 val_loss= 8.20779 val_acc= 0.42802 time= 10.28425\n",
      "Epoch: 0937 train_loss= 1.76307 train_acc= 0.27985 val_loss= 8.21111 val_acc= 0.40617 time= 11.92320\n",
      "Epoch: 0938 train_loss= 1.90045 train_acc= 0.24262 val_loss= 8.21432 val_acc= 0.40231 time= 13.42556\n",
      "Epoch: 0939 train_loss= 1.72781 train_acc= 0.25802 val_loss= 8.21667 val_acc= 0.38946 time= 11.37549\n",
      "Epoch: 0940 train_loss= 1.65850 train_acc= 0.26444 val_loss= 8.21574 val_acc= 0.42031 time= 10.29469\n",
      "Epoch: 0941 train_loss= 1.77383 train_acc= 0.26958 val_loss= 8.21372 val_acc= 0.44344 time= 10.32951\n",
      "Epoch: 0942 train_loss= 1.75293 train_acc= 0.26958 val_loss= 8.21091 val_acc= 0.44987 time= 10.27809\n",
      "Epoch: 0943 train_loss= 1.71952 train_acc= 0.26316 val_loss= 8.20843 val_acc= 0.49229 time= 10.32432\n",
      "Epoch: 0944 train_loss= 1.84675 train_acc= 0.26059 val_loss= 8.20500 val_acc= 0.52828 time= 10.31378\n",
      "Epoch: 0945 train_loss= 1.70975 train_acc= 0.27214 val_loss= 8.20218 val_acc= 0.59769 time= 10.32812\n",
      "Epoch: 0946 train_loss= 1.66225 train_acc= 0.26187 val_loss= 8.19991 val_acc= 0.64781 time= 10.33181\n",
      "Epoch: 0947 train_loss= 1.69131 train_acc= 0.25931 val_loss= 8.19868 val_acc= 0.70823 time= 10.30523\n",
      "Epoch: 0948 train_loss= 1.70069 train_acc= 0.26187 val_loss= 8.19816 val_acc= 0.74293 time= 10.27116\n",
      "Epoch: 0949 train_loss= 1.78512 train_acc= 0.25417 val_loss= 8.19722 val_acc= 0.78535 time= 10.79789\n",
      "Epoch: 0950 train_loss= 1.69540 train_acc= 0.24647 val_loss= 8.19694 val_acc= 0.82005 time= 10.59366\n",
      "Epoch: 0951 train_loss= 1.67210 train_acc= 0.25546 val_loss= 8.19686 val_acc= 0.83162 time= 10.36931\n",
      "Epoch: 0952 train_loss= 1.66593 train_acc= 0.24647 val_loss= 8.19578 val_acc= 0.82776 time= 10.26659\n",
      "Epoch: 0953 train_loss= 1.69310 train_acc= 0.24519 val_loss= 8.19461 val_acc= 0.80591 time= 10.26160\n",
      "Epoch: 0954 train_loss= 1.74210 train_acc= 0.25546 val_loss= 8.19171 val_acc= 0.77378 time= 10.28974\n",
      "Epoch: 0955 train_loss= 1.72258 train_acc= 0.26316 val_loss= 8.18821 val_acc= 0.71851 time= 10.34706\n",
      "Epoch: 0956 train_loss= 1.75483 train_acc= 0.25674 val_loss= 8.18431 val_acc= 0.65810 time= 10.35740\n",
      "Epoch: 0957 train_loss= 1.72186 train_acc= 0.24005 val_loss= 8.18043 val_acc= 0.54242 time= 10.26298\n",
      "Epoch: 0958 train_loss= 1.67622 train_acc= 0.25674 val_loss= 8.17802 val_acc= 0.45630 time= 10.28089\n",
      "Epoch: 0959 train_loss= 1.69350 train_acc= 0.24775 val_loss= 8.17682 val_acc= 0.39332 time= 10.25858\n",
      "Epoch: 0960 train_loss= 1.75199 train_acc= 0.26573 val_loss= 8.17509 val_acc= 0.31362 time= 10.26235\n",
      "Epoch: 0961 train_loss= 1.71834 train_acc= 0.24390 val_loss= 8.17386 val_acc= 0.28021 time= 10.26294\n",
      "Epoch: 0962 train_loss= 1.70064 train_acc= 0.26573 val_loss= 8.17362 val_acc= 0.26992 time= 10.36914\n",
      "Epoch: 0963 train_loss= 1.78783 train_acc= 0.24904 val_loss= 8.17269 val_acc= 0.28149 time= 10.34031\n",
      "Epoch: 0964 train_loss= 1.71046 train_acc= 0.24005 val_loss= 8.17204 val_acc= 0.32391 time= 10.25663\n",
      "Epoch: 0965 train_loss= 1.74553 train_acc= 0.25931 val_loss= 8.17155 val_acc= 0.36247 time= 10.25818\n",
      "Epoch: 0966 train_loss= 1.67842 train_acc= 0.28755 val_loss= 8.16919 val_acc= 0.40617 time= 10.26173\n",
      "Epoch: 0967 train_loss= 1.62394 train_acc= 0.25546 val_loss= 8.16760 val_acc= 0.42931 time= 10.26359\n",
      "Epoch: 0968 train_loss= 1.71676 train_acc= 0.27728 val_loss= 8.16685 val_acc= 0.46144 time= 10.35077\n",
      "Epoch: 0969 train_loss= 1.64892 train_acc= 0.27086 val_loss= 8.16615 val_acc= 0.46272 time= 10.26602\n",
      "Epoch: 0970 train_loss= 1.76353 train_acc= 0.26573 val_loss= 8.16511 val_acc= 0.48586 time= 10.25675\n",
      "Epoch: 0971 train_loss= 1.71910 train_acc= 0.24390 val_loss= 8.16512 val_acc= 0.49743 time= 10.25542\n",
      "Epoch: 0972 train_loss= 1.69404 train_acc= 0.24775 val_loss= 8.16482 val_acc= 0.48586 time= 10.27236\n",
      "Epoch: 0973 train_loss= 2.00516 train_acc= 0.24647 val_loss= 8.16179 val_acc= 0.42416 time= 10.25104\n",
      "Epoch: 0974 train_loss= 1.73582 train_acc= 0.25160 val_loss= 8.15820 val_acc= 0.36375 time= 10.34786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0975 train_loss= 1.66644 train_acc= 0.24904 val_loss= 8.15554 val_acc= 0.37147 time= 10.36219\n",
      "Epoch: 0976 train_loss= 1.70887 train_acc= 0.27471 val_loss= 8.15446 val_acc= 0.42674 time= 10.26169\n",
      "Epoch: 0977 train_loss= 1.82456 train_acc= 0.27086 val_loss= 8.15392 val_acc= 0.56941 time= 10.28230\n",
      "Epoch: 0978 train_loss= 1.70901 train_acc= 0.24262 val_loss= 8.15547 val_acc= 0.67738 time= 10.80253\n",
      "Epoch: 0979 train_loss= 1.70839 train_acc= 0.27599 val_loss= 8.15705 val_acc= 0.73265 time= 10.44877\n",
      "Epoch: 0980 train_loss= 1.66313 train_acc= 0.26829 val_loss= 8.15914 val_acc= 0.76093 time= 10.36228\n",
      "Epoch: 0981 train_loss= 1.70536 train_acc= 0.28113 val_loss= 8.16130 val_acc= 0.77506 time= 10.25504\n",
      "Epoch: 0982 train_loss= 1.67238 train_acc= 0.27728 val_loss= 8.16364 val_acc= 0.79177 time= 10.23323\n",
      "Epoch: 0983 train_loss= 1.73334 train_acc= 0.24647 val_loss= 8.16528 val_acc= 0.79820 time= 10.32583\n",
      "Epoch: 0984 train_loss= 1.66144 train_acc= 0.25546 val_loss= 8.16632 val_acc= 0.79049 time= 10.25775\n",
      "Epoch: 0985 train_loss= 1.73544 train_acc= 0.24904 val_loss= 8.16407 val_acc= 0.76992 time= 10.22996\n",
      "Epoch: 0986 train_loss= 1.77486 train_acc= 0.25289 val_loss= 8.16421 val_acc= 0.77121 time= 10.22891\n",
      "Epoch: 0987 train_loss= 1.76023 train_acc= 0.25289 val_loss= 8.16380 val_acc= 0.77892 time= 10.40159\n",
      "Epoch: 0988 train_loss= 1.72490 train_acc= 0.25289 val_loss= 8.16344 val_acc= 0.78535 time= 10.40488\n",
      "Epoch: 0989 train_loss= 1.65295 train_acc= 0.24262 val_loss= 8.16429 val_acc= 0.78920 time= 10.28516\n",
      "Epoch: 0990 train_loss= 1.71670 train_acc= 0.25032 val_loss= 8.16467 val_acc= 0.77892 time= 10.32458\n",
      "Epoch: 0991 train_loss= 1.70147 train_acc= 0.26573 val_loss= 8.16494 val_acc= 0.73265 time= 10.34067\n",
      "Epoch: 0992 train_loss= 1.67705 train_acc= 0.22850 val_loss= 8.16680 val_acc= 0.66710 time= 10.33220\n",
      "Epoch: 0993 train_loss= 1.77721 train_acc= 0.27856 val_loss= 8.17039 val_acc= 0.66710 time= 10.35553\n",
      "Epoch: 0994 train_loss= 1.67981 train_acc= 0.25160 val_loss= 8.17281 val_acc= 0.61825 time= 10.25986\n",
      "Epoch: 0995 train_loss= 1.71377 train_acc= 0.24519 val_loss= 8.17609 val_acc= 0.59255 time= 10.28623\n",
      "Epoch: 0996 train_loss= 1.70792 train_acc= 0.25289 val_loss= 8.18094 val_acc= 0.57841 time= 10.28214\n",
      "Epoch: 0997 train_loss= 1.71578 train_acc= 0.24262 val_loss= 8.18588 val_acc= 0.63368 time= 10.25409\n",
      "Epoch: 0998 train_loss= 1.66029 train_acc= 0.23877 val_loss= 8.19042 val_acc= 0.66067 time= 10.33332\n",
      "Epoch: 0999 train_loss= 1.65511 train_acc= 0.25931 val_loss= 8.19537 val_acc= 0.67481 time= 10.36726\n",
      "Epoch: 1000 train_loss= 1.73318 train_acc= 0.25417 val_loss= 8.19843 val_acc= 0.63368 time= 10.29242\n",
      "Epoch: 1001 train_loss= 1.67312 train_acc= 0.27856 val_loss= 8.20057 val_acc= 0.56941 time= 10.30616\n",
      "Epoch: 1002 train_loss= 1.76933 train_acc= 0.27214 val_loss= 8.20076 val_acc= 0.52828 time= 10.26130\n",
      "Epoch: 1003 train_loss= 1.67338 train_acc= 0.24390 val_loss= 8.19694 val_acc= 0.44730 time= 10.48479\n",
      "Epoch: 1004 train_loss= 1.66396 train_acc= 0.26316 val_loss= 8.19295 val_acc= 0.40617 time= 10.31109\n",
      "Epoch: 1005 train_loss= 1.67831 train_acc= 0.24519 val_loss= 8.18969 val_acc= 0.44602 time= 10.40337\n",
      "Epoch: 1006 train_loss= 1.78675 train_acc= 0.24775 val_loss= 8.18620 val_acc= 0.51157 time= 10.34580\n",
      "Epoch: 1007 train_loss= 1.70884 train_acc= 0.27086 val_loss= 8.18368 val_acc= 0.57455 time= 10.47474\n",
      "Epoch: 1008 train_loss= 1.72826 train_acc= 0.23492 val_loss= 8.18258 val_acc= 0.66195 time= 11.00902\n",
      "Epoch: 1009 train_loss= 1.73634 train_acc= 0.26316 val_loss= 8.17999 val_acc= 0.71851 time= 10.30029\n",
      "Epoch: 1010 train_loss= 1.71437 train_acc= 0.25032 val_loss= 8.17779 val_acc= 0.73779 time= 10.24947\n",
      "Epoch: 1011 train_loss= 1.71553 train_acc= 0.25289 val_loss= 8.17587 val_acc= 0.73008 time= 10.44056\n",
      "Epoch: 1012 train_loss= 1.66679 train_acc= 0.25289 val_loss= 8.17341 val_acc= 0.74422 time= 10.30008\n",
      "Epoch: 1013 train_loss= 1.65846 train_acc= 0.23620 val_loss= 8.17241 val_acc= 0.78406 time= 10.34772\n",
      "Epoch: 1014 train_loss= 1.71068 train_acc= 0.26958 val_loss= 8.17343 val_acc= 0.80206 time= 10.42058\n",
      "Epoch: 1015 train_loss= 1.68218 train_acc= 0.25160 val_loss= 8.17533 val_acc= 0.77121 time= 10.30260\n",
      "Epoch: 1016 train_loss= 1.72993 train_acc= 0.23107 val_loss= 8.17690 val_acc= 0.74036 time= 10.31763\n",
      "Epoch: 1017 train_loss= 1.70463 train_acc= 0.25417 val_loss= 8.17917 val_acc= 0.73650 time= 10.35583\n",
      "Epoch: 1018 train_loss= 1.68751 train_acc= 0.23748 val_loss= 8.18033 val_acc= 0.73779 time= 10.43641\n",
      "Epoch: 1019 train_loss= 1.69907 train_acc= 0.25674 val_loss= 8.17916 val_acc= 0.74165 time= 10.32284\n",
      "Epoch: 1020 train_loss= 1.68297 train_acc= 0.23748 val_loss= 8.17843 val_acc= 0.78406 time= 10.43312\n",
      "Epoch: 1021 train_loss= 1.69547 train_acc= 0.25289 val_loss= 8.17722 val_acc= 0.81362 time= 10.33297\n",
      "Epoch: 1022 train_loss= 1.65261 train_acc= 0.23235 val_loss= 8.17552 val_acc= 0.83033 time= 10.31682\n",
      "Epoch: 1023 train_loss= 1.77882 train_acc= 0.25546 val_loss= 8.17418 val_acc= 0.82905 time= 10.41450\n",
      "Epoch: 1024 train_loss= 1.69692 train_acc= 0.25931 val_loss= 8.17191 val_acc= 0.80977 time= 10.36176\n",
      "Epoch: 1025 train_loss= 1.75175 train_acc= 0.25160 val_loss= 8.16907 val_acc= 0.78792 time= 10.38496\n",
      "Epoch: 1026 train_loss= 1.78713 train_acc= 0.25160 val_loss= 8.16399 val_acc= 0.74036 time= 10.41052\n",
      "Epoch: 1027 train_loss= 1.64700 train_acc= 0.24775 val_loss= 8.15916 val_acc= 0.67481 time= 10.34706\n",
      "Epoch: 1028 train_loss= 1.69022 train_acc= 0.25674 val_loss= 8.15473 val_acc= 0.50386 time= 10.42452\n",
      "Epoch: 1029 train_loss= 1.71041 train_acc= 0.25032 val_loss= 8.14986 val_acc= 0.29563 time= 10.51620\n",
      "Epoch: 1030 train_loss= 1.71423 train_acc= 0.25546 val_loss= 8.14719 val_acc= 0.15167 time= 10.44830\n",
      "Epoch: 1031 train_loss= 1.73096 train_acc= 0.26701 val_loss= 8.14437 val_acc= 0.09897 time= 10.33016\n",
      "Epoch: 1032 train_loss= 1.64600 train_acc= 0.24775 val_loss= 8.14306 val_acc= 0.10026 time= 11.80470\n",
      "Epoch: 1033 train_loss= 1.67012 train_acc= 0.25674 val_loss= 8.14191 val_acc= 0.12982 time= 11.59463\n",
      "Epoch: 1034 train_loss= 1.72440 train_acc= 0.24005 val_loss= 8.14068 val_acc= 0.16710 time= 10.34528\n",
      "Epoch: 1035 train_loss= 1.67496 train_acc= 0.27985 val_loss= 8.14005 val_acc= 0.23136 time= 10.53186\n",
      "Epoch: 1036 train_loss= 1.67359 train_acc= 0.26829 val_loss= 8.14019 val_acc= 0.33548 time= 10.97233\n",
      "Epoch: 1037 train_loss= 1.72862 train_acc= 0.28883 val_loss= 8.14069 val_acc= 0.41517 time= 10.35484\n",
      "Epoch: 1038 train_loss= 1.66787 train_acc= 0.25674 val_loss= 8.14132 val_acc= 0.49743 time= 10.31581\n",
      "Epoch: 1039 train_loss= 1.64582 train_acc= 0.26701 val_loss= 8.14236 val_acc= 0.57198 time= 10.31032\n",
      "Epoch: 1040 train_loss= 1.66635 train_acc= 0.27728 val_loss= 8.14475 val_acc= 0.66324 time= 10.31538\n",
      "Epoch: 1041 train_loss= 1.64860 train_acc= 0.26444 val_loss= 8.14851 val_acc= 0.73008 time= 10.61963\n",
      "Epoch: 1042 train_loss= 1.69410 train_acc= 0.26444 val_loss= 8.15339 val_acc= 0.79306 time= 10.31987\n",
      "Epoch: 1043 train_loss= 1.66448 train_acc= 0.26701 val_loss= 8.15684 val_acc= 0.82262 time= 10.28870\n",
      "Epoch: 1044 train_loss= 1.70352 train_acc= 0.25160 val_loss= 8.15969 val_acc= 0.83162 time= 10.30724\n",
      "Epoch: 1045 train_loss= 1.66873 train_acc= 0.26573 val_loss= 8.16202 val_acc= 0.84062 time= 10.30940\n",
      "Epoch: 1046 train_loss= 1.70301 train_acc= 0.25289 val_loss= 8.16732 val_acc= 0.86375 time= 10.28285\n",
      "Epoch: 1047 train_loss= 1.69507 train_acc= 0.25032 val_loss= 8.17306 val_acc= 0.88689 time= 10.47151\n",
      "Epoch: 1048 train_loss= 1.66228 train_acc= 0.25546 val_loss= 8.17646 val_acc= 0.90103 time= 10.31620\n",
      "Epoch: 1049 train_loss= 1.68801 train_acc= 0.23107 val_loss= 8.17698 val_acc= 0.90617 time= 10.30198\n",
      "Epoch: 1050 train_loss= 1.65419 train_acc= 0.25802 val_loss= 8.17603 val_acc= 0.88560 time= 10.31961\n",
      "Epoch: 1051 train_loss= 1.68095 train_acc= 0.25931 val_loss= 8.17315 val_acc= 0.85347 time= 10.32689\n",
      "Epoch: 1052 train_loss= 1.67967 train_acc= 0.25417 val_loss= 8.16922 val_acc= 0.80334 time= 10.30129\n",
      "Epoch: 1053 train_loss= 1.68012 train_acc= 0.23620 val_loss= 8.16447 val_acc= 0.74165 time= 10.49951\n",
      "Epoch: 1054 train_loss= 1.73288 train_acc= 0.25289 val_loss= 8.15993 val_acc= 0.68380 time= 10.30909\n",
      "Epoch: 1055 train_loss= 1.71030 train_acc= 0.25931 val_loss= 8.15465 val_acc= 0.66967 time= 10.33755\n",
      "Epoch: 1056 train_loss= 1.73503 train_acc= 0.26444 val_loss= 8.14997 val_acc= 0.62211 time= 10.29605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1057 train_loss= 1.68738 train_acc= 0.25802 val_loss= 8.14632 val_acc= 0.61697 time= 10.29607\n",
      "Epoch: 1058 train_loss= 1.68601 train_acc= 0.25417 val_loss= 8.14410 val_acc= 0.59383 time= 10.29175\n",
      "Epoch: 1059 train_loss= 1.69118 train_acc= 0.26187 val_loss= 8.14164 val_acc= 0.58226 time= 10.39904\n",
      "Epoch: 1060 train_loss= 1.71192 train_acc= 0.25289 val_loss= 8.13820 val_acc= 0.58997 time= 10.46106\n",
      "Epoch: 1061 train_loss= 1.69852 train_acc= 0.25674 val_loss= 8.13494 val_acc= 0.51671 time= 11.17086\n",
      "Epoch: 1062 train_loss= 1.67968 train_acc= 0.28883 val_loss= 8.13225 val_acc= 0.40874 time= 10.49864\n",
      "Epoch: 1063 train_loss= 1.71334 train_acc= 0.26444 val_loss= 8.12974 val_acc= 0.37661 time= 10.71889\n",
      "Epoch: 1064 train_loss= 1.66833 train_acc= 0.24647 val_loss= 8.12843 val_acc= 0.36889 time= 10.62961\n",
      "Epoch: 1065 train_loss= 1.64975 train_acc= 0.26958 val_loss= 8.12828 val_acc= 0.31491 time= 11.31230\n",
      "Epoch: 1066 train_loss= 1.70520 train_acc= 0.27985 val_loss= 8.13043 val_acc= 0.38560 time= 10.60775\n",
      "Epoch: 1067 train_loss= 1.63207 train_acc= 0.27214 val_loss= 8.13159 val_acc= 0.48329 time= 10.97900\n",
      "Epoch: 1068 train_loss= 1.75798 train_acc= 0.25931 val_loss= 8.13154 val_acc= 0.58226 time= 11.00073\n",
      "Epoch: 1069 train_loss= 1.71242 train_acc= 0.24647 val_loss= 8.13192 val_acc= 0.64139 time= 10.71894\n",
      "Epoch: 1070 train_loss= 1.75176 train_acc= 0.24775 val_loss= 8.13237 val_acc= 0.72494 time= 10.78800\n",
      "Epoch: 1071 train_loss= 1.67701 train_acc= 0.26187 val_loss= 8.13381 val_acc= 0.82391 time= 11.25578\n",
      "Epoch: 1072 train_loss= 1.71141 train_acc= 0.23492 val_loss= 8.13630 val_acc= 0.90488 time= 10.53859\n",
      "Epoch: 1073 train_loss= 1.63177 train_acc= 0.25674 val_loss= 8.13714 val_acc= 0.93959 time= 10.25984\n",
      "Epoch: 1074 train_loss= 1.65694 train_acc= 0.25802 val_loss= 8.13803 val_acc= 0.96401 time= 10.51451\n",
      "Epoch: 1075 train_loss= 1.67959 train_acc= 0.24647 val_loss= 8.13800 val_acc= 0.97044 time= 10.32199\n",
      "Epoch: 1076 train_loss= 1.66135 train_acc= 0.24775 val_loss= 8.13823 val_acc= 0.97558 time= 10.28922\n",
      "Epoch: 1077 train_loss= 1.70387 train_acc= 0.25160 val_loss= 8.13883 val_acc= 0.97558 time= 10.76075\n",
      "Epoch: 1078 train_loss= 1.65053 train_acc= 0.23620 val_loss= 8.13698 val_acc= 0.96915 time= 10.86110\n",
      "Epoch: 1079 train_loss= 1.68609 train_acc= 0.23877 val_loss= 8.13561 val_acc= 0.94730 time= 10.61612\n",
      "Epoch: 1080 train_loss= 1.71395 train_acc= 0.25674 val_loss= 8.13427 val_acc= 0.94216 time= 11.47764\n",
      "Epoch: 1081 train_loss= 1.69136 train_acc= 0.24005 val_loss= 8.13288 val_acc= 0.93059 time= 11.65397\n",
      "Epoch: 1082 train_loss= 1.67564 train_acc= 0.26316 val_loss= 8.13218 val_acc= 0.93188 time= 10.90221\n",
      "Epoch: 1083 train_loss= 1.64651 train_acc= 0.24005 val_loss= 8.13155 val_acc= 0.93316 time= 10.61918\n",
      "Epoch: 1084 train_loss= 1.66410 train_acc= 0.24390 val_loss= 8.13241 val_acc= 0.92931 time= 10.39026\n",
      "Epoch: 1085 train_loss= 1.72622 train_acc= 0.25802 val_loss= 8.13378 val_acc= 0.92674 time= 10.27934\n",
      "Epoch: 1086 train_loss= 1.65633 train_acc= 0.25032 val_loss= 8.13671 val_acc= 0.92031 time= 10.38395\n",
      "Epoch: 1087 train_loss= 1.66473 train_acc= 0.25417 val_loss= 8.13801 val_acc= 0.91645 time= 10.31565\n",
      "Epoch: 1088 train_loss= 1.66688 train_acc= 0.25160 val_loss= 8.13717 val_acc= 0.89203 time= 12.90730\n",
      "Epoch: 1089 train_loss= 1.67407 train_acc= 0.26958 val_loss= 8.13668 val_acc= 0.85733 time= 13.96176\n",
      "Epoch: 1090 train_loss= 1.65123 train_acc= 0.27599 val_loss= 8.13598 val_acc= 0.81362 time= 12.29205\n",
      "Epoch: 1091 train_loss= 1.63987 train_acc= 0.24775 val_loss= 8.13553 val_acc= 0.76735 time= 11.41962\n",
      "Epoch: 1092 train_loss= 1.72949 train_acc= 0.24775 val_loss= 8.13446 val_acc= 0.74422 time= 11.48236\n",
      "Epoch: 1093 train_loss= 1.76280 train_acc= 0.27471 val_loss= 8.13130 val_acc= 0.71722 time= 13.40060\n",
      "Epoch: 1094 train_loss= 1.66867 train_acc= 0.24519 val_loss= 8.12813 val_acc= 0.69280 time= 10.48104\n",
      "Epoch: 1095 train_loss= 1.68867 train_acc= 0.25289 val_loss= 8.12384 val_acc= 0.64139 time= 10.26025\n",
      "Epoch: 1096 train_loss= 1.65688 train_acc= 0.26958 val_loss= 8.12012 val_acc= 0.62853 time= 10.22293\n",
      "Epoch: 1097 train_loss= 1.71735 train_acc= 0.24134 val_loss= 8.11769 val_acc= 0.61183 time= 10.25390\n",
      "Epoch: 1098 train_loss= 1.65610 train_acc= 0.27728 val_loss= 8.11607 val_acc= 0.61054 time= 10.21590\n",
      "Epoch: 1099 train_loss= 1.75006 train_acc= 0.24647 val_loss= 8.11388 val_acc= 0.59383 time= 10.24115\n",
      "Epoch: 1100 train_loss= 1.76538 train_acc= 0.26573 val_loss= 8.11256 val_acc= 0.65424 time= 10.29436\n",
      "Epoch: 1101 train_loss= 1.64315 train_acc= 0.29910 val_loss= 8.11166 val_acc= 0.71337 time= 10.22320\n",
      "Epoch: 1102 train_loss= 1.65061 train_acc= 0.26829 val_loss= 8.11175 val_acc= 0.76735 time= 10.22332\n",
      "Epoch: 1103 train_loss= 1.64123 train_acc= 0.26187 val_loss= 8.11306 val_acc= 0.79177 time= 10.23355\n",
      "Epoch: 1104 train_loss= 1.67562 train_acc= 0.27728 val_loss= 8.11368 val_acc= 0.80206 time= 10.21909\n",
      "Epoch: 1105 train_loss= 1.62378 train_acc= 0.27214 val_loss= 8.11391 val_acc= 0.81105 time= 10.20916\n",
      "Epoch: 1106 train_loss= 1.64024 train_acc= 0.24904 val_loss= 8.11390 val_acc= 0.81362 time= 10.35014\n",
      "Epoch: 1107 train_loss= 1.74945 train_acc= 0.26187 val_loss= 8.11290 val_acc= 0.81234 time= 10.20161\n",
      "Epoch: 1108 train_loss= 1.67309 train_acc= 0.26316 val_loss= 8.11157 val_acc= 0.80977 time= 10.21518\n",
      "Epoch: 1109 train_loss= 1.64722 train_acc= 0.26444 val_loss= 8.10981 val_acc= 0.79049 time= 10.24211\n",
      "Epoch: 1110 train_loss= 1.66856 train_acc= 0.24005 val_loss= 8.10824 val_acc= 0.78535 time= 10.21132\n",
      "Epoch: 1111 train_loss= 1.66332 train_acc= 0.24775 val_loss= 8.10706 val_acc= 0.78406 time= 10.19720\n",
      "Epoch: 1112 train_loss= 1.66196 train_acc= 0.25546 val_loss= 8.10494 val_acc= 0.75193 time= 10.26509\n",
      "Epoch: 1113 train_loss= 1.65942 train_acc= 0.24519 val_loss= 8.10318 val_acc= 0.71851 time= 10.20157\n",
      "Epoch: 1114 train_loss= 1.63939 train_acc= 0.24262 val_loss= 8.10258 val_acc= 0.68509 time= 10.22529\n",
      "Epoch: 1115 train_loss= 1.62044 train_acc= 0.26701 val_loss= 8.10310 val_acc= 0.65296 time= 10.22829\n",
      "Epoch: 1116 train_loss= 1.66274 train_acc= 0.26316 val_loss= 8.10438 val_acc= 0.63753 time= 10.22596\n",
      "Epoch: 1117 train_loss= 1.67350 train_acc= 0.25160 val_loss= 8.10620 val_acc= 0.64267 time= 10.46881\n",
      "Epoch: 1118 train_loss= 1.64005 train_acc= 0.23235 val_loss= 8.10806 val_acc= 0.64010 time= 10.39253\n",
      "Epoch: 1119 train_loss= 1.66731 train_acc= 0.24647 val_loss= 8.10914 val_acc= 0.63882 time= 10.21458\n",
      "Epoch: 1120 train_loss= 1.67979 train_acc= 0.24519 val_loss= 8.10992 val_acc= 0.67738 time= 10.21817\n",
      "Epoch: 1121 train_loss= 1.67592 train_acc= 0.26958 val_loss= 8.10944 val_acc= 0.72494 time= 10.22536\n",
      "Epoch: 1122 train_loss= 1.70260 train_acc= 0.24519 val_loss= 8.10767 val_acc= 0.76607 time= 10.87895\n",
      "Epoch: 1123 train_loss= 1.67127 train_acc= 0.25674 val_loss= 8.10757 val_acc= 0.81362 time= 10.20938\n",
      "Epoch: 1124 train_loss= 1.66084 train_acc= 0.24005 val_loss= 8.10867 val_acc= 0.85990 time= 10.31984\n",
      "Epoch: 1125 train_loss= 1.66423 train_acc= 0.24904 val_loss= 8.10896 val_acc= 0.88432 time= 10.22246\n",
      "Epoch: 1126 train_loss= 1.69823 train_acc= 0.24262 val_loss= 8.10918 val_acc= 0.91645 time= 10.23558\n",
      "Epoch: 1127 train_loss= 1.65953 train_acc= 0.25674 val_loss= 8.10954 val_acc= 0.93702 time= 10.19908\n",
      "Epoch: 1128 train_loss= 1.66062 train_acc= 0.23107 val_loss= 8.10880 val_acc= 0.94859 time= 10.23683\n",
      "Epoch: 1129 train_loss= 1.66111 train_acc= 0.24775 val_loss= 8.10737 val_acc= 0.94730 time= 10.23646\n",
      "Epoch: 1130 train_loss= 1.72704 train_acc= 0.23877 val_loss= 8.10530 val_acc= 0.92416 time= 10.28656\n",
      "Epoch: 1131 train_loss= 1.67296 train_acc= 0.24005 val_loss= 8.10293 val_acc= 0.91003 time= 10.21651\n",
      "Epoch: 1132 train_loss= 1.65721 train_acc= 0.24134 val_loss= 8.10005 val_acc= 0.89203 time= 10.25791\n",
      "Epoch: 1133 train_loss= 1.63865 train_acc= 0.24647 val_loss= 8.09794 val_acc= 0.86889 time= 10.21023\n",
      "Epoch: 1134 train_loss= 1.67620 train_acc= 0.24647 val_loss= 8.09651 val_acc= 0.85219 time= 10.22833\n",
      "Epoch: 1135 train_loss= 1.69234 train_acc= 0.23748 val_loss= 8.09514 val_acc= 0.83933 time= 10.28435\n",
      "Epoch: 1136 train_loss= 1.65968 train_acc= 0.25160 val_loss= 8.09426 val_acc= 0.79563 time= 10.59219\n",
      "Epoch: 1137 train_loss= 1.69729 train_acc= 0.24904 val_loss= 8.09381 val_acc= 0.77378 time= 10.19723\n",
      "Epoch: 1138 train_loss= 1.65933 train_acc= 0.26701 val_loss= 8.09428 val_acc= 0.75450 time= 10.17488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1139 train_loss= 1.64757 train_acc= 0.24262 val_loss= 8.09408 val_acc= 0.77763 time= 10.13524\n",
      "Epoch: 1140 train_loss= 1.67893 train_acc= 0.22978 val_loss= 8.09423 val_acc= 0.80334 time= 10.10948\n",
      "Epoch: 1141 train_loss= 1.65740 train_acc= 0.24390 val_loss= 8.09485 val_acc= 0.85990 time= 10.13315\n",
      "Epoch: 1142 train_loss= 1.67990 train_acc= 0.24775 val_loss= 8.09564 val_acc= 0.88175 time= 10.14095\n",
      "Epoch: 1143 train_loss= 1.66674 train_acc= 0.24775 val_loss= 8.09606 val_acc= 0.89460 time= 10.34600\n",
      "Epoch: 1144 train_loss= 1.65200 train_acc= 0.25032 val_loss= 8.09674 val_acc= 0.88689 time= 10.16376\n",
      "Epoch: 1145 train_loss= 1.68875 train_acc= 0.24262 val_loss= 8.09741 val_acc= 0.85990 time= 10.12963\n",
      "Epoch: 1146 train_loss= 1.65884 train_acc= 0.25160 val_loss= 8.09973 val_acc= 0.82005 time= 10.21808\n",
      "Epoch: 1147 train_loss= 1.67354 train_acc= 0.25546 val_loss= 8.10087 val_acc= 0.76992 time= 10.14880\n",
      "Epoch: 1148 train_loss= 1.63591 train_acc= 0.25674 val_loss= 8.10179 val_acc= 0.74293 time= 10.11935\n",
      "Epoch: 1149 train_loss= 1.71630 train_acc= 0.24005 val_loss= 8.10172 val_acc= 0.72108 time= 10.29790\n",
      "Epoch: 1150 train_loss= 1.65846 train_acc= 0.25032 val_loss= 8.10248 val_acc= 0.67095 time= 10.15876\n",
      "Epoch: 1151 train_loss= 1.64002 train_acc= 0.24904 val_loss= 8.10410 val_acc= 0.69280 time= 10.61599\n",
      "Epoch: 1152 train_loss= 1.68992 train_acc= 0.24134 val_loss= 8.10552 val_acc= 0.70437 time= 10.75093\n",
      "Epoch: 1153 train_loss= 1.64751 train_acc= 0.23877 val_loss= 8.10645 val_acc= 0.71979 time= 10.40292\n",
      "Epoch: 1154 train_loss= 1.64604 train_acc= 0.24519 val_loss= 8.10653 val_acc= 0.75707 time= 10.17989\n",
      "Epoch: 1155 train_loss= 1.71763 train_acc= 0.24519 val_loss= 8.10682 val_acc= 0.85604 time= 10.26617\n",
      "Epoch: 1156 train_loss= 1.73142 train_acc= 0.22850 val_loss= 8.10629 val_acc= 0.88303 time= 10.15758\n",
      "Epoch: 1157 train_loss= 1.69667 train_acc= 0.25289 val_loss= 8.10596 val_acc= 0.88560 time= 10.13198\n",
      "Epoch: 1158 train_loss= 1.65255 train_acc= 0.27086 val_loss= 8.10523 val_acc= 0.86632 time= 10.13658\n",
      "Epoch: 1159 train_loss= 1.67432 train_acc= 0.23492 val_loss= 8.10506 val_acc= 0.82648 time= 10.11452\n",
      "Epoch: 1160 train_loss= 1.64948 train_acc= 0.25032 val_loss= 8.10400 val_acc= 0.76350 time= 10.11745\n",
      "Epoch: 1161 train_loss= 1.68694 train_acc= 0.25289 val_loss= 8.10388 val_acc= 0.64139 time= 10.36596\n",
      "Epoch: 1162 train_loss= 1.63744 train_acc= 0.25417 val_loss= 8.10693 val_acc= 0.62725 time= 10.14520\n",
      "Epoch: 1163 train_loss= 1.68390 train_acc= 0.27728 val_loss= 8.10924 val_acc= 0.66581 time= 10.12174\n",
      "Epoch: 1164 train_loss= 1.68066 train_acc= 0.23492 val_loss= 8.11134 val_acc= 0.67609 time= 10.12854\n",
      "Epoch: 1165 train_loss= 1.67777 train_acc= 0.25032 val_loss= 8.11322 val_acc= 0.68638 time= 10.11141\n",
      "Epoch: 1166 train_loss= 1.71061 train_acc= 0.25802 val_loss= 8.11588 val_acc= 0.64653 time= 10.11347\n",
      "Epoch: 1167 train_loss= 1.65259 train_acc= 0.27471 val_loss= 8.12002 val_acc= 0.65039 time= 10.23178\n",
      "Epoch: 1168 train_loss= 1.63753 train_acc= 0.25546 val_loss= 8.12172 val_acc= 0.62339 time= 10.11802\n",
      "Epoch: 1169 train_loss= 1.66927 train_acc= 0.24134 val_loss= 8.12138 val_acc= 0.66452 time= 10.12673\n",
      "Epoch: 1170 train_loss= 1.65824 train_acc= 0.22978 val_loss= 8.12103 val_acc= 0.71979 time= 10.11808\n",
      "Epoch: 1171 train_loss= 1.65094 train_acc= 0.25289 val_loss= 8.12150 val_acc= 0.75835 time= 10.13308\n",
      "Epoch: 1172 train_loss= 1.64773 train_acc= 0.23877 val_loss= 8.12256 val_acc= 0.81620 time= 10.12348\n",
      "Epoch: 1173 train_loss= 1.65073 train_acc= 0.25931 val_loss= 8.12143 val_acc= 0.83805 time= 10.34077\n",
      "Epoch: 1174 train_loss= 1.67748 train_acc= 0.24904 val_loss= 8.11795 val_acc= 0.82391 time= 10.14012\n",
      "Epoch: 1175 train_loss= 1.65735 train_acc= 0.25160 val_loss= 8.11429 val_acc= 0.82519 time= 10.19665\n",
      "Epoch: 1176 train_loss= 1.71970 train_acc= 0.24904 val_loss= 8.11065 val_acc= 0.83548 time= 10.13319\n",
      "Epoch: 1177 train_loss= 1.70310 train_acc= 0.24005 val_loss= 8.10605 val_acc= 0.81234 time= 10.14229\n",
      "Epoch: 1178 train_loss= 1.63503 train_acc= 0.23877 val_loss= 8.10300 val_acc= 0.80591 time= 10.13217\n",
      "Epoch: 1179 train_loss= 1.77046 train_acc= 0.24904 val_loss= 8.10086 val_acc= 0.77121 time= 10.16052\n",
      "Epoch: 1180 train_loss= 1.67597 train_acc= 0.25289 val_loss= 8.10061 val_acc= 0.76221 time= 10.18500\n",
      "Epoch: 1181 train_loss= 1.65949 train_acc= 0.25802 val_loss= 8.10005 val_acc= 0.76093 time= 10.74560\n",
      "Epoch: 1182 train_loss= 1.68055 train_acc= 0.25674 val_loss= 8.09881 val_acc= 0.75193 time= 10.11904\n",
      "Epoch: 1183 train_loss= 1.62848 train_acc= 0.24519 val_loss= 8.09729 val_acc= 0.73779 time= 10.12511\n",
      "Epoch: 1184 train_loss= 1.69016 train_acc= 0.23877 val_loss= 8.09675 val_acc= 0.76864 time= 10.12750\n",
      "Epoch: 1185 train_loss= 1.71633 train_acc= 0.22721 val_loss= 8.09571 val_acc= 0.80463 time= 10.16212\n",
      "Epoch: 1186 train_loss= 1.67409 train_acc= 0.24647 val_loss= 8.09547 val_acc= 0.85733 time= 10.18394\n",
      "Epoch: 1187 train_loss= 1.68143 train_acc= 0.24005 val_loss= 8.09629 val_acc= 0.91131 time= 10.12036\n",
      "Epoch: 1188 train_loss= 1.70712 train_acc= 0.22978 val_loss= 8.09741 val_acc= 0.94216 time= 10.11082\n",
      "Epoch: 1189 train_loss= 1.69231 train_acc= 0.23492 val_loss= 8.09857 val_acc= 0.95887 time= 10.10652\n",
      "Epoch: 1190 train_loss= 1.66249 train_acc= 0.25674 val_loss= 8.09805 val_acc= 0.95887 time= 10.13355\n",
      "Epoch: 1191 train_loss= 1.67541 train_acc= 0.24134 val_loss= 8.09775 val_acc= 0.95501 time= 10.16602\n",
      "Epoch: 1192 train_loss= 1.63332 train_acc= 0.25032 val_loss= 8.09747 val_acc= 0.94216 time= 10.25005\n",
      "Epoch: 1193 train_loss= 1.63041 train_acc= 0.24775 val_loss= 8.09739 val_acc= 0.91902 time= 10.14686\n",
      "Epoch: 1194 train_loss= 1.63472 train_acc= 0.24262 val_loss= 8.09662 val_acc= 0.91388 time= 10.12483\n",
      "Epoch: 1195 train_loss= 1.63506 train_acc= 0.24390 val_loss= 8.09559 val_acc= 0.89846 time= 10.13552\n",
      "Epoch: 1196 train_loss= 1.68622 train_acc= 0.25032 val_loss= 8.09468 val_acc= 0.88432 time= 10.13716\n",
      "Epoch: 1197 train_loss= 1.64112 train_acc= 0.22850 val_loss= 8.09359 val_acc= 0.83933 time= 10.16907\n",
      "Epoch: 1198 train_loss= 1.62882 train_acc= 0.24904 val_loss= 8.09285 val_acc= 0.82776 time= 10.20628\n",
      "Epoch: 1199 train_loss= 1.66126 train_acc= 0.25931 val_loss= 8.09217 val_acc= 0.81877 time= 10.11606\n",
      "Epoch: 1200 train_loss= 1.66288 train_acc= 0.26444 val_loss= 8.09141 val_acc= 0.80077 time= 10.13171\n",
      "Epoch: 1201 train_loss= 1.64424 train_acc= 0.25289 val_loss= 8.09141 val_acc= 0.79692 time= 10.13431\n",
      "Epoch: 1202 train_loss= 1.67479 train_acc= 0.24262 val_loss= 8.09088 val_acc= 0.79306 time= 10.14047\n",
      "Epoch: 1203 train_loss= 1.66693 train_acc= 0.25160 val_loss= 8.09059 val_acc= 0.79434 time= 10.14370\n",
      "Epoch: 1204 train_loss= 1.66949 train_acc= 0.25802 val_loss= 8.09038 val_acc= 0.78920 time= 10.20361\n",
      "Epoch: 1205 train_loss= 1.69976 train_acc= 0.25546 val_loss= 8.09058 val_acc= 0.78021 time= 10.25611\n",
      "Epoch: 1206 train_loss= 1.63862 train_acc= 0.24390 val_loss= 8.09134 val_acc= 0.76735 time= 10.15640\n",
      "Epoch: 1207 train_loss= 1.65551 train_acc= 0.25289 val_loss= 8.09304 val_acc= 0.80077 time= 10.11958\n",
      "Epoch: 1208 train_loss= 1.71891 train_acc= 0.24519 val_loss= 8.09430 val_acc= 0.82905 time= 10.11177\n",
      "Epoch: 1209 train_loss= 1.66377 train_acc= 0.24775 val_loss= 8.09558 val_acc= 0.86761 time= 10.14652\n",
      "Epoch: 1210 train_loss= 1.65508 train_acc= 0.23235 val_loss= 8.09622 val_acc= 0.90231 time= 10.80773\n",
      "Epoch: 1211 train_loss= 1.71799 train_acc= 0.23363 val_loss= 8.09707 val_acc= 0.91388 time= 10.19096\n",
      "Epoch: 1212 train_loss= 1.68610 train_acc= 0.22593 val_loss= 8.09749 val_acc= 0.90103 time= 10.11367\n",
      "Epoch: 1213 train_loss= 1.65230 train_acc= 0.27599 val_loss= 8.09771 val_acc= 0.90617 time= 10.12934\n",
      "Epoch: 1214 train_loss= 1.66463 train_acc= 0.25160 val_loss= 8.09820 val_acc= 0.89589 time= 10.14848\n",
      "Epoch: 1215 train_loss= 1.63353 train_acc= 0.25931 val_loss= 8.09915 val_acc= 0.90103 time= 10.14442\n",
      "Epoch: 1216 train_loss= 1.67305 train_acc= 0.23492 val_loss= 8.09953 val_acc= 0.86247 time= 10.12417\n",
      "Epoch: 1217 train_loss= 1.68785 train_acc= 0.24904 val_loss= 8.10054 val_acc= 0.87275 time= 10.21362\n",
      "Epoch: 1218 train_loss= 1.63614 train_acc= 0.25160 val_loss= 8.10134 val_acc= 0.87404 time= 10.11058\n",
      "Epoch: 1219 train_loss= 1.65278 train_acc= 0.24904 val_loss= 8.10215 val_acc= 0.89332 time= 10.10277\n",
      "Epoch: 1220 train_loss= 1.62828 train_acc= 0.25160 val_loss= 8.10253 val_acc= 0.92159 time= 10.12694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1221 train_loss= 1.67970 train_acc= 0.23363 val_loss= 8.10361 val_acc= 0.91902 time= 10.13567\n",
      "Epoch: 1222 train_loss= 1.68056 train_acc= 0.24519 val_loss= 8.10392 val_acc= 0.91645 time= 10.11568\n",
      "Epoch: 1223 train_loss= 1.65377 train_acc= 0.25289 val_loss= 8.10377 val_acc= 0.91774 time= 10.33532\n",
      "Epoch: 1224 train_loss= 1.67657 train_acc= 0.23748 val_loss= 8.10295 val_acc= 0.92416 time= 10.12902\n",
      "Epoch: 1225 train_loss= 1.62580 train_acc= 0.25931 val_loss= 8.10149 val_acc= 0.91131 time= 10.13516\n",
      "Epoch: 1226 train_loss= 1.71576 train_acc= 0.25674 val_loss= 8.09954 val_acc= 0.85604 time= 10.13777\n",
      "Epoch: 1227 train_loss= 1.64346 train_acc= 0.25802 val_loss= 8.09838 val_acc= 0.77892 time= 10.11223\n",
      "Epoch: 1228 train_loss= 1.70166 train_acc= 0.25160 val_loss= 8.09738 val_acc= 0.71979 time= 10.10806\n",
      "Epoch: 1229 train_loss= 1.70159 train_acc= 0.25417 val_loss= 8.09768 val_acc= 0.72622 time= 10.23185\n",
      "Epoch: 1230 train_loss= 1.67267 train_acc= 0.25802 val_loss= 8.09902 val_acc= 0.74422 time= 10.12515\n",
      "Epoch: 1231 train_loss= 1.69085 train_acc= 0.24904 val_loss= 8.09879 val_acc= 0.78920 time= 10.11074\n",
      "Epoch: 1232 train_loss= 1.68221 train_acc= 0.25546 val_loss= 8.09825 val_acc= 0.80848 time= 10.14152\n",
      "Epoch: 1233 train_loss= 1.70414 train_acc= 0.24904 val_loss= 8.09821 val_acc= 0.77763 time= 10.12120\n",
      "Epoch: 1234 train_loss= 1.68926 train_acc= 0.26444 val_loss= 8.09793 val_acc= 0.67224 time= 10.35940\n",
      "Epoch: 1235 train_loss= 1.67396 train_acc= 0.23235 val_loss= 8.09737 val_acc= 0.38432 time= 10.29935\n",
      "Epoch: 1236 train_loss= 1.64155 train_acc= 0.25160 val_loss= 8.09681 val_acc= 0.17095 time= 10.14131\n",
      "Epoch: 1237 train_loss= 1.65887 train_acc= 0.22721 val_loss= 8.09623 val_acc= 0.11954 time= 10.12647\n",
      "Epoch: 1238 train_loss= 1.60773 train_acc= 0.24262 val_loss= 8.09541 val_acc= 0.15039 time= 10.16845\n",
      "Epoch: 1239 train_loss= 1.63787 train_acc= 0.23877 val_loss= 8.09550 val_acc= 0.21722 time= 10.14801\n",
      "Epoch: 1240 train_loss= 1.67094 train_acc= 0.21438 val_loss= 8.09558 val_acc= 0.27249 time= 10.72102\n",
      "Epoch: 1241 train_loss= 1.66474 train_acc= 0.23363 val_loss= 8.09600 val_acc= 0.32776 time= 10.20163\n",
      "Epoch: 1242 train_loss= 1.67851 train_acc= 0.23748 val_loss= 8.09582 val_acc= 0.32262 time= 10.16935\n",
      "Epoch: 1243 train_loss= 1.64230 train_acc= 0.24647 val_loss= 8.09559 val_acc= 0.31491 time= 10.11610\n",
      "Epoch: 1244 train_loss= 1.65524 train_acc= 0.23877 val_loss= 8.09564 val_acc= 0.30977 time= 10.21638\n",
      "Epoch: 1245 train_loss= 1.62241 train_acc= 0.25674 val_loss= 8.09669 val_acc= 0.31105 time= 10.25442\n",
      "Epoch: 1246 train_loss= 1.65765 train_acc= 0.22080 val_loss= 8.09756 val_acc= 0.35604 time= 10.19293\n",
      "Epoch: 1247 train_loss= 1.69027 train_acc= 0.22721 val_loss= 8.09883 val_acc= 0.44602 time= 10.18005\n",
      "Epoch: 1248 train_loss= 1.67771 train_acc= 0.24134 val_loss= 8.09938 val_acc= 0.50257 time= 10.25324\n",
      "Epoch: 1249 train_loss= 1.65274 train_acc= 0.23492 val_loss= 8.10010 val_acc= 0.64653 time= 10.11777\n",
      "Epoch: 1250 train_loss= 1.62877 train_acc= 0.23877 val_loss= 8.10110 val_acc= 0.73907 time= 10.16941\n",
      "Epoch: 1251 train_loss= 1.63811 train_acc= 0.25032 val_loss= 8.10272 val_acc= 0.77378 time= 10.12155\n",
      "Epoch: 1252 train_loss= 1.67032 train_acc= 0.25674 val_loss= 8.10571 val_acc= 0.82776 time= 10.13629\n",
      "Epoch: 1253 train_loss= 1.64293 train_acc= 0.25417 val_loss= 8.10790 val_acc= 0.84447 time= 10.12530\n",
      "Epoch: 1254 train_loss= 1.77563 train_acc= 0.25417 val_loss= 8.10755 val_acc= 0.83548 time= 10.19070\n",
      "Epoch: 1255 train_loss= 1.65243 train_acc= 0.23877 val_loss= 8.10535 val_acc= 0.80977 time= 10.13060\n",
      "Epoch: 1256 train_loss= 1.75257 train_acc= 0.24904 val_loss= 8.10073 val_acc= 0.73136 time= 10.15027\n",
      "Epoch: 1257 train_loss= 1.62199 train_acc= 0.26187 val_loss= 8.09743 val_acc= 0.62982 time= 10.11800\n",
      "Epoch: 1258 train_loss= 1.67625 train_acc= 0.26573 val_loss= 8.09503 val_acc= 0.54370 time= 10.11454\n",
      "Epoch: 1259 train_loss= 1.65074 train_acc= 0.25931 val_loss= 8.09358 val_acc= 0.50771 time= 10.13169\n",
      "Epoch: 1260 train_loss= 1.70362 train_acc= 0.26573 val_loss= 8.09161 val_acc= 0.37532 time= 10.20955\n",
      "Epoch: 1261 train_loss= 1.66125 train_acc= 0.24390 val_loss= 8.08971 val_acc= 0.21465 time= 10.14717\n",
      "Epoch: 1262 train_loss= 1.65282 train_acc= 0.23620 val_loss= 8.08825 val_acc= 0.24936 time= 10.15853\n",
      "Epoch: 1263 train_loss= 1.66464 train_acc= 0.24519 val_loss= 8.08708 val_acc= 0.37661 time= 10.12079\n",
      "Epoch: 1264 train_loss= 1.66308 train_acc= 0.25674 val_loss= 8.08576 val_acc= 0.43445 time= 10.18588\n",
      "Epoch: 1265 train_loss= 1.68366 train_acc= 0.25160 val_loss= 8.08564 val_acc= 0.59640 time= 10.12648\n",
      "Epoch: 1266 train_loss= 1.64427 train_acc= 0.23877 val_loss= 8.08589 val_acc= 0.73136 time= 10.22776\n",
      "Epoch: 1267 train_loss= 1.62818 train_acc= 0.26444 val_loss= 8.08631 val_acc= 0.82776 time= 10.14154\n",
      "Epoch: 1268 train_loss= 1.67241 train_acc= 0.25032 val_loss= 8.08693 val_acc= 0.92416 time= 10.13620\n",
      "Epoch: 1269 train_loss= 1.65453 train_acc= 0.25546 val_loss= 8.08879 val_acc= 0.96015 time= 10.13611\n",
      "Epoch: 1270 train_loss= 1.64534 train_acc= 0.24519 val_loss= 8.09138 val_acc= 0.97301 time= 10.71356\n",
      "Epoch: 1271 train_loss= 1.62335 train_acc= 0.25931 val_loss= 8.09503 val_acc= 0.98329 time= 10.11603\n",
      "Epoch: 1272 train_loss= 1.65430 train_acc= 0.24134 val_loss= 8.09885 val_acc= 0.98458 time= 10.30893\n",
      "Epoch: 1273 train_loss= 1.64356 train_acc= 0.24775 val_loss= 8.10336 val_acc= 0.98586 time= 10.15530\n",
      "Epoch: 1274 train_loss= 1.69380 train_acc= 0.25032 val_loss= 8.10477 val_acc= 0.98329 time= 10.14547\n",
      "Epoch: 1275 train_loss= 1.71529 train_acc= 0.24005 val_loss= 8.10550 val_acc= 0.97943 time= 10.12170\n",
      "Epoch: 1276 train_loss= 1.66567 train_acc= 0.23235 val_loss= 8.10344 val_acc= 0.94730 time= 10.13661\n",
      "Epoch: 1277 train_loss= 1.68068 train_acc= 0.26316 val_loss= 8.10197 val_acc= 0.89974 time= 10.12714\n",
      "Epoch: 1278 train_loss= 1.63524 train_acc= 0.26059 val_loss= 8.10079 val_acc= 0.82005 time= 10.11553\n",
      "Epoch: 1279 train_loss= 1.68235 train_acc= 0.26573 val_loss= 8.10071 val_acc= 0.74422 time= 10.20729\n",
      "Epoch: 1280 train_loss= 1.67344 train_acc= 0.24262 val_loss= 8.10073 val_acc= 0.71851 time= 10.11393\n",
      "Epoch: 1281 train_loss= 1.66705 train_acc= 0.25674 val_loss= 8.10021 val_acc= 0.69280 time= 10.11711\n",
      "Epoch: 1282 train_loss= 1.71348 train_acc= 0.24262 val_loss= 8.09874 val_acc= 0.65681 time= 10.14264\n",
      "Epoch: 1283 train_loss= 1.65428 train_acc= 0.25546 val_loss= 8.09696 val_acc= 0.65553 time= 10.13005\n",
      "Epoch: 1284 train_loss= 1.64854 train_acc= 0.24134 val_loss= 8.09556 val_acc= 0.68123 time= 10.11617\n",
      "Epoch: 1285 train_loss= 1.70643 train_acc= 0.23235 val_loss= 8.09362 val_acc= 0.73779 time= 10.23545\n",
      "Epoch: 1286 train_loss= 1.66834 train_acc= 0.25417 val_loss= 8.09182 val_acc= 0.79177 time= 10.12295\n",
      "Epoch: 1287 train_loss= 1.67330 train_acc= 0.25289 val_loss= 8.09031 val_acc= 0.80720 time= 10.12874\n",
      "Epoch: 1288 train_loss= 1.67933 train_acc= 0.26059 val_loss= 8.08818 val_acc= 0.82134 time= 10.13095\n",
      "Epoch: 1289 train_loss= 1.75877 train_acc= 0.25160 val_loss= 8.08502 val_acc= 0.80463 time= 10.13826\n",
      "Epoch: 1290 train_loss= 1.63478 train_acc= 0.27086 val_loss= 8.08304 val_acc= 0.77635 time= 10.11400\n",
      "Epoch: 1291 train_loss= 1.62817 train_acc= 0.25674 val_loss= 8.08233 val_acc= 0.74679 time= 10.31066\n",
      "Epoch: 1292 train_loss= 1.71114 train_acc= 0.22850 val_loss= 8.08257 val_acc= 0.72494 time= 10.12610\n",
      "Epoch: 1293 train_loss= 1.64321 train_acc= 0.24519 val_loss= 8.08359 val_acc= 0.68638 time= 10.19041\n",
      "Epoch: 1294 train_loss= 1.72675 train_acc= 0.23363 val_loss= 8.08505 val_acc= 0.66967 time= 10.16974\n",
      "Epoch: 1295 train_loss= 1.63380 train_acc= 0.26573 val_loss= 8.08632 val_acc= 0.65681 time= 10.14451\n",
      "Epoch: 1296 train_loss= 1.67549 train_acc= 0.23620 val_loss= 8.08687 val_acc= 0.66195 time= 10.11296\n",
      "Epoch: 1297 train_loss= 1.67711 train_acc= 0.27856 val_loss= 8.08752 val_acc= 0.69152 time= 10.24617\n",
      "Epoch: 1298 train_loss= 1.65487 train_acc= 0.24519 val_loss= 8.08833 val_acc= 0.73522 time= 10.12631\n",
      "Epoch: 1299 train_loss= 1.70609 train_acc= 0.25032 val_loss= 8.08924 val_acc= 0.77249 time= 10.57650\n",
      "Epoch: 1300 train_loss= 1.66572 train_acc= 0.25417 val_loss= 8.08991 val_acc= 0.81362 time= 10.16259\n",
      "Epoch: 1301 train_loss= 1.65734 train_acc= 0.24519 val_loss= 8.09075 val_acc= 0.83033 time= 10.12092\n",
      "Epoch: 1302 train_loss= 1.65942 train_acc= 0.26701 val_loss= 8.09235 val_acc= 0.85090 time= 10.12697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1303 train_loss= 1.64812 train_acc= 0.24519 val_loss= 8.09320 val_acc= 0.87661 time= 10.23249\n",
      "Epoch: 1304 train_loss= 1.63562 train_acc= 0.24134 val_loss= 8.09419 val_acc= 0.87789 time= 10.15840\n",
      "Epoch: 1305 train_loss= 1.66751 train_acc= 0.23748 val_loss= 8.09407 val_acc= 0.84833 time= 10.12645\n",
      "Epoch: 1306 train_loss= 1.66270 train_acc= 0.25802 val_loss= 8.09208 val_acc= 0.73779 time= 10.13719\n",
      "Epoch: 1307 train_loss= 1.68877 train_acc= 0.25289 val_loss= 8.09066 val_acc= 0.62339 time= 10.11390\n",
      "Epoch: 1308 train_loss= 1.66371 train_acc= 0.24262 val_loss= 8.08898 val_acc= 0.38175 time= 10.13427\n",
      "Epoch: 1309 train_loss= 1.65670 train_acc= 0.24904 val_loss= 8.08733 val_acc= 0.20308 time= 10.15508\n",
      "Epoch: 1310 train_loss= 1.63477 train_acc= 0.25931 val_loss= 8.08598 val_acc= 0.15553 time= 10.18982\n",
      "Epoch: 1311 train_loss= 1.65669 train_acc= 0.25417 val_loss= 8.08637 val_acc= 0.18252 time= 10.12875\n",
      "Epoch: 1312 train_loss= 1.66542 train_acc= 0.24647 val_loss= 8.08751 val_acc= 0.35861 time= 10.13135\n",
      "Epoch: 1313 train_loss= 1.65307 train_acc= 0.22593 val_loss= 8.08761 val_acc= 0.63625 time= 10.12565\n",
      "Epoch: 1314 train_loss= 1.65664 train_acc= 0.23748 val_loss= 8.08801 val_acc= 0.78278 time= 10.12581\n",
      "Epoch: 1315 train_loss= 1.64671 train_acc= 0.24262 val_loss= 8.08870 val_acc= 0.87147 time= 10.13294\n",
      "Epoch: 1316 train_loss= 1.65993 train_acc= 0.24647 val_loss= 8.08996 val_acc= 0.92802 time= 10.24377\n",
      "Epoch: 1317 train_loss= 1.66658 train_acc= 0.21566 val_loss= 8.08991 val_acc= 0.94987 time= 10.13240\n",
      "Epoch: 1318 train_loss= 1.65940 train_acc= 0.23748 val_loss= 8.08990 val_acc= 0.96272 time= 10.11660\n",
      "Epoch: 1319 train_loss= 1.63590 train_acc= 0.24904 val_loss= 8.08982 val_acc= 0.96787 time= 10.11703\n",
      "Epoch: 1320 train_loss= 1.67699 train_acc= 0.24005 val_loss= 8.09025 val_acc= 0.97172 time= 10.15727\n",
      "Epoch: 1321 train_loss= 1.66401 train_acc= 0.23235 val_loss= 8.09054 val_acc= 0.97429 time= 10.16248\n",
      "Epoch: 1322 train_loss= 1.66099 train_acc= 0.24390 val_loss= 8.09074 val_acc= 0.96530 time= 10.24659\n",
      "Epoch: 1323 train_loss= 1.69277 train_acc= 0.23235 val_loss= 8.09135 val_acc= 0.95887 time= 10.25982\n",
      "Epoch: 1324 train_loss= 1.63376 train_acc= 0.25674 val_loss= 8.09221 val_acc= 0.92159 time= 10.15443\n",
      "Epoch: 1325 train_loss= 1.72330 train_acc= 0.24519 val_loss= 8.09219 val_acc= 0.85476 time= 10.12556\n",
      "Epoch: 1326 train_loss= 1.69345 train_acc= 0.24005 val_loss= 8.09165 val_acc= 0.76864 time= 10.14099\n",
      "Epoch: 1327 train_loss= 1.64721 train_acc= 0.25546 val_loss= 8.09120 val_acc= 0.64653 time= 10.14833\n",
      "Epoch: 1328 train_loss= 1.67847 train_acc= 0.24134 val_loss= 8.09078 val_acc= 0.55784 time= 10.18468\n",
      "Epoch: 1329 train_loss= 1.66314 train_acc= 0.23748 val_loss= 8.09005 val_acc= 0.60283 time= 10.68681\n",
      "Epoch: 1330 train_loss= 1.70349 train_acc= 0.24775 val_loss= 8.08970 val_acc= 0.69280 time= 10.13515\n",
      "Epoch: 1331 train_loss= 1.66066 train_acc= 0.25931 val_loss= 8.08950 val_acc= 0.76350 time= 10.11175\n",
      "Epoch: 1332 train_loss= 1.62608 train_acc= 0.26444 val_loss= 8.08889 val_acc= 0.88046 time= 10.12616\n",
      "Epoch: 1333 train_loss= 1.72403 train_acc= 0.23877 val_loss= 8.08837 val_acc= 0.95244 time= 10.14534\n",
      "Epoch: 1334 train_loss= 1.67826 train_acc= 0.24519 val_loss= 8.08770 val_acc= 0.95758 time= 10.21484\n",
      "Epoch: 1335 train_loss= 1.72164 train_acc= 0.23620 val_loss= 8.08666 val_acc= 0.96144 time= 10.15062\n",
      "Epoch: 1336 train_loss= 1.66860 train_acc= 0.23877 val_loss= 8.08564 val_acc= 0.95373 time= 10.11698\n",
      "Epoch: 1337 train_loss= 1.66594 train_acc= 0.23363 val_loss= 8.08434 val_acc= 0.93316 time= 10.14221\n",
      "Epoch: 1338 train_loss= 1.71093 train_acc= 0.24775 val_loss= 8.08349 val_acc= 0.89846 time= 10.22417\n",
      "Epoch: 1339 train_loss= 1.66833 train_acc= 0.24775 val_loss= 8.08387 val_acc= 0.88432 time= 10.12964\n",
      "Epoch: 1340 train_loss= 1.70259 train_acc= 0.24647 val_loss= 8.08389 val_acc= 0.84319 time= 10.12140\n",
      "Epoch: 1341 train_loss= 1.68875 train_acc= 0.26059 val_loss= 8.08452 val_acc= 0.80591 time= 10.17480\n",
      "Epoch: 1342 train_loss= 1.68317 train_acc= 0.24647 val_loss= 8.08595 val_acc= 0.80977 time= 10.12113\n",
      "Epoch: 1343 train_loss= 1.69034 train_acc= 0.24390 val_loss= 8.08592 val_acc= 0.76992 time= 10.10928\n",
      "Epoch: 1344 train_loss= 1.68106 train_acc= 0.26059 val_loss= 8.08634 val_acc= 0.73136 time= 10.16690\n",
      "Epoch: 1345 train_loss= 1.67367 train_acc= 0.24775 val_loss= 8.08785 val_acc= 0.74036 time= 10.13303\n",
      "Epoch: 1346 train_loss= 1.64910 train_acc= 0.25417 val_loss= 8.08862 val_acc= 0.73650 time= 10.13965\n",
      "Epoch: 1347 train_loss= 1.65865 train_acc= 0.24390 val_loss= 8.08986 val_acc= 0.70951 time= 10.34499\n",
      "Epoch: 1348 train_loss= 1.65619 train_acc= 0.23620 val_loss= 8.08963 val_acc= 0.67609 time= 10.12538\n",
      "Epoch: 1349 train_loss= 1.69542 train_acc= 0.25417 val_loss= 8.08863 val_acc= 0.65039 time= 10.11906\n",
      "Epoch: 1350 train_loss= 1.64296 train_acc= 0.23877 val_loss= 8.08797 val_acc= 0.66710 time= 10.17675\n",
      "Epoch: 1351 train_loss= 1.63058 train_acc= 0.24262 val_loss= 8.08709 val_acc= 0.63239 time= 10.12578\n",
      "Epoch: 1352 train_loss= 1.69648 train_acc= 0.26573 val_loss= 8.08664 val_acc= 0.67095 time= 10.29733\n",
      "Epoch: 1353 train_loss= 1.65577 train_acc= 0.23492 val_loss= 8.08693 val_acc= 0.74550 time= 10.37457\n",
      "Epoch: 1354 train_loss= 1.67749 train_acc= 0.24647 val_loss= 8.08764 val_acc= 0.80463 time= 10.12947\n",
      "Epoch: 1355 train_loss= 1.64309 train_acc= 0.24775 val_loss= 8.08975 val_acc= 0.83805 time= 10.12318\n",
      "Epoch: 1356 train_loss= 1.68033 train_acc= 0.26187 val_loss= 8.09248 val_acc= 0.85347 time= 10.14870\n",
      "Epoch: 1357 train_loss= 1.69506 train_acc= 0.25417 val_loss= 8.09443 val_acc= 0.86247 time= 10.13919\n",
      "Epoch: 1358 train_loss= 1.66524 train_acc= 0.26701 val_loss= 8.09599 val_acc= 0.88432 time= 10.36627\n",
      "Epoch: 1359 train_loss= 1.71850 train_acc= 0.25417 val_loss= 8.09778 val_acc= 0.91902 time= 10.67191\n",
      "Epoch: 1360 train_loss= 1.71504 train_acc= 0.24647 val_loss= 8.09751 val_acc= 0.93316 time= 10.13614\n",
      "Epoch: 1361 train_loss= 1.65719 train_acc= 0.24519 val_loss= 8.09653 val_acc= 0.94859 time= 10.14106\n",
      "Epoch: 1362 train_loss= 1.65538 train_acc= 0.25546 val_loss= 8.09542 val_acc= 0.92416 time= 10.18218\n",
      "Epoch: 1363 train_loss= 1.67882 train_acc= 0.25802 val_loss= 8.09265 val_acc= 0.86504 time= 10.12629\n",
      "Epoch: 1364 train_loss= 1.66658 train_acc= 0.25289 val_loss= 8.09082 val_acc= 0.77506 time= 10.12496\n",
      "Epoch: 1365 train_loss= 1.69496 train_acc= 0.25802 val_loss= 8.09041 val_acc= 0.63368 time= 10.26537\n",
      "Epoch: 1366 train_loss= 1.66198 train_acc= 0.27728 val_loss= 8.09098 val_acc= 0.50257 time= 10.13884\n",
      "Epoch: 1367 train_loss= 1.74017 train_acc= 0.23107 val_loss= 8.09027 val_acc= 0.26350 time= 10.13532\n",
      "Epoch: 1368 train_loss= 1.65904 train_acc= 0.24775 val_loss= 8.09058 val_acc= 0.15296 time= 10.13796\n",
      "Epoch: 1369 train_loss= 1.68552 train_acc= 0.22593 val_loss= 8.09090 val_acc= 0.04627 time= 10.13027\n",
      "Epoch: 1370 train_loss= 1.67778 train_acc= 0.22465 val_loss= 8.09123 val_acc= 0.00900 time= 10.13310\n",
      "Epoch: 1371 train_loss= 1.63270 train_acc= 0.23748 val_loss= 8.09230 val_acc= 0.00000 time= 10.14590\n",
      "Epoch: 1372 train_loss= 1.70840 train_acc= 0.21181 val_loss= 8.09254 val_acc= 0.00000 time= 10.20128\n",
      "Epoch: 1373 train_loss= 1.67262 train_acc= 0.23363 val_loss= 8.09259 val_acc= 0.00000 time= 10.13541\n",
      "Epoch: 1374 train_loss= 1.68928 train_acc= 0.21566 val_loss= 8.09157 val_acc= 0.00257 time= 10.13955\n",
      "Epoch: 1375 train_loss= 1.68662 train_acc= 0.22336 val_loss= 8.09052 val_acc= 0.01414 time= 10.12669\n",
      "Epoch: 1376 train_loss= 1.66737 train_acc= 0.21823 val_loss= 8.09021 val_acc= 0.04113 time= 10.11303\n",
      "Epoch: 1377 train_loss= 1.66179 train_acc= 0.22080 val_loss= 8.09029 val_acc= 0.12982 time= 10.12026\n",
      "Epoch: 1378 train_loss= 1.69164 train_acc= 0.21951 val_loss= 8.09147 val_acc= 0.33676 time= 10.24502\n",
      "Epoch: 1379 train_loss= 1.65050 train_acc= 0.24262 val_loss= 8.09339 val_acc= 0.69666 time= 10.13783\n",
      "Epoch: 1380 train_loss= 1.69099 train_acc= 0.24904 val_loss= 8.09520 val_acc= 0.81877 time= 10.14386\n",
      "Epoch: 1381 train_loss= 1.68508 train_acc= 0.25160 val_loss= 8.09516 val_acc= 0.89203 time= 10.12049\n",
      "Epoch: 1382 train_loss= 1.66878 train_acc= 0.24775 val_loss= 8.09464 val_acc= 0.94987 time= 10.20849\n",
      "Epoch: 1383 train_loss= 1.66753 train_acc= 0.24262 val_loss= 8.09414 val_acc= 0.96915 time= 10.14563\n",
      "Epoch: 1384 train_loss= 1.71185 train_acc= 0.25032 val_loss= 8.09336 val_acc= 0.96915 time= 10.21623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1385 train_loss= 1.65388 train_acc= 0.24390 val_loss= 8.09250 val_acc= 0.96272 time= 10.14425\n",
      "Epoch: 1386 train_loss= 1.71795 train_acc= 0.25160 val_loss= 8.09031 val_acc= 0.94473 time= 10.14059\n",
      "Epoch: 1387 train_loss= 1.70124 train_acc= 0.23107 val_loss= 8.08762 val_acc= 0.89332 time= 10.11761\n",
      "Epoch: 1388 train_loss= 1.74681 train_acc= 0.25160 val_loss= 8.08596 val_acc= 0.85990 time= 10.77840\n",
      "Epoch: 1389 train_loss= 1.70948 train_acc= 0.24519 val_loss= 8.08559 val_acc= 0.83033 time= 10.14791\n",
      "Epoch: 1390 train_loss= 1.68714 train_acc= 0.26444 val_loss= 8.08610 val_acc= 0.80591 time= 10.22256\n",
      "Epoch: 1391 train_loss= 1.72027 train_acc= 0.24904 val_loss= 8.08639 val_acc= 0.78021 time= 10.14842\n",
      "Epoch: 1392 train_loss= 1.69145 train_acc= 0.22721 val_loss= 8.08670 val_acc= 0.76093 time= 10.12478\n",
      "Epoch: 1393 train_loss= 1.62962 train_acc= 0.24904 val_loss= 8.08695 val_acc= 0.74679 time= 10.13918\n",
      "Epoch: 1394 train_loss= 1.68321 train_acc= 0.26444 val_loss= 8.08686 val_acc= 0.71722 time= 10.12244\n",
      "Epoch: 1395 train_loss= 1.72012 train_acc= 0.24775 val_loss= 8.08716 val_acc= 0.72494 time= 10.11672\n",
      "Epoch: 1396 train_loss= 1.66970 train_acc= 0.25674 val_loss= 8.08859 val_acc= 0.73650 time= 10.20677\n",
      "Epoch: 1397 train_loss= 1.67726 train_acc= 0.26187 val_loss= 8.08933 val_acc= 0.73907 time= 10.14727\n",
      "Epoch: 1398 train_loss= 1.63330 train_acc= 0.27343 val_loss= 8.08943 val_acc= 0.69794 time= 10.12528\n",
      "Epoch: 1399 train_loss= 1.68236 train_acc= 0.24134 val_loss= 8.08960 val_acc= 0.69152 time= 10.11916\n",
      "Epoch: 1400 train_loss= 1.68181 train_acc= 0.25417 val_loss= 8.08975 val_acc= 0.71594 time= 10.11653\n",
      "Epoch: 1401 train_loss= 1.66030 train_acc= 0.25160 val_loss= 8.09133 val_acc= 0.76350 time= 10.09897\n",
      "Epoch: 1402 train_loss= 1.67885 train_acc= 0.25802 val_loss= 8.09218 val_acc= 0.78920 time= 10.19204\n",
      "Epoch: 1403 train_loss= 1.68502 train_acc= 0.26059 val_loss= 8.09373 val_acc= 0.82391 time= 10.19472\n",
      "Epoch: 1404 train_loss= 1.65325 train_acc= 0.26573 val_loss= 8.09511 val_acc= 0.83933 time= 10.13336\n",
      "Epoch: 1405 train_loss= 1.66431 train_acc= 0.24519 val_loss= 8.09560 val_acc= 0.82905 time= 10.12461\n",
      "Epoch: 1406 train_loss= 1.62249 train_acc= 0.25032 val_loss= 8.09564 val_acc= 0.82776 time= 10.12234\n",
      "Epoch: 1407 train_loss= 1.63500 train_acc= 0.25032 val_loss= 8.09555 val_acc= 0.82776 time= 10.12012\n",
      "Epoch: 1408 train_loss= 1.62941 train_acc= 0.25674 val_loss= 8.09572 val_acc= 0.82648 time= 10.11119\n",
      "Epoch: 1409 train_loss= 1.67320 train_acc= 0.24519 val_loss= 8.09539 val_acc= 0.79949 time= 10.27819\n",
      "Epoch: 1410 train_loss= 1.66448 train_acc= 0.26829 val_loss= 8.09547 val_acc= 0.75707 time= 10.13202\n",
      "Epoch: 1411 train_loss= 1.66306 train_acc= 0.25931 val_loss= 8.09949 val_acc= 0.72237 time= 10.24108\n",
      "Epoch: 1412 train_loss= 1.64875 train_acc= 0.26829 val_loss= 8.10302 val_acc= 0.68895 time= 10.15860\n",
      "Epoch: 1413 train_loss= 1.67245 train_acc= 0.27343 val_loss= 8.10696 val_acc= 0.65938 time= 10.13277\n",
      "Epoch: 1414 train_loss= 1.72081 train_acc= 0.24134 val_loss= 8.11106 val_acc= 0.69409 time= 10.13828\n",
      "Epoch: 1415 train_loss= 1.66954 train_acc= 0.24904 val_loss= 8.11288 val_acc= 0.71594 time= 10.31217\n",
      "Epoch: 1416 train_loss= 1.65246 train_acc= 0.25417 val_loss= 8.11367 val_acc= 0.71337 time= 10.12684\n",
      "Epoch: 1417 train_loss= 1.72526 train_acc= 0.27599 val_loss= 8.11430 val_acc= 0.73907 time= 10.11400\n",
      "Epoch: 1418 train_loss= 1.65006 train_acc= 0.25931 val_loss= 8.11407 val_acc= 0.74807 time= 10.72336\n",
      "Epoch: 1419 train_loss= 1.73640 train_acc= 0.24775 val_loss= 8.11227 val_acc= 0.74807 time= 10.20056\n",
      "Epoch: 1420 train_loss= 1.65690 train_acc= 0.26573 val_loss= 8.11067 val_acc= 0.75321 time= 10.23013\n",
      "Epoch: 1421 train_loss= 1.63372 train_acc= 0.25674 val_loss= 8.10980 val_acc= 0.75835 time= 10.27356\n",
      "Epoch: 1422 train_loss= 1.65808 train_acc= 0.26444 val_loss= 8.10740 val_acc= 0.74165 time= 10.17036\n",
      "Epoch: 1423 train_loss= 1.69984 train_acc= 0.24134 val_loss= 8.10430 val_acc= 0.73779 time= 10.11853\n",
      "Epoch: 1424 train_loss= 1.64000 train_acc= 0.25417 val_loss= 8.10159 val_acc= 0.74936 time= 10.12222\n",
      "Epoch: 1425 train_loss= 1.70318 train_acc= 0.25032 val_loss= 8.09866 val_acc= 0.72365 time= 10.11936\n",
      "Epoch: 1426 train_loss= 1.71933 train_acc= 0.24519 val_loss= 8.09416 val_acc= 0.58740 time= 10.13596\n",
      "Epoch: 1427 train_loss= 1.66316 train_acc= 0.24647 val_loss= 8.09142 val_acc= 0.44859 time= 10.23734\n",
      "Epoch: 1428 train_loss= 1.67811 train_acc= 0.23877 val_loss= 8.08794 val_acc= 0.27635 time= 10.14860\n",
      "Epoch: 1429 train_loss= 1.66202 train_acc= 0.22208 val_loss= 8.08558 val_acc= 0.22622 time= 10.12093\n",
      "Epoch: 1430 train_loss= 1.69174 train_acc= 0.19769 val_loss= 8.08486 val_acc= 0.28278 time= 10.13311\n",
      "Epoch: 1431 train_loss= 1.65018 train_acc= 0.22208 val_loss= 8.08560 val_acc= 0.44602 time= 10.12577\n",
      "Epoch: 1432 train_loss= 1.68698 train_acc= 0.23107 val_loss= 8.08711 val_acc= 0.82005 time= 10.12656\n",
      "Epoch: 1433 train_loss= 1.67858 train_acc= 0.23748 val_loss= 8.08892 val_acc= 0.90488 time= 10.35003\n",
      "Epoch: 1434 train_loss= 1.67703 train_acc= 0.23107 val_loss= 8.09066 val_acc= 0.94087 time= 10.15141\n",
      "Epoch: 1435 train_loss= 1.64786 train_acc= 0.25931 val_loss= 8.09174 val_acc= 0.96015 time= 10.13198\n",
      "Epoch: 1436 train_loss= 1.66813 train_acc= 0.24390 val_loss= 8.09302 val_acc= 0.96915 time= 10.10547\n",
      "Epoch: 1437 train_loss= 1.65992 train_acc= 0.24390 val_loss= 8.09481 val_acc= 0.97815 time= 10.11905\n",
      "Epoch: 1438 train_loss= 1.69683 train_acc= 0.23492 val_loss= 8.09667 val_acc= 0.98329 time= 10.18436\n",
      "Epoch: 1439 train_loss= 1.68013 train_acc= 0.24775 val_loss= 8.09862 val_acc= 0.98329 time= 10.14694\n",
      "Epoch: 1440 train_loss= 1.66590 train_acc= 0.24390 val_loss= 8.10130 val_acc= 0.98715 time= 10.20381\n",
      "Epoch: 1441 train_loss= 1.70594 train_acc= 0.24904 val_loss= 8.10245 val_acc= 0.98458 time= 10.30263\n",
      "Epoch: 1442 train_loss= 1.62799 train_acc= 0.24134 val_loss= 8.10253 val_acc= 0.97686 time= 10.13929\n",
      "Epoch: 1443 train_loss= 1.65010 train_acc= 0.24005 val_loss= 8.10218 val_acc= 0.96530 time= 10.11044\n",
      "Epoch: 1444 train_loss= 1.70099 train_acc= 0.25032 val_loss= 8.10023 val_acc= 0.95501 time= 10.13357\n",
      "Epoch: 1445 train_loss= 1.65659 train_acc= 0.24775 val_loss= 8.09738 val_acc= 0.89203 time= 10.13337\n",
      "Epoch: 1446 train_loss= 1.66518 train_acc= 0.25674 val_loss= 8.09476 val_acc= 0.73393 time= 10.21501\n",
      "Epoch: 1447 train_loss= 1.64461 train_acc= 0.25032 val_loss= 8.09254 val_acc= 0.47943 time= 10.75369\n",
      "Epoch: 1448 train_loss= 1.65716 train_acc= 0.26958 val_loss= 8.09109 val_acc= 0.31234 time= 10.14760\n",
      "Epoch: 1449 train_loss= 1.67689 train_acc= 0.24262 val_loss= 8.09041 val_acc= 0.22365 time= 10.14051\n",
      "Epoch: 1450 train_loss= 1.66069 train_acc= 0.26444 val_loss= 8.09009 val_acc= 0.17481 time= 10.15657\n",
      "Epoch: 1451 train_loss= 1.67594 train_acc= 0.23620 val_loss= 8.08993 val_acc= 0.17095 time= 10.13356\n",
      "Epoch: 1452 train_loss= 1.71229 train_acc= 0.23877 val_loss= 8.08947 val_acc= 0.19537 time= 10.33918\n",
      "Epoch: 1453 train_loss= 1.64945 train_acc= 0.26059 val_loss= 8.08904 val_acc= 0.20437 time= 10.11830\n",
      "Epoch: 1454 train_loss= 1.65344 train_acc= 0.23748 val_loss= 8.08871 val_acc= 0.21594 time= 10.10873\n",
      "Epoch: 1455 train_loss= 1.66307 train_acc= 0.22208 val_loss= 8.08880 val_acc= 0.29820 time= 10.13209\n",
      "Epoch: 1456 train_loss= 1.67473 train_acc= 0.21438 val_loss= 8.08716 val_acc= 0.53470 time= 10.17276\n",
      "Epoch: 1457 train_loss= 1.66777 train_acc= 0.24775 val_loss= 8.08593 val_acc= 0.70180 time= 10.13389\n",
      "Epoch: 1458 train_loss= 1.64667 train_acc= 0.26829 val_loss= 8.08538 val_acc= 0.78663 time= 10.23110\n",
      "Epoch: 1459 train_loss= 1.64502 train_acc= 0.24390 val_loss= 8.08536 val_acc= 0.80848 time= 10.11063\n",
      "Epoch: 1460 train_loss= 1.66742 train_acc= 0.24262 val_loss= 8.08561 val_acc= 0.81620 time= 10.13460\n",
      "Epoch: 1461 train_loss= 1.66813 train_acc= 0.25931 val_loss= 8.08634 val_acc= 0.80720 time= 10.12356\n",
      "Epoch: 1462 train_loss= 1.65727 train_acc= 0.25032 val_loss= 8.08702 val_acc= 0.79692 time= 10.15904\n",
      "Epoch: 1463 train_loss= 1.69091 train_acc= 0.26829 val_loss= 8.08749 val_acc= 0.78406 time= 10.13749\n",
      "Epoch: 1464 train_loss= 1.68368 train_acc= 0.26059 val_loss= 8.08704 val_acc= 0.77506 time= 10.19983\n",
      "Epoch: 1465 train_loss= 1.70715 train_acc= 0.26187 val_loss= 8.08608 val_acc= 0.75450 time= 10.12215\n",
      "Epoch: 1466 train_loss= 1.66243 train_acc= 0.25032 val_loss= 8.08446 val_acc= 0.75450 time= 10.12040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1467 train_loss= 1.65935 train_acc= 0.24647 val_loss= 8.08215 val_acc= 0.75964 time= 10.12847\n",
      "Epoch: 1468 train_loss= 1.63161 train_acc= 0.27214 val_loss= 8.08119 val_acc= 0.77121 time= 10.16344\n",
      "Epoch: 1469 train_loss= 1.69412 train_acc= 0.25289 val_loss= 8.08111 val_acc= 0.79049 time= 10.12870\n",
      "Epoch: 1470 train_loss= 1.68888 train_acc= 0.25032 val_loss= 8.08160 val_acc= 0.80206 time= 10.37723\n",
      "Epoch: 1471 train_loss= 1.67581 train_acc= 0.24262 val_loss= 8.08193 val_acc= 0.80463 time= 10.21634\n",
      "Epoch: 1472 train_loss= 1.65053 train_acc= 0.23620 val_loss= 8.08180 val_acc= 0.81105 time= 10.12044\n",
      "Epoch: 1473 train_loss= 1.63931 train_acc= 0.24519 val_loss= 8.08136 val_acc= 0.78535 time= 10.12142\n",
      "Epoch: 1474 train_loss= 1.63326 train_acc= 0.26701 val_loss= 8.08087 val_acc= 0.74936 time= 10.19096\n",
      "Epoch: 1475 train_loss= 1.65077 train_acc= 0.25674 val_loss= 8.08054 val_acc= 0.71080 time= 10.15245\n",
      "Epoch: 1476 train_loss= 1.69665 train_acc= 0.23492 val_loss= 8.08114 val_acc= 0.66324 time= 10.10881\n",
      "Epoch: 1477 train_loss= 1.63500 train_acc= 0.23235 val_loss= 8.08251 val_acc= 0.69794 time= 10.74148\n",
      "Epoch: 1478 train_loss= 1.63605 train_acc= 0.25032 val_loss= 8.08316 val_acc= 0.72622 time= 10.13737\n",
      "Epoch: 1479 train_loss= 1.64258 train_acc= 0.22721 val_loss= 8.08339 val_acc= 0.76864 time= 10.11013\n",
      "Epoch: 1480 train_loss= 1.68072 train_acc= 0.24262 val_loss= 8.08304 val_acc= 0.78535 time= 10.15902\n",
      "Epoch: 1481 train_loss= 1.65643 train_acc= 0.25289 val_loss= 8.08243 val_acc= 0.80334 time= 10.13551\n",
      "Epoch: 1482 train_loss= 1.65574 train_acc= 0.24904 val_loss= 8.08095 val_acc= 0.80591 time= 10.13516\n",
      "Epoch: 1483 train_loss= 1.65570 train_acc= 0.24519 val_loss= 8.07990 val_acc= 0.80977 time= 10.20099\n",
      "Epoch: 1484 train_loss= 1.69870 train_acc= 0.23877 val_loss= 8.07970 val_acc= 0.81620 time= 10.10772\n",
      "Epoch: 1485 train_loss= 1.63052 train_acc= 0.25417 val_loss= 8.08018 val_acc= 0.83033 time= 10.10865\n",
      "Epoch: 1486 train_loss= 1.65793 train_acc= 0.25032 val_loss= 8.08117 val_acc= 0.84190 time= 10.14252\n",
      "Epoch: 1487 train_loss= 1.75885 train_acc= 0.24775 val_loss= 8.08258 val_acc= 0.85604 time= 10.13361\n",
      "Epoch: 1488 train_loss= 1.69176 train_acc= 0.25032 val_loss= 8.08432 val_acc= 0.88817 time= 10.12019\n",
      "Epoch: 1489 train_loss= 1.69738 train_acc= 0.24005 val_loss= 8.08597 val_acc= 0.88689 time= 10.26647\n",
      "Epoch: 1490 train_loss= 1.67810 train_acc= 0.24262 val_loss= 8.08797 val_acc= 0.87275 time= 10.12463\n",
      "Epoch: 1491 train_loss= 1.67623 train_acc= 0.24134 val_loss= 8.09063 val_acc= 0.85090 time= 10.14068\n",
      "Epoch: 1492 train_loss= 1.63673 train_acc= 0.25032 val_loss= 8.09317 val_acc= 0.83033 time= 10.16369\n",
      "Epoch: 1493 train_loss= 1.70187 train_acc= 0.24005 val_loss= 8.09461 val_acc= 0.80848 time= 10.12225\n",
      "Epoch: 1494 train_loss= 1.68590 train_acc= 0.25802 val_loss= 8.09442 val_acc= 0.81105 time= 10.12449\n",
      "Epoch: 1495 train_loss= 1.68243 train_acc= 0.23492 val_loss= 8.09288 val_acc= 0.85090 time= 10.19695\n",
      "Epoch: 1496 train_loss= 1.65563 train_acc= 0.24134 val_loss= 8.09041 val_acc= 0.89589 time= 10.12460\n",
      "Epoch: 1497 train_loss= 1.65500 train_acc= 0.24519 val_loss= 8.08849 val_acc= 0.94859 time= 10.16195\n",
      "Epoch: 1498 train_loss= 1.67538 train_acc= 0.23363 val_loss= 8.08825 val_acc= 0.97558 time= 10.15414\n",
      "Epoch: 1499 train_loss= 1.63989 train_acc= 0.26316 val_loss= 8.08893 val_acc= 0.98972 time= 10.11897\n",
      "Epoch: 1500 train_loss= 1.65471 train_acc= 0.24262 val_loss= 8.08969 val_acc= 0.99871 time= 10.22005\n",
      "Epoch: 1501 train_loss= 1.66527 train_acc= 0.23492 val_loss= 8.09034 val_acc= 0.99871 time= 10.14813\n",
      "Epoch: 1502 train_loss= 1.67730 train_acc= 0.23748 val_loss= 8.08956 val_acc= 0.99614 time= 10.27784\n",
      "Epoch: 1503 train_loss= 1.65143 train_acc= 0.22465 val_loss= 8.08945 val_acc= 0.98972 time= 10.15150\n",
      "Epoch: 1504 train_loss= 1.66879 train_acc= 0.26573 val_loss= 8.08897 val_acc= 0.97301 time= 10.13015\n",
      "Epoch: 1505 train_loss= 1.67963 train_acc= 0.24904 val_loss= 8.08907 val_acc= 0.96915 time= 10.13809\n",
      "Epoch: 1506 train_loss= 1.62255 train_acc= 0.23877 val_loss= 8.08917 val_acc= 0.91260 time= 10.88550\n",
      "Epoch: 1507 train_loss= 1.65908 train_acc= 0.24775 val_loss= 8.08974 val_acc= 0.79692 time= 10.81802\n",
      "Epoch: 1508 train_loss= 1.64561 train_acc= 0.25674 val_loss= 8.09009 val_acc= 0.67738 time= 10.21559\n",
      "Epoch: 1509 train_loss= 1.67324 train_acc= 0.21438 val_loss= 8.09033 val_acc= 0.59640 time= 10.16500\n",
      "Epoch: 1510 train_loss= 1.63669 train_acc= 0.23492 val_loss= 8.08999 val_acc= 0.56941 time= 10.14815\n",
      "Epoch: 1511 train_loss= 1.64181 train_acc= 0.25289 val_loss= 8.08967 val_acc= 0.56041 time= 10.12497\n",
      "Epoch: 1512 train_loss= 1.67150 train_acc= 0.23107 val_loss= 8.08797 val_acc= 0.58355 time= 11.34654\n",
      "Epoch: 1513 train_loss= 1.65352 train_acc= 0.24390 val_loss= 8.08681 val_acc= 0.59512 time= 10.38343\n",
      "Epoch: 1514 train_loss= 1.69988 train_acc= 0.25802 val_loss= 8.08603 val_acc= 0.63753 time= 11.42168\n",
      "Epoch: 1515 train_loss= 1.71264 train_acc= 0.22465 val_loss= 8.08478 val_acc= 0.64139 time= 13.30105\n",
      "Epoch: 1516 train_loss= 1.65748 train_acc= 0.25802 val_loss= 8.08366 val_acc= 0.61568 time= 15.01442\n",
      "Epoch: 1517 train_loss= 1.64877 train_acc= 0.24005 val_loss= 8.08300 val_acc= 0.62853 time= 12.48193\n",
      "Epoch: 1518 train_loss= 1.66834 train_acc= 0.24390 val_loss= 8.08285 val_acc= 0.69923 time= 11.59147\n",
      "Epoch: 1519 train_loss= 1.65070 train_acc= 0.25546 val_loss= 8.08274 val_acc= 0.72751 time= 12.27935\n",
      "Epoch: 1520 train_loss= 1.65617 train_acc= 0.27471 val_loss= 8.08297 val_acc= 0.80206 time= 10.74957\n",
      "Epoch: 1521 train_loss= 1.65400 train_acc= 0.25417 val_loss= 8.08286 val_acc= 0.83933 time= 10.48909\n",
      "Epoch: 1522 train_loss= 1.61459 train_acc= 0.24775 val_loss= 8.08301 val_acc= 0.86632 time= 11.88374\n",
      "Epoch: 1523 train_loss= 1.64459 train_acc= 0.25546 val_loss= 8.08293 val_acc= 0.85219 time= 12.11006\n",
      "Epoch: 1524 train_loss= 1.64501 train_acc= 0.25160 val_loss= 8.08329 val_acc= 0.83033 time= 11.51884\n",
      "Epoch: 1525 train_loss= 1.71054 train_acc= 0.22850 val_loss= 8.08361 val_acc= 0.80720 time= 10.73432\n",
      "Epoch: 1526 train_loss= 1.65511 train_acc= 0.23877 val_loss= 8.08406 val_acc= 0.77249 time= 12.03116\n",
      "Epoch: 1527 train_loss= 1.65590 train_acc= 0.24647 val_loss= 8.08472 val_acc= 0.72237 time= 14.32874\n",
      "Epoch: 1528 train_loss= 1.65788 train_acc= 0.23748 val_loss= 8.08617 val_acc= 0.71337 time= 13.55409\n",
      "Epoch: 1529 train_loss= 1.69434 train_acc= 0.22721 val_loss= 8.08889 val_acc= 0.73393 time= 13.65597\n",
      "Epoch: 1530 train_loss= 1.65845 train_acc= 0.25417 val_loss= 8.09251 val_acc= 0.75064 time= 12.64299\n",
      "Epoch: 1531 train_loss= 1.66892 train_acc= 0.24519 val_loss= 8.09577 val_acc= 0.79563 time= 10.22101\n",
      "Epoch: 1532 train_loss= 1.69596 train_acc= 0.22850 val_loss= 8.09890 val_acc= 0.81491 time= 10.74081\n",
      "Epoch: 1533 train_loss= 1.66564 train_acc= 0.23877 val_loss= 8.10133 val_acc= 0.82262 time= 10.18041\n",
      "Epoch: 1534 train_loss= 1.68818 train_acc= 0.24519 val_loss= 8.10202 val_acc= 0.82391 time= 10.15141\n",
      "Epoch: 1535 train_loss= 1.68128 train_acc= 0.24262 val_loss= 8.10158 val_acc= 0.81491 time= 10.26757\n",
      "Epoch: 1536 train_loss= 1.68613 train_acc= 0.25674 val_loss= 8.10040 val_acc= 0.77635 time= 10.15362\n",
      "Epoch: 1537 train_loss= 1.67145 train_acc= 0.22978 val_loss= 8.09984 val_acc= 0.73907 time= 10.15284\n",
      "Epoch: 1538 train_loss= 1.68456 train_acc= 0.26573 val_loss= 8.09912 val_acc= 0.69152 time= 10.15446\n",
      "Epoch: 1539 train_loss= 1.70105 train_acc= 0.24904 val_loss= 8.09861 val_acc= 0.61440 time= 10.14295\n",
      "Epoch: 1540 train_loss= 1.66414 train_acc= 0.27471 val_loss= 8.09884 val_acc= 0.60797 time= 10.15406\n",
      "Epoch: 1541 train_loss= 1.63580 train_acc= 0.27214 val_loss= 8.09966 val_acc= 0.62853 time= 10.24341\n",
      "Epoch: 1542 train_loss= 1.66431 train_acc= 0.24390 val_loss= 8.09925 val_acc= 0.72237 time= 10.15002\n",
      "Epoch: 1543 train_loss= 1.65251 train_acc= 0.24262 val_loss= 8.09889 val_acc= 0.79949 time= 10.15611\n",
      "Epoch: 1544 train_loss= 1.66587 train_acc= 0.25674 val_loss= 8.09758 val_acc= 0.82905 time= 10.14359\n",
      "Epoch: 1545 train_loss= 1.71790 train_acc= 0.25931 val_loss= 8.09554 val_acc= 0.83290 time= 10.14626\n",
      "Epoch: 1546 train_loss= 1.65647 train_acc= 0.23363 val_loss= 8.09315 val_acc= 0.83419 time= 10.17504\n",
      "Epoch: 1547 train_loss= 1.66977 train_acc= 0.25289 val_loss= 8.09080 val_acc= 0.82776 time= 10.29177\n",
      "Epoch: 1548 train_loss= 1.65678 train_acc= 0.25546 val_loss= 8.08920 val_acc= 0.82262 time= 10.74679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1549 train_loss= 1.69188 train_acc= 0.27086 val_loss= 8.08710 val_acc= 0.80720 time= 10.17674\n",
      "Epoch: 1550 train_loss= 1.68826 train_acc= 0.25289 val_loss= 8.08574 val_acc= 0.75578 time= 10.18377\n",
      "Epoch: 1551 train_loss= 1.64460 train_acc= 0.26444 val_loss= 8.08520 val_acc= 0.66324 time= 10.19314\n",
      "Epoch: 1552 train_loss= 1.63123 train_acc= 0.24904 val_loss= 8.08480 val_acc= 0.57584 time= 10.22416\n",
      "Epoch: 1553 train_loss= 1.69950 train_acc= 0.27343 val_loss= 8.08422 val_acc= 0.47943 time= 10.32600\n",
      "Epoch: 1554 train_loss= 1.64236 train_acc= 0.26829 val_loss= 8.08395 val_acc= 0.46658 time= 10.25023\n",
      "Epoch: 1555 train_loss= 1.65747 train_acc= 0.25674 val_loss= 8.08354 val_acc= 0.37532 time= 10.27551\n",
      "Epoch: 1556 train_loss= 1.64790 train_acc= 0.26573 val_loss= 8.08343 val_acc= 0.29692 time= 10.21833\n",
      "Epoch: 1557 train_loss= 1.68487 train_acc= 0.27214 val_loss= 8.08356 val_acc= 0.26093 time= 10.18696\n",
      "Epoch: 1558 train_loss= 1.67366 train_acc= 0.26573 val_loss= 8.08399 val_acc= 0.25964 time= 10.21189\n",
      "Epoch: 1559 train_loss= 1.64365 train_acc= 0.28498 val_loss= 8.08552 val_acc= 0.24550 time= 10.41893\n",
      "Epoch: 1560 train_loss= 1.65188 train_acc= 0.26187 val_loss= 8.08696 val_acc= 0.22622 time= 10.16313\n",
      "Epoch: 1561 train_loss= 1.66442 train_acc= 0.26316 val_loss= 8.08685 val_acc= 0.24036 time= 10.18341\n",
      "Epoch: 1562 train_loss= 1.62529 train_acc= 0.27343 val_loss= 8.08712 val_acc= 0.25964 time= 10.87443\n",
      "Epoch: 1563 train_loss= 1.70064 train_acc= 0.26958 val_loss= 8.08740 val_acc= 0.30206 time= 10.20703\n",
      "Epoch: 1564 train_loss= 1.65300 train_acc= 0.27728 val_loss= 8.08785 val_acc= 0.35733 time= 10.20872\n",
      "Epoch: 1565 train_loss= 1.65803 train_acc= 0.25417 val_loss= 8.08795 val_acc= 0.47172 time= 10.46890\n",
      "Epoch: 1566 train_loss= 1.63719 train_acc= 0.27214 val_loss= 8.08775 val_acc= 0.65553 time= 10.28089\n",
      "Epoch: 1567 train_loss= 1.65504 train_acc= 0.25931 val_loss= 8.08783 val_acc= 0.74550 time= 10.18418\n",
      "Epoch: 1568 train_loss= 1.62274 train_acc= 0.25546 val_loss= 8.08798 val_acc= 0.78535 time= 10.18070\n",
      "Epoch: 1569 train_loss= 1.65577 train_acc= 0.25417 val_loss= 8.08831 val_acc= 0.79563 time= 10.18548\n",
      "Epoch: 1570 train_loss= 1.67728 train_acc= 0.24134 val_loss= 8.08912 val_acc= 0.80334 time= 10.18201\n",
      "Epoch: 1571 train_loss= 1.68083 train_acc= 0.25032 val_loss= 8.09062 val_acc= 0.81362 time= 10.19575\n",
      "Epoch: 1572 train_loss= 1.68065 train_acc= 0.25160 val_loss= 8.09100 val_acc= 0.81234 time= 10.30969\n",
      "Epoch: 1573 train_loss= 1.64898 train_acc= 0.24775 val_loss= 8.09212 val_acc= 0.80334 time= 10.19102\n",
      "Epoch: 1574 train_loss= 1.62856 train_acc= 0.25546 val_loss= 8.09380 val_acc= 0.75321 time= 10.17118\n",
      "Epoch: 1575 train_loss= 1.65953 train_acc= 0.22721 val_loss= 8.09436 val_acc= 0.65167 time= 10.19252\n",
      "Epoch: 1576 train_loss= 1.67455 train_acc= 0.20796 val_loss= 8.09332 val_acc= 0.73008 time= 10.19731\n",
      "Epoch: 1577 train_loss= 1.62720 train_acc= 0.21181 val_loss= 8.09197 val_acc= 0.80206 time= 10.75702\n",
      "Epoch: 1578 train_loss= 1.61418 train_acc= 0.24134 val_loss= 8.09081 val_acc= 0.82519 time= 14.28000\n",
      "Epoch: 1579 train_loss= 1.68304 train_acc= 0.21694 val_loss= 8.09003 val_acc= 0.85347 time= 10.99885\n",
      "Epoch: 1580 train_loss= 1.69389 train_acc= 0.25160 val_loss= 8.08889 val_acc= 0.89974 time= 10.30762\n",
      "Epoch: 1581 train_loss= 1.68304 train_acc= 0.24647 val_loss= 8.08826 val_acc= 0.93959 time= 11.32317\n",
      "Epoch: 1582 train_loss= 1.63903 train_acc= 0.24904 val_loss= 8.08792 val_acc= 0.94859 time= 10.90146\n",
      "Epoch: 1583 train_loss= 1.71630 train_acc= 0.24390 val_loss= 8.08726 val_acc= 0.94987 time= 12.80851\n",
      "Epoch: 1584 train_loss= 1.69795 train_acc= 0.23620 val_loss= 8.08705 val_acc= 0.94087 time= 11.90209\n",
      "Epoch: 1585 train_loss= 1.68041 train_acc= 0.24519 val_loss= 8.08703 val_acc= 0.92288 time= 11.82883\n",
      "Epoch: 1586 train_loss= 1.69222 train_acc= 0.24904 val_loss= 8.08677 val_acc= 0.90103 time= 11.88934\n",
      "Epoch: 1587 train_loss= 1.64006 train_acc= 0.24262 val_loss= 8.08605 val_acc= 0.87918 time= 11.94349\n",
      "Epoch: 1588 train_loss= 1.67602 train_acc= 0.24262 val_loss= 8.08641 val_acc= 0.85476 time= 12.12379\n",
      "Epoch: 1589 train_loss= 1.69592 train_acc= 0.23363 val_loss= 8.08827 val_acc= 0.82905 time= 12.50715\n",
      "Epoch: 1590 train_loss= 1.65540 train_acc= 0.23363 val_loss= 8.08987 val_acc= 0.81877 time= 11.82860\n",
      "Epoch: 1591 train_loss= 1.65923 train_acc= 0.22721 val_loss= 8.09136 val_acc= 0.84319 time= 11.87396\n",
      "Epoch: 1592 train_loss= 1.63449 train_acc= 0.23620 val_loss= 8.09219 val_acc= 0.85476 time= 11.93620\n",
      "Epoch: 1593 train_loss= 1.66569 train_acc= 0.23235 val_loss= 8.09250 val_acc= 0.89075 time= 12.06830\n",
      "Epoch: 1594 train_loss= 1.67634 train_acc= 0.23492 val_loss= 8.09196 val_acc= 0.90103 time= 13.11766\n",
      "Epoch: 1595 train_loss= 1.66386 train_acc= 0.25160 val_loss= 8.09104 val_acc= 0.88560 time= 15.62458\n",
      "Epoch: 1596 train_loss= 1.68166 train_acc= 0.22465 val_loss= 8.08971 val_acc= 0.85861 time= 13.97172\n",
      "Epoch: 1597 train_loss= 1.66878 train_acc= 0.23620 val_loss= 8.08838 val_acc= 0.83419 time= 14.13958\n",
      "Epoch: 1598 train_loss= 1.64037 train_acc= 0.26187 val_loss= 8.08679 val_acc= 0.84190 time= 14.87205\n",
      "Epoch: 1599 train_loss= 1.63842 train_acc= 0.21309 val_loss= 8.08543 val_acc= 0.82776 time= 12.96403\n",
      "Epoch: 1600 train_loss= 1.65556 train_acc= 0.22593 val_loss= 8.08395 val_acc= 0.79949 time= 12.12147\n",
      "Epoch: 1601 train_loss= 1.66405 train_acc= 0.24262 val_loss= 8.08249 val_acc= 0.79434 time= 12.03956\n",
      "Epoch: 1602 train_loss= 1.67973 train_acc= 0.24262 val_loss= 8.08019 val_acc= 0.84062 time= 12.00229\n",
      "Epoch: 1603 train_loss= 1.63418 train_acc= 0.24775 val_loss= 8.07790 val_acc= 0.83162 time= 12.22123\n",
      "Epoch: 1604 train_loss= 1.68489 train_acc= 0.26958 val_loss= 8.07623 val_acc= 0.77121 time= 11.98313\n",
      "Epoch: 1605 train_loss= 1.70681 train_acc= 0.26958 val_loss= 8.07480 val_acc= 0.60154 time= 12.04789\n",
      "Epoch: 1606 train_loss= 1.64949 train_acc= 0.24904 val_loss= 8.07421 val_acc= 0.47815 time= 12.01963\n",
      "Epoch: 1607 train_loss= 1.66410 train_acc= 0.25417 val_loss= 8.07425 val_acc= 0.41517 time= 12.32246\n",
      "Epoch: 1608 train_loss= 1.63968 train_acc= 0.27086 val_loss= 8.07508 val_acc= 0.51799 time= 12.19871\n",
      "Epoch: 1609 train_loss= 1.66737 train_acc= 0.26187 val_loss= 8.07602 val_acc= 0.65167 time= 12.05429\n",
      "Epoch: 1610 train_loss= 1.69213 train_acc= 0.25931 val_loss= 8.07685 val_acc= 0.83033 time= 12.32472\n",
      "Epoch: 1611 train_loss= 1.65397 train_acc= 0.24647 val_loss= 8.07703 val_acc= 0.88046 time= 12.17257\n",
      "Epoch: 1612 train_loss= 1.69231 train_acc= 0.24134 val_loss= 8.07764 val_acc= 0.91774 time= 13.20525\n",
      "Epoch: 1613 train_loss= 1.64725 train_acc= 0.23235 val_loss= 8.07830 val_acc= 0.92288 time= 14.72640\n",
      "Epoch: 1614 train_loss= 1.70491 train_acc= 0.25546 val_loss= 8.07877 val_acc= 0.95116 time= 12.99718\n",
      "Epoch: 1615 train_loss= 1.67212 train_acc= 0.24775 val_loss= 8.07882 val_acc= 0.92545 time= 11.97533\n",
      "Epoch: 1616 train_loss= 1.63484 train_acc= 0.24262 val_loss= 8.07898 val_acc= 0.88817 time= 12.02319\n",
      "Epoch: 1617 train_loss= 1.65189 train_acc= 0.23877 val_loss= 8.07937 val_acc= 0.80591 time= 12.10819\n",
      "Epoch: 1618 train_loss= 1.63377 train_acc= 0.23620 val_loss= 8.07999 val_acc= 0.70951 time= 12.10336\n",
      "Epoch: 1619 train_loss= 1.64103 train_acc= 0.23620 val_loss= 8.08106 val_acc= 0.67481 time= 12.15788\n",
      "Epoch: 1620 train_loss= 1.68840 train_acc= 0.23235 val_loss= 8.08311 val_acc= 0.81234 time= 12.03416\n",
      "Epoch: 1621 train_loss= 1.65573 train_acc= 0.25417 val_loss= 8.08474 val_acc= 0.87918 time= 12.08087\n",
      "Epoch: 1622 train_loss= 1.70078 train_acc= 0.23877 val_loss= 8.08655 val_acc= 0.91131 time= 12.06003\n",
      "Epoch: 1623 train_loss= 1.67687 train_acc= 0.24134 val_loss= 8.08816 val_acc= 0.89589 time= 12.06637\n",
      "Epoch: 1624 train_loss= 1.64445 train_acc= 0.23235 val_loss= 8.08960 val_acc= 0.86889 time= 12.10539\n",
      "Epoch: 1625 train_loss= 1.68996 train_acc= 0.25032 val_loss= 8.09052 val_acc= 0.86118 time= 12.09130\n",
      "Epoch: 1626 train_loss= 1.66122 train_acc= 0.25289 val_loss= 8.09048 val_acc= 0.86632 time= 11.95161\n",
      "Epoch: 1627 train_loss= 1.68245 train_acc= 0.22850 val_loss= 8.08987 val_acc= 0.87532 time= 13.34438\n",
      "Epoch: 1628 train_loss= 1.67688 train_acc= 0.23620 val_loss= 8.08948 val_acc= 0.90874 time= 13.66823\n",
      "Epoch: 1629 train_loss= 1.71096 train_acc= 0.25032 val_loss= 8.08949 val_acc= 0.93316 time= 12.14647\n",
      "Epoch: 1630 train_loss= 1.67459 train_acc= 0.24005 val_loss= 8.08943 val_acc= 0.94859 time= 12.09949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1631 train_loss= 1.66819 train_acc= 0.23620 val_loss= 8.08911 val_acc= 0.95244 time= 12.02154\n",
      "Epoch: 1632 train_loss= 1.66716 train_acc= 0.25289 val_loss= 8.08840 val_acc= 0.95116 time= 12.05427\n",
      "Epoch: 1633 train_loss= 1.71769 train_acc= 0.24390 val_loss= 8.08765 val_acc= 0.94602 time= 11.97112\n",
      "Epoch: 1634 train_loss= 1.68909 train_acc= 0.24134 val_loss= 8.08713 val_acc= 0.94473 time= 11.92887\n",
      "Epoch: 1635 train_loss= 1.68757 train_acc= 0.24134 val_loss= 8.08638 val_acc= 0.94216 time= 12.03521\n",
      "Epoch: 1636 train_loss= 1.66604 train_acc= 0.24904 val_loss= 8.08575 val_acc= 0.92288 time= 11.97456\n",
      "Epoch: 1637 train_loss= 1.71101 train_acc= 0.22978 val_loss= 8.08515 val_acc= 0.89589 time= 11.96119\n",
      "Epoch: 1638 train_loss= 1.72270 train_acc= 0.23107 val_loss= 8.08399 val_acc= 0.88946 time= 12.68744\n",
      "Epoch: 1639 train_loss= 1.66719 train_acc= 0.24904 val_loss= 8.08275 val_acc= 0.88175 time= 11.23083\n",
      "Epoch: 1640 train_loss= 1.65936 train_acc= 0.25160 val_loss= 8.08204 val_acc= 0.86761 time= 10.29321\n",
      "Epoch: 1641 train_loss= 1.68446 train_acc= 0.25160 val_loss= 8.08163 val_acc= 0.87404 time= 10.22038\n",
      "Epoch: 1642 train_loss= 1.69650 train_acc= 0.23492 val_loss= 8.08170 val_acc= 0.85990 time= 10.21410\n",
      "Epoch: 1643 train_loss= 1.66177 train_acc= 0.27343 val_loss= 8.08218 val_acc= 0.83676 time= 11.66208\n",
      "Epoch: 1644 train_loss= 1.69328 train_acc= 0.23492 val_loss= 8.08294 val_acc= 0.81877 time= 11.95064\n",
      "Epoch: 1645 train_loss= 1.68865 train_acc= 0.26958 val_loss= 8.08422 val_acc= 0.82134 time= 12.00202\n",
      "Epoch: 1646 train_loss= 1.64658 train_acc= 0.25032 val_loss= 8.08476 val_acc= 0.85347 time= 11.97263\n",
      "Epoch: 1647 train_loss= 1.70676 train_acc= 0.23748 val_loss= 8.08561 val_acc= 0.85347 time= 11.97771\n",
      "Epoch: 1648 train_loss= 1.63030 train_acc= 0.25289 val_loss= 8.08627 val_acc= 0.85604 time= 11.97916\n",
      "Epoch: 1649 train_loss= 1.65123 train_acc= 0.26316 val_loss= 8.08626 val_acc= 0.83933 time= 12.02984\n",
      "Epoch: 1650 train_loss= 1.68268 train_acc= 0.24647 val_loss= 8.08614 val_acc= 0.83290 time= 12.08302\n",
      "Epoch: 1651 train_loss= 1.65383 train_acc= 0.25931 val_loss= 8.08637 val_acc= 0.81105 time= 11.97724\n",
      "Epoch: 1652 train_loss= 1.64026 train_acc= 0.25160 val_loss= 8.08719 val_acc= 0.79177 time= 11.95159\n",
      "Epoch: 1653 train_loss= 1.66701 train_acc= 0.24262 val_loss= 8.08786 val_acc= 0.76221 time= 11.93559\n",
      "Epoch: 1654 train_loss= 1.64780 train_acc= 0.24904 val_loss= 8.08762 val_acc= 0.84062 time= 11.99509\n",
      "Epoch: 1655 train_loss= 1.66519 train_acc= 0.25289 val_loss= 8.08684 val_acc= 0.87404 time= 12.12802\n",
      "Epoch: 1656 train_loss= 1.62695 train_acc= 0.25289 val_loss= 8.08669 val_acc= 0.90488 time= 12.06679\n",
      "Epoch: 1657 train_loss= 1.65041 train_acc= 0.25417 val_loss= 8.08649 val_acc= 0.91645 time= 12.07127\n",
      "Epoch: 1658 train_loss= 1.66902 train_acc= 0.25546 val_loss= 8.08627 val_acc= 0.91131 time= 11.97377\n",
      "Epoch: 1659 train_loss= 1.65064 train_acc= 0.23620 val_loss= 8.08672 val_acc= 0.90103 time= 11.96104\n",
      "Epoch: 1660 train_loss= 1.66567 train_acc= 0.24390 val_loss= 8.08701 val_acc= 0.89203 time= 12.04512\n",
      "Epoch: 1661 train_loss= 1.66832 train_acc= 0.23492 val_loss= 8.08695 val_acc= 0.87275 time= 11.96504\n",
      "Epoch: 1662 train_loss= 1.67558 train_acc= 0.22978 val_loss= 8.08624 val_acc= 0.84062 time= 12.02407\n",
      "Epoch: 1663 train_loss= 1.63069 train_acc= 0.26187 val_loss= 8.08582 val_acc= 0.80848 time= 12.64900\n",
      "Epoch: 1664 train_loss= 1.67721 train_acc= 0.25674 val_loss= 8.08480 val_acc= 0.75193 time= 11.96567\n",
      "Epoch: 1665 train_loss= 1.64336 train_acc= 0.26829 val_loss= 8.08379 val_acc= 0.68509 time= 11.94958\n",
      "Epoch: 1666 train_loss= 1.62653 train_acc= 0.26444 val_loss= 8.08311 val_acc= 0.63111 time= 11.94897\n",
      "Epoch: 1667 train_loss= 1.64929 train_acc= 0.25546 val_loss= 8.08315 val_acc= 0.60540 time= 11.95233\n",
      "Epoch: 1668 train_loss= 1.61920 train_acc= 0.26444 val_loss= 8.08280 val_acc= 0.55527 time= 11.95558\n",
      "Epoch: 1669 train_loss= 1.67487 train_acc= 0.27214 val_loss= 8.08354 val_acc= 0.51542 time= 12.00730\n",
      "Epoch: 1670 train_loss= 1.63905 train_acc= 0.26701 val_loss= 8.08377 val_acc= 0.44473 time= 11.92786\n",
      "Epoch: 1671 train_loss= 1.65159 train_acc= 0.25931 val_loss= 8.08392 val_acc= 0.44602 time= 11.98315\n",
      "Epoch: 1672 train_loss= 1.67176 train_acc= 0.24519 val_loss= 8.08306 val_acc= 0.44344 time= 11.91394\n",
      "Epoch: 1673 train_loss= 1.62440 train_acc= 0.26187 val_loss= 8.08272 val_acc= 0.52314 time= 11.97617\n",
      "Epoch: 1674 train_loss= 1.63115 train_acc= 0.25417 val_loss= 8.08243 val_acc= 0.55270 time= 12.16381\n",
      "Epoch: 1675 train_loss= 1.70460 train_acc= 0.25802 val_loss= 8.08172 val_acc= 0.57069 time= 11.97569\n",
      "Epoch: 1676 train_loss= 1.67474 train_acc= 0.25931 val_loss= 8.08139 val_acc= 0.61954 time= 12.08496\n",
      "Epoch: 1677 train_loss= 1.63648 train_acc= 0.26958 val_loss= 8.08041 val_acc= 0.62853 time= 12.01747\n",
      "Epoch: 1678 train_loss= 1.63055 train_acc= 0.26444 val_loss= 8.08004 val_acc= 0.65553 time= 11.98887\n",
      "Epoch: 1679 train_loss= 1.66844 train_acc= 0.25160 val_loss= 8.07985 val_acc= 0.68123 time= 11.93503\n",
      "Epoch: 1680 train_loss= 1.65164 train_acc= 0.26187 val_loss= 8.07933 val_acc= 0.70951 time= 11.96310\n",
      "Epoch: 1681 train_loss= 1.68191 train_acc= 0.24904 val_loss= 8.07922 val_acc= 0.72751 time= 12.27145\n",
      "Epoch: 1682 train_loss= 1.64317 train_acc= 0.25546 val_loss= 8.07878 val_acc= 0.72494 time= 12.08546\n",
      "Epoch: 1683 train_loss= 1.64030 train_acc= 0.27471 val_loss= 8.07862 val_acc= 0.70951 time= 11.94515\n",
      "Epoch: 1684 train_loss= 1.66488 train_acc= 0.24904 val_loss= 8.07877 val_acc= 0.71337 time= 11.90682\n",
      "Epoch: 1685 train_loss= 1.66341 train_acc= 0.25546 val_loss= 8.07909 val_acc= 0.76864 time= 11.95237\n",
      "Epoch: 1686 train_loss= 1.66703 train_acc= 0.24262 val_loss= 8.07937 val_acc= 0.80591 time= 11.97827\n",
      "Epoch: 1687 train_loss= 1.66833 train_acc= 0.26701 val_loss= 8.08053 val_acc= 0.84447 time= 12.03132\n",
      "Epoch: 1688 train_loss= 1.66650 train_acc= 0.25802 val_loss= 8.08142 val_acc= 0.85861 time= 12.62614\n",
      "Epoch: 1689 train_loss= 1.66583 train_acc= 0.25160 val_loss= 8.08245 val_acc= 0.87532 time= 11.96229\n",
      "Epoch: 1690 train_loss= 1.63570 train_acc= 0.25802 val_loss= 8.08375 val_acc= 0.90488 time= 11.90582\n",
      "Epoch: 1691 train_loss= 1.64701 train_acc= 0.25417 val_loss= 8.08521 val_acc= 0.93702 time= 11.94730\n",
      "Epoch: 1692 train_loss= 1.62296 train_acc= 0.24904 val_loss= 8.08616 val_acc= 0.95373 time= 11.93874\n",
      "Epoch: 1693 train_loss= 1.64450 train_acc= 0.24647 val_loss= 8.08698 val_acc= 0.94473 time= 11.87435\n",
      "Epoch: 1694 train_loss= 1.69194 train_acc= 0.23748 val_loss= 8.08752 val_acc= 0.90746 time= 11.91479\n",
      "Epoch: 1695 train_loss= 1.66722 train_acc= 0.25160 val_loss= 8.08841 val_acc= 0.88689 time= 11.84527\n",
      "Epoch: 1696 train_loss= 1.65079 train_acc= 0.25032 val_loss= 8.08894 val_acc= 0.87018 time= 11.86914\n",
      "Epoch: 1697 train_loss= 1.65090 train_acc= 0.24134 val_loss= 8.08939 val_acc= 0.86375 time= 11.87961\n",
      "Epoch: 1698 train_loss= 1.63726 train_acc= 0.24775 val_loss= 8.08977 val_acc= 0.85604 time= 11.91182\n",
      "Epoch: 1699 train_loss= 1.66716 train_acc= 0.25802 val_loss= 8.08945 val_acc= 0.84833 time= 12.03691\n",
      "Epoch: 1700 train_loss= 1.65811 train_acc= 0.24519 val_loss= 8.08838 val_acc= 0.84961 time= 11.91950\n",
      "Epoch: 1701 train_loss= 1.65988 train_acc= 0.24904 val_loss= 8.08721 val_acc= 0.84190 time= 11.93292\n",
      "Epoch: 1702 train_loss= 1.62615 train_acc= 0.25802 val_loss= 8.08638 val_acc= 0.82776 time= 12.04001\n",
      "Epoch: 1703 train_loss= 1.64135 train_acc= 0.26444 val_loss= 8.08588 val_acc= 0.83162 time= 11.84136\n",
      "Epoch: 1704 train_loss= 1.64745 train_acc= 0.24390 val_loss= 8.08551 val_acc= 0.84062 time= 11.93812\n",
      "Epoch: 1705 train_loss= 1.66953 train_acc= 0.24390 val_loss= 8.08462 val_acc= 0.84190 time= 11.89482\n",
      "Epoch: 1706 train_loss= 1.65822 train_acc= 0.24134 val_loss= 8.08480 val_acc= 0.84961 time= 11.29117\n",
      "Epoch: 1707 train_loss= 1.63218 train_acc= 0.23363 val_loss= 8.08574 val_acc= 0.84576 time= 10.39010\n",
      "Epoch: 1708 train_loss= 1.69761 train_acc= 0.22465 val_loss= 8.08680 val_acc= 0.80977 time= 10.44783\n",
      "Epoch: 1709 train_loss= 1.65305 train_acc= 0.23877 val_loss= 8.08795 val_acc= 0.76478 time= 11.75574\n",
      "Epoch: 1710 train_loss= 1.73355 train_acc= 0.25674 val_loss= 8.08762 val_acc= 0.72108 time= 11.92779\n",
      "Epoch: 1711 train_loss= 1.67959 train_acc= 0.24005 val_loss= 8.08635 val_acc= 0.74422 time= 11.97410\n",
      "Epoch: 1712 train_loss= 1.61946 train_acc= 0.23748 val_loss= 8.08491 val_acc= 0.80206 time= 11.97693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1713 train_loss= 1.69234 train_acc= 0.21694 val_loss= 8.08352 val_acc= 0.90360 time= 12.02432\n",
      "Epoch: 1714 train_loss= 1.67376 train_acc= 0.24005 val_loss= 8.08243 val_acc= 0.90488 time= 12.86000\n",
      "Epoch: 1715 train_loss= 1.67313 train_acc= 0.23620 val_loss= 8.08200 val_acc= 0.90360 time= 11.94961\n",
      "Epoch: 1716 train_loss= 1.67075 train_acc= 0.26059 val_loss= 8.08173 val_acc= 0.88046 time= 12.00440\n",
      "Epoch: 1717 train_loss= 1.64150 train_acc= 0.24775 val_loss= 8.08149 val_acc= 0.82262 time= 12.01163\n",
      "Epoch: 1718 train_loss= 1.64067 train_acc= 0.26316 val_loss= 8.08095 val_acc= 0.75064 time= 11.31630\n",
      "Epoch: 1719 train_loss= 1.65097 train_acc= 0.25674 val_loss= 8.08117 val_acc= 0.66581 time= 10.31072\n",
      "Epoch: 1720 train_loss= 1.65382 train_acc= 0.25160 val_loss= 8.08172 val_acc= 0.72879 time= 10.26047\n",
      "Epoch: 1721 train_loss= 1.65845 train_acc= 0.25032 val_loss= 8.08309 val_acc= 0.82905 time= 10.23085\n",
      "Epoch: 1722 train_loss= 1.66038 train_acc= 0.25674 val_loss= 8.08401 val_acc= 0.89460 time= 10.23395\n",
      "Epoch: 1723 train_loss= 1.67472 train_acc= 0.22721 val_loss= 8.08554 val_acc= 0.92416 time= 10.23999\n",
      "Epoch: 1724 train_loss= 1.68070 train_acc= 0.23620 val_loss= 8.08558 val_acc= 0.94087 time= 11.87341\n",
      "Epoch: 1725 train_loss= 1.68732 train_acc= 0.23748 val_loss= 8.08486 val_acc= 0.93573 time= 11.92917\n",
      "Epoch: 1726 train_loss= 1.64378 train_acc= 0.23107 val_loss= 8.08528 val_acc= 0.94087 time= 11.99022\n",
      "Epoch: 1727 train_loss= 1.65269 train_acc= 0.23620 val_loss= 8.08585 val_acc= 0.93059 time= 11.95856\n",
      "Epoch: 1728 train_loss= 1.63400 train_acc= 0.23235 val_loss= 8.08647 val_acc= 0.91902 time= 11.96173\n",
      "Epoch: 1729 train_loss= 1.63515 train_acc= 0.24262 val_loss= 8.08723 val_acc= 0.88046 time= 11.95595\n",
      "Epoch: 1730 train_loss= 1.65013 train_acc= 0.23363 val_loss= 8.08820 val_acc= 0.81362 time= 12.04855\n",
      "Epoch: 1731 train_loss= 1.65517 train_acc= 0.25417 val_loss= 8.08880 val_acc= 0.81491 time= 11.96728\n",
      "Epoch: 1732 train_loss= 1.70005 train_acc= 0.24519 val_loss= 8.08832 val_acc= 0.87404 time= 11.94703\n",
      "Epoch: 1733 train_loss= 1.65749 train_acc= 0.25160 val_loss= 8.08822 val_acc= 0.90874 time= 12.09169\n",
      "Epoch: 1734 train_loss= 1.66104 train_acc= 0.24904 val_loss= 8.08824 val_acc= 0.91260 time= 11.90912\n",
      "Epoch: 1735 train_loss= 1.66893 train_acc= 0.25546 val_loss= 8.08831 val_acc= 0.89974 time= 11.92313\n",
      "Epoch: 1736 train_loss= 1.66530 train_acc= 0.25417 val_loss= 8.08821 val_acc= 0.88817 time= 12.01988\n",
      "Epoch: 1737 train_loss= 1.68172 train_acc= 0.23877 val_loss= 8.08838 val_acc= 0.88046 time= 11.98150\n",
      "Epoch: 1738 train_loss= 1.64140 train_acc= 0.24134 val_loss= 8.08797 val_acc= 0.87661 time= 11.94556\n",
      "Epoch: 1739 train_loss= 1.68579 train_acc= 0.24519 val_loss= 8.08728 val_acc= 0.84961 time= 11.96158\n",
      "Epoch: 1740 train_loss= 1.64578 train_acc= 0.25931 val_loss= 8.08591 val_acc= 0.83033 time= 13.21951\n",
      "Epoch: 1741 train_loss= 1.63419 train_acc= 0.26958 val_loss= 8.08464 val_acc= 0.78792 time= 12.61822\n",
      "Epoch: 1742 train_loss= 1.66087 train_acc= 0.25417 val_loss= 8.08364 val_acc= 0.73265 time= 11.92486\n",
      "Epoch: 1743 train_loss= 1.68550 train_acc= 0.26316 val_loss= 8.08298 val_acc= 0.69794 time= 12.15640\n",
      "Epoch: 1744 train_loss= 1.64938 train_acc= 0.23492 val_loss= 8.08224 val_acc= 0.63239 time= 12.61289\n",
      "Epoch: 1745 train_loss= 1.64008 train_acc= 0.24005 val_loss= 8.08167 val_acc= 0.58483 time= 15.26754\n",
      "Epoch: 1746 train_loss= 1.69069 train_acc= 0.24519 val_loss= 8.08164 val_acc= 0.58226 time= 11.01663\n",
      "Epoch: 1747 train_loss= 1.62760 train_acc= 0.25931 val_loss= 8.08165 val_acc= 0.64139 time= 11.65310\n",
      "Epoch: 1748 train_loss= 1.65951 train_acc= 0.23877 val_loss= 8.08112 val_acc= 0.70437 time= 11.28322\n",
      "Epoch: 1749 train_loss= 1.71377 train_acc= 0.25931 val_loss= 8.08071 val_acc= 0.76864 time= 11.85574\n",
      "Epoch: 1750 train_loss= 1.61863 train_acc= 0.27471 val_loss= 8.08060 val_acc= 0.79177 time= 11.58870\n",
      "Epoch: 1751 train_loss= 1.64078 train_acc= 0.25032 val_loss= 8.08069 val_acc= 0.80720 time= 10.95772\n",
      "Epoch: 1752 train_loss= 1.63257 train_acc= 0.25931 val_loss= 8.08150 val_acc= 0.82776 time= 10.82451\n",
      "Epoch: 1753 train_loss= 1.63787 train_acc= 0.24519 val_loss= 8.08224 val_acc= 0.83033 time= 11.04163\n",
      "Epoch: 1754 train_loss= 1.63736 train_acc= 0.25802 val_loss= 8.08304 val_acc= 0.83548 time= 11.38481\n",
      "Epoch: 1755 train_loss= 1.66269 train_acc= 0.24775 val_loss= 8.08381 val_acc= 0.84062 time= 10.70584\n",
      "Epoch: 1756 train_loss= 1.64996 train_acc= 0.24134 val_loss= 8.08354 val_acc= 0.83290 time= 10.58240\n",
      "Epoch: 1757 train_loss= 1.65344 train_acc= 0.26059 val_loss= 8.08228 val_acc= 0.81362 time= 11.50101\n",
      "Epoch: 1758 train_loss= 1.66848 train_acc= 0.24775 val_loss= 8.08126 val_acc= 0.78920 time= 11.81390\n",
      "Epoch: 1759 train_loss= 1.64560 train_acc= 0.25032 val_loss= 8.08040 val_acc= 0.74550 time= 11.02532\n",
      "Epoch: 1760 train_loss= 1.63717 train_acc= 0.26059 val_loss= 8.08016 val_acc= 0.69409 time= 11.04190\n",
      "Epoch: 1761 train_loss= 1.65635 train_acc= 0.26444 val_loss= 8.08000 val_acc= 0.58098 time= 10.87281\n",
      "Epoch: 1762 train_loss= 1.64242 train_acc= 0.25289 val_loss= 8.07997 val_acc= 0.49614 time= 11.37486\n",
      "Epoch: 1763 train_loss= 1.62365 train_acc= 0.25417 val_loss= 8.08009 val_acc= 0.52314 time= 11.09357\n",
      "Epoch: 1764 train_loss= 1.65373 train_acc= 0.24775 val_loss= 8.07980 val_acc= 0.61440 time= 10.54401\n",
      "Epoch: 1765 train_loss= 1.65047 train_acc= 0.22978 val_loss= 8.08005 val_acc= 0.71594 time= 10.47148\n",
      "Epoch: 1766 train_loss= 1.66488 train_acc= 0.26316 val_loss= 8.08008 val_acc= 0.79177 time= 11.10740\n",
      "Epoch: 1767 train_loss= 1.68250 train_acc= 0.25417 val_loss= 8.08029 val_acc= 0.82005 time= 10.73304\n",
      "Epoch: 1768 train_loss= 1.65687 train_acc= 0.25546 val_loss= 8.08068 val_acc= 0.83419 time= 10.76382\n",
      "Epoch: 1769 train_loss= 1.61472 train_acc= 0.25417 val_loss= 8.08109 val_acc= 0.83676 time= 10.75935\n",
      "Epoch: 1770 train_loss= 1.66344 train_acc= 0.24134 val_loss= 8.08153 val_acc= 0.85347 time= 10.74383\n",
      "Epoch: 1771 train_loss= 1.64486 train_acc= 0.24647 val_loss= 8.08197 val_acc= 0.87018 time= 11.03402\n",
      "Epoch: 1772 train_loss= 1.68129 train_acc= 0.23107 val_loss= 8.08218 val_acc= 0.89460 time= 11.23597\n",
      "Epoch: 1773 train_loss= 1.67790 train_acc= 0.24519 val_loss= 8.08276 val_acc= 0.91260 time= 10.76066\n",
      "Epoch: 1774 train_loss= 1.69245 train_acc= 0.24134 val_loss= 8.08318 val_acc= 0.91774 time= 10.84219\n",
      "Epoch: 1775 train_loss= 1.63993 train_acc= 0.24775 val_loss= 8.08331 val_acc= 0.91902 time= 10.51579\n",
      "Epoch: 1776 train_loss= 1.65556 train_acc= 0.24134 val_loss= 8.08309 val_acc= 0.91902 time= 10.61759\n",
      "Epoch: 1777 train_loss= 1.64795 train_acc= 0.23877 val_loss= 8.08274 val_acc= 0.90231 time= 12.49511\n",
      "Epoch: 1778 train_loss= 1.66384 train_acc= 0.24904 val_loss= 8.08166 val_acc= 0.88046 time= 11.07018\n",
      "Epoch: 1779 train_loss= 1.64762 train_acc= 0.24647 val_loss= 8.08011 val_acc= 0.86375 time= 11.25301\n",
      "Epoch: 1780 train_loss= 1.63226 train_acc= 0.25160 val_loss= 8.07948 val_acc= 0.86247 time= 10.71265\n",
      "Epoch: 1781 train_loss= 1.64112 train_acc= 0.25674 val_loss= 8.07887 val_acc= 0.85604 time= 10.50525\n",
      "Epoch: 1782 train_loss= 1.65715 train_acc= 0.25674 val_loss= 8.07868 val_acc= 0.87275 time= 10.79463\n",
      "Epoch: 1783 train_loss= 1.66182 train_acc= 0.25417 val_loss= 8.07872 val_acc= 0.89075 time= 10.72883\n",
      "Epoch: 1784 train_loss= 1.69287 train_acc= 0.25546 val_loss= 8.07927 val_acc= 0.92802 time= 11.58928\n",
      "Epoch: 1785 train_loss= 1.63857 train_acc= 0.23235 val_loss= 8.07989 val_acc= 0.95630 time= 11.26182\n",
      "Epoch: 1786 train_loss= 1.66592 train_acc= 0.24262 val_loss= 8.08081 val_acc= 0.96530 time= 10.61826\n",
      "Epoch: 1787 train_loss= 1.65215 train_acc= 0.24390 val_loss= 8.08142 val_acc= 0.96787 time= 11.42734\n",
      "Epoch: 1788 train_loss= 1.64560 train_acc= 0.24904 val_loss= 8.08175 val_acc= 0.95758 time= 11.77149\n",
      "Epoch: 1789 train_loss= 1.65751 train_acc= 0.25289 val_loss= 8.08171 val_acc= 0.94859 time= 11.28979\n",
      "Epoch: 1790 train_loss= 1.66289 train_acc= 0.25931 val_loss= 8.08126 val_acc= 0.94087 time= 10.55656\n",
      "Epoch: 1791 train_loss= 1.66025 train_acc= 0.24519 val_loss= 8.08118 val_acc= 0.93316 time= 12.87720\n",
      "Epoch: 1792 train_loss= 1.64915 train_acc= 0.24904 val_loss= 8.08123 val_acc= 0.94730 time= 10.76181\n",
      "Epoch: 1793 train_loss= 1.62804 train_acc= 0.25802 val_loss= 8.08078 val_acc= 0.95244 time= 10.80566\n",
      "Epoch: 1794 train_loss= 1.63757 train_acc= 0.26829 val_loss= 8.08077 val_acc= 0.95116 time= 10.24770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1795 train_loss= 1.66230 train_acc= 0.23748 val_loss= 8.08098 val_acc= 0.95758 time= 10.23281\n",
      "Epoch: 1796 train_loss= 1.66905 train_acc= 0.25802 val_loss= 8.08093 val_acc= 0.94987 time= 10.31614\n",
      "Epoch: 1797 train_loss= 1.67050 train_acc= 0.27086 val_loss= 8.08192 val_acc= 0.93188 time= 10.22674\n",
      "Epoch: 1798 train_loss= 1.67545 train_acc= 0.25160 val_loss= 8.08376 val_acc= 0.89460 time= 10.22856\n",
      "Epoch: 1799 train_loss= 1.64781 train_acc= 0.23620 val_loss= 8.08577 val_acc= 0.84704 time= 10.20597\n",
      "Epoch: 1800 train_loss= 1.68771 train_acc= 0.25802 val_loss= 8.08739 val_acc= 0.80720 time= 10.24014\n",
      "Epoch: 1801 train_loss= 1.67247 train_acc= 0.25417 val_loss= 8.08864 val_acc= 0.76864 time= 10.21636\n",
      "Epoch: 1802 train_loss= 1.61493 train_acc= 0.26573 val_loss= 8.08984 val_acc= 0.76350 time= 10.27442\n",
      "Epoch: 1803 train_loss= 1.69501 train_acc= 0.24519 val_loss= 8.09063 val_acc= 0.76350 time= 10.27950\n",
      "Epoch: 1804 train_loss= 1.67585 train_acc= 0.26573 val_loss= 8.09114 val_acc= 0.82648 time= 10.23451\n",
      "Epoch: 1805 train_loss= 1.64578 train_acc= 0.25802 val_loss= 8.09175 val_acc= 0.87918 time= 10.23316\n",
      "Epoch: 1806 train_loss= 1.61231 train_acc= 0.26444 val_loss= 8.09286 val_acc= 0.90360 time= 10.25003\n",
      "Epoch: 1807 train_loss= 1.64645 train_acc= 0.24519 val_loss= 8.09397 val_acc= 0.92031 time= 10.21000\n",
      "Epoch: 1808 train_loss= 1.64577 train_acc= 0.24775 val_loss= 8.09455 val_acc= 0.91517 time= 10.20841\n",
      "Epoch: 1809 train_loss= 1.65711 train_acc= 0.22978 val_loss= 8.09447 val_acc= 0.88689 time= 10.28830\n",
      "Epoch: 1810 train_loss= 1.64691 train_acc= 0.23620 val_loss= 8.09431 val_acc= 0.85604 time= 10.22540\n",
      "Epoch: 1811 train_loss= 1.61307 train_acc= 0.23620 val_loss= 8.09444 val_acc= 0.82648 time= 10.23400\n",
      "Epoch: 1812 train_loss= 1.71020 train_acc= 0.25289 val_loss= 8.09482 val_acc= 0.83805 time= 10.22197\n",
      "Epoch: 1813 train_loss= 1.66268 train_acc= 0.23877 val_loss= 8.09509 val_acc= 0.80463 time= 10.22197\n",
      "Epoch: 1814 train_loss= 1.66824 train_acc= 0.23492 val_loss= 8.09592 val_acc= 0.76221 time= 10.31025\n",
      "Epoch: 1815 train_loss= 1.65565 train_acc= 0.25160 val_loss= 8.09700 val_acc= 0.73907 time= 10.38900\n",
      "Epoch: 1816 train_loss= 1.69916 train_acc= 0.23235 val_loss= 8.09805 val_acc= 0.78663 time= 10.20636\n",
      "Epoch: 1817 train_loss= 1.67238 train_acc= 0.25289 val_loss= 8.09858 val_acc= 0.78278 time= 10.23358\n",
      "Epoch: 1818 train_loss= 1.69921 train_acc= 0.24005 val_loss= 8.09693 val_acc= 0.77892 time= 10.21869\n",
      "Epoch: 1819 train_loss= 1.65427 train_acc= 0.25417 val_loss= 8.09424 val_acc= 0.72365 time= 10.20930\n",
      "Epoch: 1820 train_loss= 1.65510 train_acc= 0.26187 val_loss= 8.09205 val_acc= 0.67609 time= 10.90292\n",
      "Epoch: 1821 train_loss= 1.65070 train_acc= 0.26701 val_loss= 8.09023 val_acc= 0.62468 time= 10.38495\n",
      "Epoch: 1822 train_loss= 1.67803 train_acc= 0.25931 val_loss= 8.08882 val_acc= 0.63625 time= 10.33423\n",
      "Epoch: 1823 train_loss= 1.65692 train_acc= 0.26059 val_loss= 8.08803 val_acc= 0.62211 time= 10.69457\n",
      "Epoch: 1824 train_loss= 1.63788 train_acc= 0.26316 val_loss= 8.08730 val_acc= 0.58740 time= 10.21950\n",
      "Epoch: 1825 train_loss= 1.63432 train_acc= 0.26316 val_loss= 8.08694 val_acc= 0.52057 time= 10.21197\n",
      "Epoch: 1826 train_loss= 1.64623 train_acc= 0.26187 val_loss= 8.08735 val_acc= 0.50771 time= 10.20997\n",
      "Epoch: 1827 train_loss= 1.63769 train_acc= 0.25674 val_loss= 8.08690 val_acc= 0.46144 time= 10.29230\n",
      "Epoch: 1828 train_loss= 1.69147 train_acc= 0.25546 val_loss= 8.08645 val_acc= 0.47301 time= 10.21345\n",
      "Epoch: 1829 train_loss= 1.66500 train_acc= 0.25289 val_loss= 8.08655 val_acc= 0.53085 time= 10.23841\n",
      "Epoch: 1830 train_loss= 1.62424 train_acc= 0.26573 val_loss= 8.08677 val_acc= 0.54884 time= 10.20762\n",
      "Epoch: 1831 train_loss= 1.72974 train_acc= 0.27086 val_loss= 8.08664 val_acc= 0.60154 time= 10.19781\n",
      "Epoch: 1832 train_loss= 1.66165 train_acc= 0.24134 val_loss= 8.08649 val_acc= 0.63368 time= 10.23408\n",
      "Epoch: 1833 train_loss= 1.70494 train_acc= 0.25289 val_loss= 8.08576 val_acc= 0.70566 time= 10.32277\n",
      "Epoch: 1834 train_loss= 1.67638 train_acc= 0.24519 val_loss= 8.08558 val_acc= 0.78149 time= 10.23284\n",
      "Epoch: 1835 train_loss= 1.65766 train_acc= 0.26059 val_loss= 8.08585 val_acc= 0.85090 time= 10.28339\n",
      "Epoch: 1836 train_loss= 1.70017 train_acc= 0.24134 val_loss= 8.08643 val_acc= 0.89460 time= 10.23405\n",
      "Epoch: 1837 train_loss= 1.64828 train_acc= 0.23877 val_loss= 8.08711 val_acc= 0.92159 time= 10.19927\n",
      "Epoch: 1838 train_loss= 1.67355 train_acc= 0.24775 val_loss= 8.08759 val_acc= 0.93445 time= 10.21944\n",
      "Epoch: 1839 train_loss= 1.67909 train_acc= 0.23363 val_loss= 8.08767 val_acc= 0.93702 time= 10.34037\n",
      "Epoch: 1840 train_loss= 1.68887 train_acc= 0.24005 val_loss= 8.08667 val_acc= 0.92802 time= 10.21148\n",
      "Epoch: 1841 train_loss= 1.69332 train_acc= 0.23492 val_loss= 8.08556 val_acc= 0.91645 time= 10.23421\n",
      "Epoch: 1842 train_loss= 1.67854 train_acc= 0.25160 val_loss= 8.08430 val_acc= 0.87018 time= 10.21923\n",
      "Epoch: 1843 train_loss= 1.67850 train_acc= 0.23748 val_loss= 8.08372 val_acc= 0.84576 time= 10.36460\n",
      "Epoch: 1844 train_loss= 1.69097 train_acc= 0.24390 val_loss= 8.08306 val_acc= 0.73136 time= 10.23572\n",
      "Epoch: 1845 train_loss= 1.64578 train_acc= 0.23620 val_loss= 8.08237 val_acc= 0.61568 time= 10.32244\n",
      "Epoch: 1846 train_loss= 1.64546 train_acc= 0.25417 val_loss= 8.08168 val_acc= 0.53728 time= 10.22212\n",
      "Epoch: 1847 train_loss= 1.66888 train_acc= 0.27728 val_loss= 8.08113 val_acc= 0.42416 time= 10.21104\n",
      "Epoch: 1848 train_loss= 1.66182 train_acc= 0.20539 val_loss= 8.08091 val_acc= 0.35604 time= 10.20861\n",
      "Epoch: 1849 train_loss= 1.64930 train_acc= 0.25032 val_loss= 8.08048 val_acc= 0.24293 time= 10.21625\n",
      "Epoch: 1850 train_loss= 1.65057 train_acc= 0.25160 val_loss= 8.07994 val_acc= 0.11954 time= 10.20340\n",
      "Epoch: 1851 train_loss= 1.66550 train_acc= 0.26059 val_loss= 8.07946 val_acc= 0.14524 time= 10.21498\n",
      "Epoch: 1852 train_loss= 1.64924 train_acc= 0.27728 val_loss= 8.07951 val_acc= 0.20051 time= 10.92419\n",
      "Epoch: 1853 train_loss= 1.63311 train_acc= 0.21309 val_loss= 8.07967 val_acc= 0.21337 time= 10.22341\n",
      "Epoch: 1854 train_loss= 1.74254 train_acc= 0.25160 val_loss= 8.07780 val_acc= 0.31362 time= 10.25630\n",
      "Epoch: 1855 train_loss= 1.67276 train_acc= 0.23748 val_loss= 8.07652 val_acc= 0.47172 time= 10.23583\n",
      "Epoch: 1856 train_loss= 1.62734 train_acc= 0.27343 val_loss= 8.07557 val_acc= 0.58740 time= 10.20621\n",
      "Epoch: 1857 train_loss= 1.63115 train_acc= 0.23620 val_loss= 8.07485 val_acc= 0.72751 time= 10.19407\n",
      "Epoch: 1858 train_loss= 1.66470 train_acc= 0.24005 val_loss= 8.07491 val_acc= 0.81234 time= 10.36909\n",
      "Epoch: 1859 train_loss= 1.64954 train_acc= 0.22721 val_loss= 8.07548 val_acc= 0.83419 time= 10.23503\n",
      "Epoch: 1860 train_loss= 1.64746 train_acc= 0.24647 val_loss= 8.07595 val_acc= 0.85861 time= 10.20887\n",
      "Epoch: 1861 train_loss= 1.66663 train_acc= 0.25289 val_loss= 8.07629 val_acc= 0.86889 time= 10.19608\n",
      "Epoch: 1862 train_loss= 1.68987 train_acc= 0.23748 val_loss= 8.07649 val_acc= 0.86118 time= 10.21596\n",
      "Epoch: 1863 train_loss= 1.63747 train_acc= 0.25289 val_loss= 8.07691 val_acc= 0.84190 time= 10.21628\n",
      "Epoch: 1864 train_loss= 1.67485 train_acc= 0.25674 val_loss= 8.07767 val_acc= 0.82391 time= 10.33359\n",
      "Epoch: 1865 train_loss= 1.70243 train_acc= 0.24262 val_loss= 8.07793 val_acc= 0.79434 time= 10.22904\n",
      "Epoch: 1866 train_loss= 1.65112 train_acc= 0.24904 val_loss= 8.07875 val_acc= 0.74807 time= 10.20302\n",
      "Epoch: 1867 train_loss= 1.71100 train_acc= 0.26829 val_loss= 8.07886 val_acc= 0.66967 time= 10.20617\n",
      "Epoch: 1868 train_loss= 1.68859 train_acc= 0.26701 val_loss= 8.07997 val_acc= 0.62468 time= 10.23512\n",
      "Epoch: 1869 train_loss= 1.64800 train_acc= 0.28755 val_loss= 8.08101 val_acc= 0.56941 time= 10.22222\n",
      "Epoch: 1870 train_loss= 1.63536 train_acc= 0.27471 val_loss= 8.08185 val_acc= 0.55141 time= 10.29451\n",
      "Epoch: 1871 train_loss= 1.68769 train_acc= 0.24775 val_loss= 8.08265 val_acc= 0.53985 time= 10.22678\n",
      "Epoch: 1872 train_loss= 1.65578 train_acc= 0.27214 val_loss= 8.08329 val_acc= 0.52699 time= 10.29366\n",
      "Epoch: 1873 train_loss= 1.66065 train_acc= 0.25931 val_loss= 8.08381 val_acc= 0.48586 time= 10.24105\n",
      "Epoch: 1874 train_loss= 1.68081 train_acc= 0.26573 val_loss= 8.08420 val_acc= 0.61697 time= 10.21486\n",
      "Epoch: 1875 train_loss= 1.71943 train_acc= 0.25546 val_loss= 8.08458 val_acc= 0.74550 time= 10.22582\n",
      "Epoch: 1876 train_loss= 1.66182 train_acc= 0.24904 val_loss= 8.08540 val_acc= 0.80720 time= 10.34048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1877 train_loss= 1.64832 train_acc= 0.23748 val_loss= 8.08618 val_acc= 0.85090 time= 10.20983\n",
      "Epoch: 1878 train_loss= 1.67846 train_acc= 0.25546 val_loss= 8.08720 val_acc= 0.88175 time= 10.21126\n",
      "Epoch: 1879 train_loss= 1.68599 train_acc= 0.24005 val_loss= 8.08819 val_acc= 0.89589 time= 10.21837\n",
      "Epoch: 1880 train_loss= 1.63428 train_acc= 0.24904 val_loss= 8.08893 val_acc= 0.91131 time= 10.21575\n",
      "Epoch: 1881 train_loss= 1.68094 train_acc= 0.24134 val_loss= 8.08896 val_acc= 0.92288 time= 10.74168\n",
      "Epoch: 1882 train_loss= 1.65454 train_acc= 0.24904 val_loss= 8.08872 val_acc= 0.94987 time= 10.36053\n",
      "Epoch: 1883 train_loss= 1.70166 train_acc= 0.23748 val_loss= 8.08793 val_acc= 0.95630 time= 10.23291\n",
      "Epoch: 1884 train_loss= 1.67150 train_acc= 0.24262 val_loss= 8.08735 val_acc= 0.95758 time= 10.22510\n",
      "Epoch: 1885 train_loss= 1.63701 train_acc= 0.25032 val_loss= 8.08676 val_acc= 0.94859 time= 10.22137\n",
      "Epoch: 1886 train_loss= 1.65403 train_acc= 0.23235 val_loss= 8.08607 val_acc= 0.92674 time= 10.22182\n",
      "Epoch: 1887 train_loss= 1.63576 train_acc= 0.23748 val_loss= 8.08562 val_acc= 0.88560 time= 10.22931\n",
      "Epoch: 1888 train_loss= 1.69091 train_acc= 0.23235 val_loss= 8.08573 val_acc= 0.83162 time= 10.39111\n",
      "Epoch: 1889 train_loss= 1.64325 train_acc= 0.24390 val_loss= 8.08598 val_acc= 0.72494 time= 10.22245\n",
      "Epoch: 1890 train_loss= 1.64037 train_acc= 0.25931 val_loss= 8.08693 val_acc= 0.59640 time= 10.21565\n",
      "Epoch: 1891 train_loss= 1.66547 train_acc= 0.22465 val_loss= 8.08741 val_acc= 0.48843 time= 10.21429\n",
      "Epoch: 1892 train_loss= 1.66054 train_acc= 0.25931 val_loss= 8.08781 val_acc= 0.47686 time= 10.19567\n",
      "Epoch: 1893 train_loss= 1.65284 train_acc= 0.24519 val_loss= 8.08835 val_acc= 0.46915 time= 10.23662\n",
      "Epoch: 1894 train_loss= 1.63508 train_acc= 0.24647 val_loss= 8.08907 val_acc= 0.49614 time= 10.29870\n",
      "Epoch: 1895 train_loss= 1.65762 train_acc= 0.24519 val_loss= 8.08983 val_acc= 0.53342 time= 10.28303\n",
      "Epoch: 1896 train_loss= 1.66413 train_acc= 0.26444 val_loss= 8.09024 val_acc= 0.60925 time= 10.22687\n",
      "Epoch: 1897 train_loss= 1.70210 train_acc= 0.22336 val_loss= 8.09039 val_acc= 0.75578 time= 10.22674\n",
      "Epoch: 1898 train_loss= 1.64776 train_acc= 0.24134 val_loss= 8.09073 val_acc= 0.83290 time= 10.20737\n",
      "Epoch: 1899 train_loss= 1.64255 train_acc= 0.24262 val_loss= 8.09053 val_acc= 0.87532 time= 10.28054\n",
      "Epoch: 1900 train_loss= 1.66690 train_acc= 0.25417 val_loss= 8.08917 val_acc= 0.91131 time= 10.21260\n",
      "Epoch: 1901 train_loss= 1.66093 train_acc= 0.25160 val_loss= 8.08779 val_acc= 0.95244 time= 10.52303\n",
      "Epoch: 1902 train_loss= 1.64696 train_acc= 0.23877 val_loss= 8.08628 val_acc= 0.95630 time= 10.27079\n",
      "Epoch: 1903 train_loss= 1.69297 train_acc= 0.24647 val_loss= 8.08523 val_acc= 0.95630 time= 10.22833\n",
      "Epoch: 1904 train_loss= 1.65137 train_acc= 0.23235 val_loss= 8.08347 val_acc= 0.91388 time= 10.21883\n",
      "Epoch: 1905 train_loss= 1.68637 train_acc= 0.23620 val_loss= 8.08214 val_acc= 0.88560 time= 10.25037\n",
      "Epoch: 1906 train_loss= 1.66920 train_acc= 0.24775 val_loss= 8.08043 val_acc= 0.82905 time= 10.20034\n",
      "Epoch: 1907 train_loss= 1.68426 train_acc= 0.25674 val_loss= 8.07930 val_acc= 0.76992 time= 10.29907\n",
      "Epoch: 1908 train_loss= 1.65057 train_acc= 0.25160 val_loss= 8.07868 val_acc= 0.73136 time= 10.23052\n",
      "Epoch: 1909 train_loss= 1.66243 train_acc= 0.27471 val_loss= 8.07789 val_acc= 0.68509 time= 10.24376\n",
      "Epoch: 1910 train_loss= 1.67395 train_acc= 0.22850 val_loss= 8.07706 val_acc= 0.61568 time= 10.20612\n",
      "Epoch: 1911 train_loss= 1.67808 train_acc= 0.25417 val_loss= 8.07687 val_acc= 0.60154 time= 10.82158\n",
      "Epoch: 1912 train_loss= 1.66719 train_acc= 0.28113 val_loss= 8.07645 val_acc= 0.55913 time= 10.22358\n",
      "Epoch: 1913 train_loss= 1.64164 train_acc= 0.27343 val_loss= 8.07630 val_acc= 0.57841 time= 10.33099\n",
      "Epoch: 1914 train_loss= 1.65605 train_acc= 0.27214 val_loss= 8.07649 val_acc= 0.61054 time= 10.21210\n",
      "Epoch: 1915 train_loss= 1.66512 train_acc= 0.22593 val_loss= 8.07722 val_acc= 0.66452 time= 10.22597\n",
      "Epoch: 1916 train_loss= 1.69588 train_acc= 0.26316 val_loss= 8.07820 val_acc= 0.72494 time= 10.21022\n",
      "Epoch: 1917 train_loss= 1.62689 train_acc= 0.26701 val_loss= 8.07954 val_acc= 0.76478 time= 10.24654\n",
      "Epoch: 1918 train_loss= 1.65488 train_acc= 0.26444 val_loss= 8.08052 val_acc= 0.78535 time= 10.20495\n",
      "Epoch: 1919 train_loss= 1.62709 train_acc= 0.25289 val_loss= 8.08153 val_acc= 0.80591 time= 10.29201\n",
      "Epoch: 1920 train_loss= 1.67054 train_acc= 0.25160 val_loss= 8.08219 val_acc= 0.83548 time= 10.21371\n",
      "Epoch: 1921 train_loss= 1.64051 train_acc= 0.24519 val_loss= 8.08239 val_acc= 0.83933 time= 10.20596\n",
      "Epoch: 1922 train_loss= 1.66897 train_acc= 0.25674 val_loss= 8.08272 val_acc= 0.84319 time= 10.22659\n",
      "Epoch: 1923 train_loss= 1.65449 train_acc= 0.25931 val_loss= 8.08258 val_acc= 0.84190 time= 10.22529\n",
      "Epoch: 1924 train_loss= 1.69181 train_acc= 0.24775 val_loss= 8.08203 val_acc= 0.81620 time= 10.22035\n",
      "Epoch: 1925 train_loss= 1.62240 train_acc= 0.25802 val_loss= 8.08159 val_acc= 0.81234 time= 10.32764\n",
      "Epoch: 1926 train_loss= 1.65252 train_acc= 0.25417 val_loss= 8.08110 val_acc= 0.81877 time= 10.22490\n",
      "Epoch: 1927 train_loss= 1.69142 train_acc= 0.26316 val_loss= 8.08033 val_acc= 0.79949 time= 10.21597\n",
      "Epoch: 1928 train_loss= 1.66208 train_acc= 0.25289 val_loss= 8.07907 val_acc= 0.75450 time= 10.23876\n",
      "Epoch: 1929 train_loss= 1.65822 train_acc= 0.24005 val_loss= 8.07806 val_acc= 0.68766 time= 10.22920\n",
      "Epoch: 1930 train_loss= 1.64105 train_acc= 0.26829 val_loss= 8.07730 val_acc= 0.63368 time= 10.22350\n",
      "Epoch: 1931 train_loss= 1.66495 train_acc= 0.26573 val_loss= 8.07652 val_acc= 0.57069 time= 10.40089\n",
      "Epoch: 1932 train_loss= 1.65857 train_acc= 0.26316 val_loss= 8.07634 val_acc= 0.52828 time= 10.23913\n",
      "Epoch: 1933 train_loss= 1.66292 train_acc= 0.25802 val_loss= 8.07631 val_acc= 0.51799 time= 10.21982\n",
      "Epoch: 1934 train_loss= 1.63441 train_acc= 0.26187 val_loss= 8.07700 val_acc= 0.50386 time= 10.23918\n",
      "Epoch: 1935 train_loss= 1.62721 train_acc= 0.24775 val_loss= 8.07802 val_acc= 0.51928 time= 10.22378\n",
      "Epoch: 1936 train_loss= 1.62506 train_acc= 0.26059 val_loss= 8.07946 val_acc= 0.55656 time= 10.20176\n",
      "Epoch: 1937 train_loss= 1.64037 train_acc= 0.23748 val_loss= 8.08089 val_acc= 0.61825 time= 10.28937\n",
      "Epoch: 1938 train_loss= 1.65338 train_acc= 0.24262 val_loss= 8.08248 val_acc= 0.70823 time= 10.24730\n",
      "Epoch: 1939 train_loss= 1.66195 train_acc= 0.23877 val_loss= 8.08376 val_acc= 0.77378 time= 10.22644\n",
      "Epoch: 1940 train_loss= 1.62295 train_acc= 0.26444 val_loss= 8.08517 val_acc= 0.82262 time= 10.84914\n",
      "Epoch: 1941 train_loss= 1.64555 train_acc= 0.24262 val_loss= 8.08674 val_acc= 0.85861 time= 10.20591\n",
      "Epoch: 1942 train_loss= 1.66043 train_acc= 0.23877 val_loss= 8.08883 val_acc= 0.86375 time= 10.21962\n",
      "Epoch: 1943 train_loss= 1.67782 train_acc= 0.24904 val_loss= 8.08960 val_acc= 0.85219 time= 10.21080\n",
      "Epoch: 1944 train_loss= 1.69170 train_acc= 0.25417 val_loss= 8.08973 val_acc= 0.84190 time= 10.26907\n",
      "Epoch: 1945 train_loss= 1.67897 train_acc= 0.24647 val_loss= 8.08989 val_acc= 0.84704 time= 10.20884\n",
      "Epoch: 1946 train_loss= 1.64119 train_acc= 0.24005 val_loss= 8.09026 val_acc= 0.84319 time= 10.23433\n",
      "Epoch: 1947 train_loss= 1.64247 train_acc= 0.24904 val_loss= 8.08977 val_acc= 0.81620 time= 10.20985\n",
      "Epoch: 1948 train_loss= 1.65363 train_acc= 0.24904 val_loss= 8.08913 val_acc= 0.81877 time= 10.19572\n",
      "Epoch: 1949 train_loss= 1.72281 train_acc= 0.25289 val_loss= 8.08874 val_acc= 0.83933 time= 10.21521\n",
      "Epoch: 1950 train_loss= 1.68859 train_acc= 0.24262 val_loss= 8.08805 val_acc= 0.84190 time= 10.36862\n",
      "Epoch: 1951 train_loss= 1.66912 train_acc= 0.24904 val_loss= 8.08766 val_acc= 0.85604 time= 10.22885\n",
      "Epoch: 1952 train_loss= 1.65469 train_acc= 0.25417 val_loss= 8.08751 val_acc= 0.85733 time= 10.23488\n",
      "Epoch: 1953 train_loss= 1.67551 train_acc= 0.25032 val_loss= 8.08710 val_acc= 0.84190 time= 10.21235\n",
      "Epoch: 1954 train_loss= 1.67217 train_acc= 0.25417 val_loss= 8.08704 val_acc= 0.83548 time= 10.21702\n",
      "Epoch: 1955 train_loss= 1.64202 train_acc= 0.25931 val_loss= 8.08717 val_acc= 0.82134 time= 10.22172\n",
      "Epoch: 1956 train_loss= 1.65134 train_acc= 0.25546 val_loss= 8.08746 val_acc= 0.82262 time= 10.32089\n",
      "Epoch: 1957 train_loss= 1.64691 train_acc= 0.26187 val_loss= 8.08771 val_acc= 0.82134 time= 10.21149\n",
      "Epoch: 1958 train_loss= 1.65454 train_acc= 0.25289 val_loss= 8.08747 val_acc= 0.78920 time= 10.26204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1959 train_loss= 1.66035 train_acc= 0.24775 val_loss= 8.08663 val_acc= 0.76735 time= 10.23044\n",
      "Epoch: 1960 train_loss= 1.63919 train_acc= 0.24519 val_loss= 8.08555 val_acc= 0.68509 time= 10.34826\n",
      "Epoch: 1961 train_loss= 1.66013 train_acc= 0.25674 val_loss= 8.08439 val_acc= 0.60411 time= 10.25741\n",
      "Epoch: 1962 train_loss= 1.64671 train_acc= 0.23620 val_loss= 8.08336 val_acc= 0.47044 time= 10.27655\n",
      "Epoch: 1963 train_loss= 1.63487 train_acc= 0.26316 val_loss= 8.08280 val_acc= 0.44602 time= 10.23689\n",
      "Epoch: 1964 train_loss= 1.64557 train_acc= 0.27985 val_loss= 8.08349 val_acc= 0.52956 time= 10.21841\n",
      "Epoch: 1965 train_loss= 1.65360 train_acc= 0.24775 val_loss= 8.08400 val_acc= 0.60797 time= 10.20750\n",
      "Epoch: 1966 train_loss= 1.66511 train_acc= 0.24775 val_loss= 8.08447 val_acc= 0.73136 time= 10.22461\n",
      "Epoch: 1967 train_loss= 1.64808 train_acc= 0.26316 val_loss= 8.08462 val_acc= 0.75450 time= 10.23000\n",
      "Epoch: 1968 train_loss= 1.68261 train_acc= 0.26958 val_loss= 8.08464 val_acc= 0.78406 time= 10.32645\n",
      "Epoch: 1969 train_loss= 1.66891 train_acc= 0.25931 val_loss= 8.08463 val_acc= 0.81105 time= 10.87689\n",
      "Epoch: 1970 train_loss= 1.63679 train_acc= 0.25674 val_loss= 8.08482 val_acc= 0.82519 time= 10.24396\n",
      "Epoch: 1971 train_loss= 1.63308 train_acc= 0.27214 val_loss= 8.08529 val_acc= 0.82776 time= 10.20751\n",
      "Epoch: 1972 train_loss= 1.63523 train_acc= 0.26444 val_loss= 8.08580 val_acc= 0.81491 time= 10.21476\n",
      "Epoch: 1973 train_loss= 1.64833 train_acc= 0.26059 val_loss= 8.08636 val_acc= 0.81491 time= 10.20713\n",
      "Epoch: 1974 train_loss= 1.66665 train_acc= 0.27343 val_loss= 8.08633 val_acc= 0.78535 time= 10.27991\n",
      "Epoch: 1975 train_loss= 1.67605 train_acc= 0.25032 val_loss= 8.08566 val_acc= 0.73779 time= 10.25001\n",
      "Epoch: 1976 train_loss= 1.64771 train_acc= 0.24904 val_loss= 8.08474 val_acc= 0.71722 time= 10.20665\n",
      "Epoch: 1977 train_loss= 1.65423 train_acc= 0.27343 val_loss= 8.08323 val_acc= 0.73265 time= 10.20842\n",
      "Epoch: 1978 train_loss= 1.65086 train_acc= 0.24904 val_loss= 8.08193 val_acc= 0.78406 time= 10.20762\n",
      "Epoch: 1979 train_loss= 1.64203 train_acc= 0.25032 val_loss= 8.08147 val_acc= 0.78920 time= 10.21295\n",
      "Epoch: 1980 train_loss= 1.66663 train_acc= 0.24775 val_loss= 8.08126 val_acc= 0.80334 time= 10.29181\n",
      "Epoch: 1981 train_loss= 1.65116 train_acc= 0.24134 val_loss= 8.08092 val_acc= 0.79949 time= 10.27581\n",
      "Epoch: 1982 train_loss= 1.62783 train_acc= 0.25674 val_loss= 8.08068 val_acc= 0.75964 time= 10.21425\n",
      "Epoch: 1983 train_loss= 1.66704 train_acc= 0.25160 val_loss= 8.07975 val_acc= 0.73779 time= 10.20459\n",
      "Epoch: 1984 train_loss= 1.67445 train_acc= 0.25032 val_loss= 8.07752 val_acc= 0.77763 time= 10.21332\n",
      "Epoch: 1985 train_loss= 1.65434 train_acc= 0.24005 val_loss= 8.07543 val_acc= 0.76992 time= 10.20746\n",
      "Epoch: 1986 train_loss= 1.68966 train_acc= 0.25417 val_loss= 8.07443 val_acc= 0.77635 time= 10.23622\n",
      "Epoch: 1987 train_loss= 1.62091 train_acc= 0.26573 val_loss= 8.07412 val_acc= 0.77635 time= 10.39493\n",
      "Epoch: 1988 train_loss= 1.61838 train_acc= 0.25674 val_loss= 8.07441 val_acc= 0.76864 time= 10.23581\n",
      "Epoch: 1989 train_loss= 1.64824 train_acc= 0.25674 val_loss= 8.07556 val_acc= 0.79820 time= 11.29862\n",
      "Epoch: 1990 train_loss= 1.67012 train_acc= 0.24647 val_loss= 8.07702 val_acc= 0.83419 time= 10.36665\n",
      "Epoch: 1991 train_loss= 1.62962 train_acc= 0.25417 val_loss= 8.07789 val_acc= 0.85733 time= 10.23081\n",
      "Epoch: 1992 train_loss= 1.67700 train_acc= 0.22850 val_loss= 8.07846 val_acc= 0.88560 time= 10.25068\n",
      "Epoch: 1993 train_loss= 1.67420 train_acc= 0.24390 val_loss= 8.07849 val_acc= 0.94087 time= 10.29097\n",
      "Epoch: 1994 train_loss= 1.64676 train_acc= 0.23748 val_loss= 8.07798 val_acc= 0.95887 time= 10.19876\n",
      "Epoch: 1995 train_loss= 1.65773 train_acc= 0.23492 val_loss= 8.07717 val_acc= 0.94859 time= 10.21020\n",
      "Epoch: 1996 train_loss= 1.65673 train_acc= 0.24134 val_loss= 8.07691 val_acc= 0.94473 time= 10.22677\n",
      "Epoch: 1997 train_loss= 1.65974 train_acc= 0.24134 val_loss= 8.07689 val_acc= 0.94473 time= 10.23202\n",
      "Epoch: 1998 train_loss= 1.64100 train_acc= 0.23620 val_loss= 8.07709 val_acc= 0.94602 time= 10.22677\n",
      "Epoch: 1999 train_loss= 1.71872 train_acc= 0.23492 val_loss= 8.07714 val_acc= 0.93188 time= 10.88048\n",
      "Epoch: 2000 train_loss= 1.62722 train_acc= 0.24134 val_loss= 8.07701 val_acc= 0.91003 time= 10.21175\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "cost_val = []\n",
    "\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, train_labels_mat, train_mask_mat[:,1], placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, val_labels_mat, val_mask_mat[:,1], placeholders, sess, model)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 1.61192 accuracy= 0.27121 time= 6.48976\n",
      "[39.417572, 37.68326, 36.662716, 35.950226, 35.040947, 34.0105, 32.79931, 31.685692, 30.612103, 29.522766, 28.421473, 27.267614, 26.04314, 24.885725, 23.816912, 22.911482, 23.14679, 23.502064, 23.898617, 24.375204, 24.817476, 25.239853, 25.526617, 25.858856, 26.139387, 26.424393, 26.627184, 26.812918, 27.091627, 27.36971, 27.593872, 27.979736, 28.283022, 28.618864, 29.029453, 29.448526, 29.876125, 30.279856, 30.6507, 30.958248, 31.245161, 31.501953, 31.7274, 31.950003, 32.24564, 32.449863, 32.61364, 32.748634, 32.787884, 32.878876, 32.916332, 32.883392, 32.85091, 32.859634, 32.95164, 32.96809, 32.98352, 33.06282, 33.126617, 33.15312, 33.240543, 33.33949, 33.372524, 33.442852, 33.45361, 33.35721, 33.25648, 33.16203, 33.048542, 32.84707, 32.676517, 32.422215, 32.205143, 31.94729, 31.657776, 31.421373, 31.261932, 31.153067, 31.088106, 31.04863, 30.961594, 30.857067, 30.753826, 30.67305, 30.616137, 30.582897, 30.563549, 30.482517, 30.354006, 30.272455, 30.164352, 30.032501, 29.89946, 29.74567, 29.55219, 29.323086, 29.14723, 28.974674, 28.888145, 28.81657, 28.853277, 28.865486, 28.85413, 28.762285, 28.7233, 28.642227, 28.56274, 28.497364, 28.418404, 28.34021, 28.178783, 27.993849, 27.78705, 27.524702, 27.26147, 27.058855, 26.868038, 26.699339, 26.568241, 26.477306, 26.41264, 26.293064, 26.155785, 25.890955, 25.6779, 25.465466, 25.31334, 25.163975, 25.022125, 24.897432, 24.803703, 24.679852, 24.544548, 24.395327, 24.240305, 24.067081, 23.978884, 23.862715, 23.759495, 23.707415, 23.665108, 23.621708, 23.587519, 23.583492, 23.56407, 23.505878, 23.42124, 23.36078, 23.273909, 23.113668, 22.951925, 22.794563, 22.643993, 22.5088, 22.375704, 22.26644, 22.190813, 22.120852, 22.056742, 21.986856, 21.90744, 21.803566, 21.734072, 21.67608, 21.63949, 21.592718, 21.55916, 21.515684, 21.519028, 21.510098, 21.521935, 21.51331, 21.534693, 21.506523, 21.454819, 21.405096, 21.370785, 21.362076, 21.332178, 21.311293, 21.305586, 21.26537, 21.238676, 21.248037, 21.239267, 21.231853, 21.215292, 21.19933, 21.211935, 21.200384, 21.185099, 21.165016, 21.126343, 21.067207, 21.000822, 20.943232, 20.885519, 20.8186, 20.753338, 20.694561, 20.620014, 20.531158, 20.40803, 20.312042, 20.215849, 20.131462, 20.063358, 20.012835, 19.969799, 19.900259, 19.831833, 19.773691, 19.704556, 19.631779, 19.541033, 19.444553, 19.358116, 19.25509, 19.171297, 19.095364, 19.00856, 18.931702, 18.84569, 18.740017, 18.635176, 18.5223, 18.421595, 18.338074, 18.254108, 18.164791, 18.10431, 18.026503, 17.953934, 17.883913, 17.815983, 17.761318, 17.709982, 17.663399, 17.604343, 17.550674, 17.507397, 17.482822, 17.439465, 17.412497, 17.381975, 17.375906, 17.373625, 17.354536, 17.330658, 17.302326, 17.28163, 17.271372, 17.26196, 17.250711, 17.22401, 17.202322, 17.141165, 17.053547, 16.956696, 16.855724, 16.75721, 16.657145, 16.56132, 16.470896, 16.368002, 16.259024, 16.1519, 16.056025, 15.952693, 15.8801985, 15.811672, 15.75001, 15.692529, 15.624673, 15.560894, 15.487743, 15.428719, 15.384416, 15.35308, 15.316533, 15.286866, 15.270329, 15.234061, 15.19657, 15.166243, 15.126927, 15.078023, 15.02507, 14.962917, 14.904273, 14.847864, 14.799075, 14.7539625, 14.690543, 14.622457, 14.563796, 14.506548, 14.445518, 14.388055, 14.317328, 14.247244, 14.178952, 14.124626, 14.077763, 14.039367, 13.996637, 13.963616, 13.933286, 13.908656, 13.881895, 13.855445, 13.834527, 13.801941, 13.771716, 13.739967, 13.717426, 13.6852045, 13.65164, 13.614122, 13.570125, 13.523297, 13.490516, 13.4616, 13.430237, 13.392673, 13.370539, 13.345541, 13.32617, 13.306785, 13.27764, 13.251717, 13.226129, 13.193237, 13.163488, 13.119397, 13.078869, 13.043688, 13.020719, 12.9961815, 12.978743, 12.964351, 12.952267, 12.931184, 12.906711, 12.883161, 12.859063, 12.832977, 12.803479, 12.776082, 12.754237, 12.736577, 12.718399, 12.695682, 12.669103, 12.64119, 12.608586, 12.574644, 12.534717, 12.497195, 12.467277, 12.442218, 12.420238, 12.402106, 12.388087, 12.365966, 12.349249, 12.334696, 12.317066, 12.3000345, 12.275267, 12.2475815, 12.214209, 12.183229, 12.153045, 12.124403, 12.094091, 12.06138, 12.0273695, 11.98948, 11.959962, 11.928063, 11.897041, 11.869489, 11.841488, 11.807763, 11.770553, 11.734192, 11.7065115, 11.67863, 11.64963, 11.624235, 11.604489, 11.581317, 11.56491, 11.548538, 11.53948, 11.533501, 11.525613, 11.517448, 11.496769, 11.472912, 11.4437065, 11.413768, 11.382723, 11.359164, 11.327766, 11.298508, 11.265941, 11.233879, 11.202539, 11.175685, 11.145147, 11.115497, 11.093744, 11.070187, 11.051084, 11.031593, 11.018122, 11.006546, 10.990371, 10.973913, 10.956585, 10.93844, 10.923889, 10.91173, 10.902153, 10.887259, 10.869426, 10.856134, 10.844225, 10.8330765, 10.820047, 10.806406, 10.7948265, 10.773204, 10.753875, 10.735107, 10.725756, 10.717392, 10.71317, 10.703347, 10.690967, 10.67989, 10.671123, 10.66769, 10.662747, 10.65944, 10.655503, 10.651823, 10.651487, 10.648339, 10.645405, 10.643148, 10.641318, 10.639991, 10.626141, 10.615315, 10.608099, 10.606534, 10.606108, 10.604544, 10.600578, 10.607425, 10.606902, 10.600491, 10.594047, 10.5897875, 10.583924, 10.585876, 10.584563, 10.579967, 10.573077, 10.566003, 10.559826, 10.549701, 10.531174, 10.510753, 10.481794, 10.453655, 10.425776, 10.398263, 10.372279, 10.347111, 10.323376, 10.305482, 10.289282, 10.273646, 10.254343, 10.237934, 10.216995, 10.19701, 10.169246, 10.148415, 10.135423, 10.127081, 10.118999, 10.112338, 10.104279, 10.098844, 10.094424, 10.089631, 10.08426, 10.074946, 10.063832, 10.050875, 10.03538, 10.019922, 10.005308, 9.992988, 9.983729, 9.975327, 9.963803, 9.955204, 9.945969, 9.942334, 9.936383, 9.932801, 9.92277, 9.918189, 9.913518, 9.908363, 9.901791, 9.89376, 9.88604, 9.879409, 9.870816, 9.862915, 9.861389, 9.858449, 9.852222, 9.851957, 9.849788, 9.844158, 9.837003, 9.829207, 9.82241, 9.813804, 9.8007345, 9.792123, 9.785175, 9.775914, 9.771568, 9.765019, 9.7589245, 9.755472, 9.752553, 9.7501135, 9.749554, 9.753411, 9.757653, 9.760031, 9.760338, 9.754305, 9.752246, 9.749989, 9.742112, 9.736432, 9.73035, 9.718417, 9.705554, 9.695126, 9.685869, 9.672768, 9.662163, 9.645429, 9.629801, 9.616005, 9.598748, 9.586354, 9.573168, 9.559452, 9.546951, 9.535105, 9.523998, 9.5084305, 9.494049, 9.480888, 9.468289, 9.457125, 9.44763, 9.435136, 9.422319, 9.413264, 9.405222, 9.399969, 9.397623, 9.401088, 9.406423, 9.412339, 9.413399, 9.410703, 9.408969, 9.403836, 9.401006, 9.396119, 9.392272, 9.38678, 9.379749, 9.377226, 9.37749, 9.379124, 9.383648, 9.387163, 9.389396, 9.392253, 9.390905, 9.389496, 9.389017, 9.387938, 9.386796, 9.385622, 9.385392, 9.379884, 9.376493, 9.373369, 9.367088, 9.36394, 9.352434, 9.340164, 9.328136, 9.318291, 9.310491, 9.303294, 9.296503, 9.291675, 9.288433, 9.284099, 9.282135, 9.279377, 9.277779, 9.279425, 9.279595, 9.277583, 9.276268, 9.273137, 9.268265, 9.264188, 9.266605, 9.268582, 9.267928, 9.265951, 9.262717, 9.259636, 9.254372, 9.246677, 9.236786, 9.224412, 9.213505, 9.20301, 9.192123, 9.183174, 9.174688, 9.168476, 9.162437, 9.154586, 9.149225, 9.140864, 9.135862, 9.131997, 9.123889, 9.117205, 9.115212, 9.10722, 9.095748, 9.085688, 9.073956, 9.062079, 9.053425, 9.046844, 9.041009, 9.034462, 9.030446, 9.026341, 9.023576, 9.018221, 9.016023, 9.013906, 9.014775, 9.019579, 9.024237, 9.027786, 9.028969, 9.033035, 9.033731, 9.033312, 9.028801, 9.026957, 9.023696, 9.019493, 9.017926, 9.015406, 9.013414, 9.013823, 9.013957, 9.010734, 9.00348, 8.997937, 8.994128, 8.98756, 8.97692, 8.968455, 8.960881, 8.952148, 8.941501, 8.93038, 8.920678, 8.911553, 8.904155, 8.8975315, 8.892023, 8.884639, 8.875805, 8.867071, 8.859283, 8.85151, 8.844256, 8.835381, 8.823847, 8.814601, 8.808018, 8.801583, 8.796679, 8.793561, 8.789949, 8.787131, 8.785293, 8.783597, 8.781982, 8.780759, 8.7771225, 8.773097, 8.770436, 8.7665, 8.762725, 8.76144, 8.763309, 8.766448, 8.766163, 8.765614, 8.762589, 8.757454, 8.75302, 8.746611, 8.737632, 8.729738, 8.721491, 8.716739, 8.711842, 8.707939, 8.69866, 8.685192, 8.678994, 8.673013, 8.666459, 8.659919, 8.653391, 8.649045, 8.645482, 8.642243, 8.637278, 8.632061, 8.626988, 8.622173, 8.618075, 8.60913, 8.599832, 8.591189, 8.582604, 8.572112, 8.559403, 8.549565, 8.545402, 8.544419, 8.545458, 8.545467, 8.546754, 8.548514, 8.551336, 8.556208, 8.560292, 8.563746, 8.569747, 8.578432, 8.586871, 8.5897665, 8.591504, 8.588024, 8.582108, 8.5701475, 8.559227, 8.550489, 8.5455475, 8.536397, 8.530713, 8.528401, 8.524762, 8.520801, 8.515114, 8.510545, 8.508399, 8.503222, 8.498897, 8.497057, 8.494631, 8.495882, 8.4970045, 8.496076, 8.492089, 8.486644, 8.476056, 8.462605, 8.450269, 8.442498, 8.436517, 8.435278, 8.43604, 8.438077, 8.441489, 8.445411, 8.444248, 8.442916, 8.441878, 8.43634, 8.431956, 8.425227, 8.419292, 8.414645, 8.411016, 8.408254, 8.406579, 8.405602, 8.403516, 8.400999, 8.398015, 8.395243, 8.3932295, 8.391886, 8.390558, 8.388379, 8.3857765, 8.383368, 8.382302, 8.381433, 8.380593, 8.378572, 8.37653, 8.375473, 8.3746805, 8.374515, 8.3729105, 8.371697, 8.369482, 8.368672, 8.36877, 8.36837, 8.368662, 8.368086, 8.366246, 8.363093, 8.35827, 8.354196, 8.349623, 8.345177, 8.34272, 8.340625, 8.340368, 8.33882, 8.339338, 8.341214, 8.344954, 8.347152, 8.350387, 8.352211, 8.354692, 8.357281, 8.355765, 8.352225, 8.347723, 8.342279, 8.333776, 8.325427, 8.317336, 8.309435, 8.302111, 8.296363, 8.292352, 8.28944, 8.289129, 8.290398, 8.291015, 8.290469, 8.288586, 8.2856045, 8.28188, 8.27924, 8.275809, 8.272625, 8.268798, 8.266597, 8.263477, 8.260681, 8.257609, 8.254928, 8.253371, 8.252028, 8.251193, 8.249424, 8.248318, 8.247126, 8.246769, 8.246191, 8.245629, 8.245347, 8.244997, 8.242567, 8.241389, 8.242216, 8.243603, 8.244336, 8.245411, 8.246497, 8.246698, 8.244116, 8.238405, 8.231777, 8.228141, 8.224433, 8.21965, 8.215755, 8.212301, 8.208702, 8.205035, 8.203298, 8.2035265, 8.204989, 8.207787, 8.211115, 8.214319, 8.216674, 8.215735, 8.213718, 8.210911, 8.2084255, 8.204999, 8.202178, 8.199911, 8.19868, 8.19816, 8.197224, 8.196938, 8.196862, 8.195776, 8.194606, 8.191714, 8.188211, 8.184307, 8.180427, 8.178019, 8.176818, 8.175094, 8.17386, 8.173624, 8.172694, 8.172038, 8.171554, 8.169192, 8.167599, 8.166851, 8.166151, 8.165114, 8.165121, 8.1648245, 8.16179, 8.158202, 8.155543, 8.154458, 8.15392, 8.15547, 8.157049, 8.15914, 8.1613, 8.163639, 8.165282, 8.166315, 8.164071, 8.16421, 8.163803, 8.163444, 8.164291, 8.164667, 8.164941, 8.1668005, 8.170385, 8.172814, 8.176091, 8.180942, 8.185884, 8.190415, 8.195367, 8.198434, 8.200571, 8.200756, 8.19694, 8.1929455, 8.18969, 8.186197, 8.18368, 8.182579, 8.179989, 8.177787, 8.175865, 8.1734085, 8.172414, 8.17343, 8.175325, 8.176896, 8.179173, 8.180329, 8.179165, 8.17843, 8.1772175, 8.17552, 8.174181, 8.171906, 8.169067, 8.163988, 8.159159, 8.154727, 8.149862, 8.147194, 8.144375, 8.143056, 8.141914, 8.140678, 8.140048, 8.140191, 8.140693, 8.141324, 8.142359, 8.144749, 8.148514, 8.153386, 8.156838, 8.159688, 8.1620245, 8.16732, 8.173058, 8.176461, 8.176982, 8.176031, 8.173147, 8.169219, 8.1644745, 8.159931, 8.1546545, 8.149975, 8.146316, 8.144097, 8.141641, 8.138195, 8.134942, 8.132255, 8.129739, 8.1284275, 8.128278, 8.130432, 8.131593, 8.131541, 8.131924, 8.132371, 8.13381, 8.136304, 8.137136, 8.138031, 8.138002, 8.13823, 8.13883, 8.136975, 8.135606, 8.134271, 8.132882, 8.13218, 8.131554, 8.132406, 8.133784, 8.136709, 8.138015, 8.137172, 8.136679, 8.135982, 8.135526, 8.13446, 8.131296, 8.128135, 8.123843, 8.12012, 8.117689, 8.116069, 8.11388, 8.11256, 8.111661, 8.111751, 8.113064, 8.113676, 8.113906, 8.113895, 8.112898, 8.111573, 8.109815, 8.108242, 8.10706, 8.104942, 8.103184, 8.102582, 8.103105, 8.104378, 8.106204, 8.108062, 8.10914, 8.109921, 8.109442, 8.107673, 8.10757, 8.108667, 8.108963, 8.109178, 8.109544, 8.108798, 8.107366, 8.1053, 8.102933, 8.100046, 8.097937, 8.096509, 8.09514, 8.094257, 8.093813, 8.094278, 8.094077, 8.094233, 8.094849, 8.095644, 8.096058, 8.09674, 8.097412, 8.099727, 8.100868, 8.10179, 8.10172, 8.102478, 8.104099, 8.105521, 8.106447, 8.106531, 8.10682, 8.106289, 8.105956, 8.105233, 8.105059, 8.103998, 8.103884, 8.106927, 8.109243, 8.11134, 8.11322, 8.115881, 8.120024, 8.121724, 8.121377, 8.12103, 8.121505, 8.122562, 8.121428, 8.117954, 8.11429, 8.110654, 8.106053, 8.102999, 8.100861, 8.100611, 8.10005, 8.098815, 8.097293, 8.096754, 8.09571, 8.095474, 8.0962925, 8.09741, 8.098566, 8.098055, 8.097754, 8.097473, 8.097394, 8.0966215, 8.095594, 8.094684, 8.09359, 8.092852, 8.092174, 8.091414, 8.091409, 8.090881, 8.090594, 8.090377, 8.090576, 8.091345, 8.093039, 8.0942955, 8.095577, 8.096215, 8.097074, 8.097492, 8.097713, 8.098205, 8.09915, 8.099531, 8.100538, 8.101341, 8.102146, 8.102528, 8.103614, 8.10392, 8.103768, 8.102947, 8.10149, 8.099537, 8.098376, 8.097384, 8.097678, 8.09902, 8.098795, 8.098247, 8.098206, 8.097926, 8.097369, 8.09681, 8.096228, 8.09541, 8.095503, 8.095579, 8.096002, 8.095822, 8.095588, 8.095642, 8.096689, 8.097558, 8.098825, 8.099382, 8.1001, 8.101096, 8.10272, 8.105708, 8.1079035, 8.107553, 8.105353, 8.100729, 8.097426, 8.095029, 8.093579, 8.091609, 8.089706, 8.088248, 8.087083, 8.085757, 8.085638, 8.085888, 8.086307, 8.086928, 8.088793, 8.091384, 8.095035, 8.0988455, 8.103363, 8.104768, 8.105501, 8.103443, 8.101969, 8.100795, 8.100706, 8.100728, 8.100205, 8.098737, 8.096964, 8.09556, 8.093618, 8.091822, 8.090307, 8.088181, 8.085023, 8.083037, 8.0823345, 8.082574, 8.083586, 8.085047, 8.086321, 8.086867, 8.08752, 8.0883255, 8.089245, 8.089914, 8.0907545, 8.092345, 8.093197, 8.094193, 8.094072, 8.092076, 8.090664, 8.088983, 8.087331, 8.085984, 8.086372, 8.08751, 8.087613, 8.088011, 8.088699, 8.089961, 8.089908, 8.089898, 8.089822, 8.09025, 8.090536, 8.090745, 8.091346, 8.092213, 8.092189, 8.091655, 8.091203, 8.090779, 8.090051, 8.089699, 8.089504, 8.088893, 8.088374, 8.087697, 8.086663, 8.085645, 8.084336, 8.083488, 8.083866, 8.083893, 8.0845175, 8.085946, 8.08592, 8.08634, 8.087846, 8.088615, 8.089857, 8.089635, 8.0886345, 8.087969, 8.0870905, 8.086644, 8.08693, 8.087643, 8.089751, 8.092484, 8.09443, 8.095991, 8.097779, 8.097506, 8.096529, 8.095423, 8.092646, 8.090822, 8.090413, 8.0909815, 8.090265, 8.090578, 8.0908985, 8.091227, 8.092297, 8.092543, 8.092594, 8.091574, 8.090516, 8.090214, 8.090294, 8.091467, 8.093393, 8.095202, 8.095161, 8.094642, 8.094143, 8.093362, 8.092502, 8.090305, 8.087616, 8.085962, 8.085594, 8.086102, 8.086392, 8.086704, 8.086952, 8.086864, 8.087159, 8.088586, 8.08933, 8.089428, 8.089602, 8.089748, 8.09133, 8.09218, 8.093729, 8.09511, 8.095604, 8.095638, 8.0955515, 8.09572, 8.095388, 8.095471, 8.099493, 8.103017, 8.1069565, 8.111063, 8.112877, 8.1136675, 8.114299, 8.114071, 8.112268, 8.110673, 8.1097975, 8.107405, 8.104303, 8.101585, 8.098662, 8.094158, 8.091425, 8.087937, 8.085584, 8.084855, 8.085601, 8.087113, 8.088916, 8.090656, 8.091742, 8.093019, 8.094814, 8.096673, 8.098621, 8.101297, 8.102447, 8.102532, 8.10218, 8.100231, 8.0973835, 8.094764, 8.092539, 8.091094, 8.090409, 8.090088, 8.089929, 8.089466, 8.089036, 8.088712, 8.088804, 8.087158, 8.085925, 8.085376, 8.085361, 8.085613, 8.086336, 8.087019, 8.087485, 8.087044, 8.086076, 8.084456, 8.082154, 8.081186, 8.081114, 8.081596, 8.081935, 8.081804, 8.081357, 8.080867, 8.080536, 8.081141, 8.082514, 8.083158, 8.083387, 8.083042, 8.082425, 8.080952, 8.0799, 8.079699, 8.080183, 8.081165, 8.082584, 8.084319, 8.085967, 8.087968, 8.090628, 8.093173, 8.094612, 8.094417, 8.092878, 8.090412, 8.08849, 8.08825, 8.088934, 8.089694, 8.090338, 8.0895605, 8.089448, 8.088975, 8.089074, 8.089172, 8.089735, 8.09009, 8.090325, 8.0899935, 8.089668, 8.087973, 8.086808, 8.086025, 8.084783, 8.083657, 8.082996, 8.082846, 8.082745, 8.082968, 8.082865, 8.08301, 8.082934, 8.083288, 8.083609, 8.084064, 8.084723, 8.086175, 8.088886, 8.092509, 8.095765, 8.098896, 8.101329, 8.102019, 8.101581, 8.100401, 8.099837, 8.099122, 8.098615, 8.098836, 8.09966, 8.099253, 8.098894, 8.097584, 8.095536, 8.093153, 8.090796, 8.089198, 8.087097, 8.085745, 8.0852, 8.084802, 8.084221, 8.083953, 8.083538, 8.083434, 8.083564, 8.083987, 8.085524, 8.086955, 8.086847, 8.087123, 8.087402, 8.087847, 8.087951, 8.087753, 8.087826, 8.087976, 8.088311, 8.089122, 8.090625, 8.091002, 8.092124, 8.0938, 8.0943575, 8.093319, 8.091966, 8.090808, 8.090029, 8.088891, 8.088261, 8.087923, 8.087257, 8.087049, 8.08703, 8.086771, 8.08605, 8.086409, 8.088273, 8.089869, 8.091355, 8.092187, 8.0925, 8.091958, 8.091038, 8.089707, 8.08838, 8.086792, 8.085434, 8.083948, 8.082487, 8.080189, 8.077905, 8.076234, 8.074801, 8.074215, 8.074245, 8.07508, 8.076019, 8.076855, 8.077035, 8.077642, 8.0782995, 8.078768, 8.07882, 8.078984, 8.079368, 8.079991, 8.081062, 8.083113, 8.08474, 8.086549, 8.088158, 8.0896, 8.090516, 8.090477, 8.089874, 8.089483, 8.089485, 8.089434, 8.0891075, 8.088402, 8.0876465, 8.087126, 8.086376, 8.085749, 8.085148, 8.083994, 8.082751, 8.082045, 8.081634, 8.081703, 8.082182, 8.082943, 8.084218, 8.084756, 8.085606, 8.08627, 8.086262, 8.086142, 8.086367, 8.087194, 8.087863, 8.087621, 8.086844, 8.08669, 8.086486, 8.086266, 8.0867195, 8.087006, 8.086948, 8.086237, 8.085821, 8.084796, 8.083793, 8.083114, 8.083152, 8.082804, 8.083545, 8.083773, 8.083921, 8.083063, 8.082715, 8.082431, 8.081722, 8.081391, 8.08041, 8.080036, 8.079845, 8.0793295, 8.079222, 8.078781, 8.078619, 8.078773, 8.079094, 8.079371, 8.080533, 8.08142, 8.082448, 8.083749, 8.085207, 8.086164, 8.086977, 8.087522, 8.088409, 8.088937, 8.089388, 8.089774, 8.089446, 8.088385, 8.0872135, 8.086377, 8.085883, 8.085508, 8.084619, 8.084804, 8.085745, 8.086802, 8.0879545, 8.08762, 8.086347, 8.0849085, 8.083517, 8.082432, 8.082001, 8.081731, 8.0814905, 8.080954, 8.081172, 8.081718, 8.083091, 8.084005, 8.085543, 8.085579, 8.084863, 8.085283, 8.085848, 8.086468, 8.087229, 8.088196, 8.088797, 8.08832, 8.0882225, 8.0882435, 8.08831, 8.088212, 8.088377, 8.087966, 8.087282, 8.085907, 8.084637, 8.083636, 8.082981, 8.082244, 8.081673, 8.081638, 8.081646, 8.081118, 8.080712, 8.080595, 8.080691, 8.081502, 8.082236, 8.083035, 8.08381, 8.083545, 8.082282, 8.081258, 8.0804, 8.080165, 8.080002, 8.079967, 8.080091, 8.0798025, 8.080052, 8.080085, 8.080288, 8.080679, 8.081095, 8.081533, 8.081971, 8.082178, 8.08276, 8.083178, 8.083305, 8.083091, 8.08274, 8.081656, 8.080109, 8.079481, 8.07887, 8.078682, 8.078715, 8.079266, 8.079886, 8.080813, 8.08142, 8.081752, 8.081715, 8.081259, 8.081182, 8.081227, 8.080783, 8.080774, 8.080977, 8.080934, 8.081919, 8.0837555, 8.085771, 8.087394, 8.088641, 8.089841, 8.09063, 8.091141, 8.091754, 8.092856, 8.0939665, 8.094553, 8.094466, 8.094311, 8.094439, 8.094825, 8.095092, 8.095918, 8.097004, 8.098048, 8.0985775, 8.096931, 8.094242, 8.092048, 8.090232, 8.088816, 8.088034, 8.087303, 8.086938, 8.087352, 8.086899, 8.086451, 8.086546, 8.086768, 8.086639, 8.0864935, 8.085763, 8.085575, 8.085851, 8.086433, 8.087114, 8.087591, 8.087674, 8.086671, 8.085556, 8.084297, 8.083715, 8.083062, 8.082371, 8.081677, 8.081133, 8.080906, 8.080484, 8.07994, 8.079457, 8.079514, 8.079675, 8.077798, 8.076522, 8.075574, 8.074852, 8.074914, 8.075479, 8.075948, 8.076288, 8.076488, 8.076906, 8.077667, 8.07793, 8.078752, 8.07886, 8.079967, 8.08101, 8.081848, 8.082648, 8.08329, 8.08381, 8.084202, 8.08458, 8.085399, 8.086181, 8.087203, 8.088187, 8.088934, 8.088964, 8.08872, 8.087928, 8.087346, 8.08676, 8.086068, 8.085623, 8.085729, 8.0859785, 8.086931, 8.087406, 8.087813, 8.088354, 8.089071, 8.089826, 8.090243, 8.09039, 8.090729, 8.09053, 8.089174, 8.087789, 8.086285, 8.085231, 8.0834675, 8.082135, 8.080435, 8.079295, 8.078683, 8.0778885, 8.077064, 8.076874, 8.076452, 8.076301, 8.076493, 8.077219, 8.078197, 8.079538, 8.0805235, 8.081528, 8.082189, 8.082387, 8.082723, 8.0825815, 8.08203, 8.081591, 8.081096, 8.080335, 8.07907, 8.078059, 8.077298, 8.076515, 8.076341, 8.076305, 8.077003, 8.078016, 8.079462, 8.080887, 8.082481, 8.083756, 8.085173, 8.086737, 8.088827, 8.089601, 8.089727, 8.089892, 8.090263, 8.089766, 8.0891285, 8.08874, 8.088052, 8.087656, 8.087507, 8.087101, 8.087041, 8.087173, 8.087456, 8.087711, 8.087474, 8.086635, 8.085551, 8.08439, 8.0833645, 8.082803, 8.083486, 8.0840025, 8.084472, 8.084622, 8.084636, 8.084628, 8.084816, 8.08529, 8.085801, 8.08636, 8.086331, 8.085663, 8.084742, 8.083227, 8.081928, 8.0814705, 8.081263, 8.08092, 8.080683, 8.079755, 8.07752, 8.07543, 8.0744295, 8.074117, 8.074406, 8.075561, 8.077019, 8.077888, 8.078458, 8.078492, 8.077976, 8.077171, 8.076907, 8.07689, 8.077086, 8.077142, 8.077009]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, val_labels_mat, val_mask_mat[:,1], placeholders, sess, model)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
