{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from models import GCN\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get paths to available grahs metadata\n",
    "N_sample = 200000\n",
    "adj_mat, features_mat, train_labels_mat, test_labels_mat, val_labels_mat, train_mask_mat, test_mask_mat, val_mask_mat = load_data(\"txt_graph2216_21012020\", N_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465542, 465542)\n",
      "(465542, 100)\n",
      "(465542, 5)\n",
      "(465542, 5)\n",
      "(465542, 5)\n",
      "(465542, 5)\n",
      "(465542, 5)\n",
      "(465542, 5)\n",
      "number of masked elements\n",
      "train: 779\n",
      "test: 668\n",
      "val: 778\n",
      "\n",
      " positions of ones in masked labels\n",
      "[2 1 0 4 0 4 4 1 3 0 3 3 0 4 2 0 0 1 4 1 1 3 0 2 0 1 1 0 4 0 4 3 3 2 2 0 2\n",
      " 2 4 4 1 3 2 2 0 4 4 4 3 1 2 0 3 1 2 0 1 3 4 0 3 3 3 4 0 1 4 1 3 4 3 1 1 1\n",
      " 3 0 0 4 3 2 1 0 0 0 2 0 4 3 0 0 4 4 3 2 3 3 1 4 3 4 1 0 3 2 4 3 4 4 0 1 4\n",
      " 3 1 0 1 3 3 4 4 3 3 3 4 3 2 3 1 2 3 1 3 0 2 0 3 2 4 2 0 2 0 4 2 0 2 1 0 2\n",
      " 0 0 3 0 1 2 3 3 3 1 4 3 2 4 3 3 0 4 3 1 3 1 3 0 4 2 4 2 0 3 2 3 2 2 2 0 2\n",
      " 2 4 0 2 3 0 2 0 0 0 2 0 0 2 2 2 3 2 0 3 1 4 3 0 0 4 1 2 2 4 0 2 3 0 4 2 3\n",
      " 0 0 2 1 0 0 3 3 3 1 2 4 0 3 0 3 3 1 4 3 2 4 2 3 4 0 3 4 0 3 3 4 0 0 0 3 3\n",
      " 0 2 1 4 4 3 0 0 4 4 1 1 4 3 0 3 4 3 2 3 2 1 4 3 4 0 3 0 4 4 0 4 1 2 2 3 3\n",
      " 3 0 2 4 3 3 2 3 4 2 1 0 2 4 4 1 4 3 4 3 1 3 3 1 3 2 0 4 1 0 2 3 0 4 4 4 1\n",
      " 3 3 4 1 0 4 2 3 3 0 0 1 2 3 1 0 4 0 2 3 1 1 0 2 3 3 1 0 4 3 3 2 4 0 0 1 1\n",
      " 0 0 0 2 0 0 3 3 4 3 1 1 0 4 2 1 0 0 3 1 4 1 3 0 3 1 2 4 3 3 4 1 4 2 0 2 1\n",
      " 4 3 3 2 4 0 0 1 1 2 0 2 2 0 3 0 3 3 2 0 1 4 2 0 2 1 3 0 4 0 0 0 4 3 0 2 0\n",
      " 2 3 0 3 4 4 1 0 1 4 1 2 3 4 3 0 1 2 3 4 2 4 3 2 2 0 4 2 0 3 4 3 3 3 2 4 2\n",
      " 1 4 4 0 3 2 2 1 0 1 4 3 2 1 1 3 0 2 2 0 2 3 2 3 1 0 1 3 1 4 0 0 4 4 0 0 0\n",
      " 0 2 3 3 1 1 2 0 0 3 2 4 4 2 3 1 0 2 0 3 2 4 2 1 0 3 4 1 1 4 2 3 1 3 3 1 4\n",
      " 1 3 4 2 2 2 2 3 0 2 3 3 4 0 4 0 2 4 0 4 0 0 3 4 0 4 0 3 4 3 3 1 4 0 4 4 3\n",
      " 3 3 3 1 1 2 0 2 3 3 2 3 3 4 1 0 2 3 0 1 4 1 0 1 2 0 2 1 1 0 4 3 4 3 4 3 3\n",
      " 0 2 0 0 1 1 3 1 1 0 1 1 3 3 0 3 1 2 3 4 2 4 4 4 3 3 0 3 0 3 4 4 2 0 4 3 3\n",
      " 0 3 1 0 1 0 0 0 4 0 3 3 2 4 3 3 4 1 0 0 0 0 0 3 2 3 4 0 3 3 0 4 1 2 1 0 2\n",
      " 1 1 0 0 2 3 2 1 3 3 3 0 1 3 0 0 2 3 2 1 0 4 3 0 2 1 4 0 4 3 0 4 4 2 0 4 4\n",
      " 0 3 2 2 0 1 4 4 2 4 2 0 3 3 0 3 1 2 4 0 4 1 2 2 2 0 2 0 0 4 4 4 2 3 1 1 4\n",
      " 1 1]\n",
      "[0 2 3 4 1 3 4 4 0 4 2 0 4 2 1 3 3 0 0 3 2 3 2 2 4 3 0 0 2 0 0 3 3 3 3 2 3\n",
      " 2 2 1 2 4 4 2 0 3 2 4 0 3 3 4 2 3 4 3 3 4 2 2 4 3 4 0 3 4 2 4 2 3 0 4 2 2\n",
      " 3 0 1 1 1 3 0 1 0 0 0 3 3 0 2 4 4 2 3 1 3 3 2 0 1 0 2 3 0 0 2 1 2 3 1 1 4\n",
      " 4 0 0 0 1 2 3 0 0 4 1 3 3 1 1 3 2 3 2 0 3 4 1 0 4 3 0 1 4 2 3 2 1 2 3 1 3\n",
      " 0 0 3 2 1 3 0 3 2 3 3 2 1 4 1 3 4 1 0 4 1 2 4 4 4 1 0 1 1 0 0 4 2 3 1 0 2\n",
      " 3 0 1 3 0 1 4 4 0 3 1 2 2 4 1 4 3 1 4 0 2 1 3 2 4 0 0 3 4 0 3 0 0 4 3 3 1\n",
      " 2 4 1 3 4 4 2 3 0 3 3 1 2 1 3 1 4 2 2 2 4 1 1 2 2 3 3 3 3 0 2 2 3 2 1 1 3\n",
      " 2 4 1 1 3 3 0 2 4 1 2 1 3 1 1 0 3 1 0 1 1 3 2 0 3 1 0 3 4 1 1 4 3 0 3 2 2\n",
      " 2 2 4 0 0 2 4 3 2 4 4 1 2 0 3 1 2 3 4 1 1 3 4 4 3 2 3 0 0 3 4 0 1 3 2 4 4\n",
      " 1 0 1 1 0 2 2 2 2 4 3 2 3 2 3 3 4 0 3 2 1 0 4 2 0 0 3 0 0 3 3 2 2 0 1 1 3\n",
      " 0 4 0 4 1 0 4 0 1 1 0 0 3 1 3 4 4 3 1 0 3 0 3 1 3 3 3 1 0 3 4 2 4 3 1 2 0\n",
      " 2 3 3 4 3 0 1 1 2 3 3 4 1 3 1 1 3 1 4 0 1 1 2 4 0 3 1 0 1 2 1 1 0 1 1 3 3\n",
      " 2 4 1 2 2 4 4 0 0 2 3 3 2 3 2 3 4 2 0 2 3 0 3 1 2 2 1 3 4 4 3 1 3 2 1 1 2\n",
      " 4 0 4 3 1 3 0 4 0 4 4 0 3 4 0 2 0 1 3 1 4 4 2 3 1 0 3 4 2 2 4 4 4 0 4 3 1\n",
      " 1 4 2 0 2 1 2 3 3 4 2 0 0 0 3 2 2 1 1 0 3 2 2 0 1 3 2 4 2 4 4 3 2 0 1 1 4\n",
      " 3 3 4 3 2 0 0 0 1 4 3 2 2 0 4 1 3 1 0 3 1 2 2 1 3 1 3 4 0 4 4 1 3 3 2 0 2\n",
      " 2 2 0 1 0 0 4 3 4 3 0 0 3 2 3 4 4 3 2 0 4 1 0 0 2 3 2 1 2 4 2 3 0 2 2 3 2\n",
      " 3 4 3 1 0 2 1 0 3 2 2 0 0 3 3 0 3 2 1 2 1 4 2 1 2 1 2 3 3 3 3 2 3 0 0 0 1\n",
      " 2 2]\n",
      "[3 0 2 2 4 3 1 2 2 3 0 2 4 2 4 4 0 0 3 3 0 0 1 4 4 1 2 1 2 3 1 0 4 4 2 2 3\n",
      " 2 1 0 1 1 3 4 0 1 0 2 1 2 3 4 0 0 1 4 1 0 4 2 0 3 3 1 3 1 3 4 0 3 3 0 1 3\n",
      " 3 2 3 3 2 1 2 0 0 1 1 1 3 0 3 3 3 0 3 1 1 1 4 4 3 4 1 0 0 3 2 4 4 0 4 3 2\n",
      " 0 3 2 0 2 2 3 1 4 4 4 4 0 3 3 0 4 2 4 1 4 4 4 2 3 2 1 4 0 1 4 3 0 1 1 1 0\n",
      " 4 0 2 2 3 1 1 3 0 1 0 2 0 2 2 3 2 3 2 0 0 1 2 2 2 0 0 0 0 2 3 3 1 0 0 4 0\n",
      " 4 4 0 0 4 1 1 0 1 3 4 0 0 3 4 3 3 0 4 1 4 0 0 3 2 1 2 3 3 3 3 3 1 0 3 2 0\n",
      " 3 0 1 1 4 0 0 3 0 3 0 0 1 4 4 1 4 2 0 2 4 4 1 3 0 4 2 4 0 0 1 4 4 4 0 0 0\n",
      " 2 0 0 4 4 4 3 1 2 0 2 3 0 1 1 1 2 3 3 0 0 1 3 1 4 4 3 0 0 0 2 4 3 3 1 2 0\n",
      " 4 3 1 3 0 3 3 0 4 1 4 1 1 2 0 0 4 0 0 0 4 2 1 1 1 0 2 0 1 0 4 0 3 2 1 3 0\n",
      " 4 2 3 0 0 0 3 3 3 3 4 4 1 1 0 0 0 0 4 4 2 3 1 2 0 1 1 2 2 0 1 1 0 3 3 1 1\n",
      " 0 3 4 4 3 0 0 2 1 1 3 0 0 2 1 0 0 4 4 3 3 2 0 2 2 3 3 4 2 4 4 3 4 3 3 4 3\n",
      " 2 1 1 3 4 1 1 2 1 3 2 2 0 0 2 2 0 4 1 0 4 1 2 2 3 4 3 1 3 4 3 2 1 1 1 1 3\n",
      " 0 0 3 0 0 0 3 0 0 2 2 3 0 4 1 2 2 3 3 3 2 1 3 0 0 3 0 2 0 1 0 0 4 3 4 4 4\n",
      " 0 1 2 2 4 2 3 2 4 0 2 0 1 3 3 4 0 2 1 2 0 2 1 4 0 3 4 2 4 4 0 0 1 0 0 3 2\n",
      " 0 4 4 4 3 3 2 1 2 1 1 1 0 4 2 1 3 0 1 4 1 0 3 2 4 1 2 3 1 3 0 0 2 0 3 4 4\n",
      " 1 3 3 1 0 3 3 4 0 3 3 4 0 2 0 1 0 0 1 4 0 2 2 3 2 2 1 4 0 4 3 1 3 0 0 3 4\n",
      " 4 4 2 2 2 1 0 1 3 3 2 4 0 0 3 3 0 4 4 2 0 2 0 2 2 3 2 0 0 4 4 2 0 0 2 3 2\n",
      " 1 0 2 0 4 2 3 0 2 4 2 1 1 0 0 2 0 1 4 4 4 0 3 1 4 1 4 1 2 2 1 3 1 1 1 3 1\n",
      " 0 2 0 3 2 1 0 0 2 0 3 0 3 1 1 0 3 3 1 4 0 0 3 2 1 2 3 1 1 2 3 4 0 3 0 3 3\n",
      " 2 1 2 0 4 3 0 3 4 2 2 0 0 3 1 0 4 4 2 0 1 4 4 1 4 4 3 4 0 3 0 0 2 2 1 1 4\n",
      " 4 2 3 4 2 3 2 4 3 3 3 0 1 0 3 2 0 4 0 0 1 1 4 0 4 0 1 2 0 1 2 4 2 0 2 0 3\n",
      " 2]\n",
      "\n",
      " ones outside the mask\n",
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(adj_mat.shape)\n",
    "print(features_mat.shape)\n",
    "print(train_labels_mat.shape)\n",
    "print(test_labels_mat.shape)\n",
    "print(val_labels_mat.shape)\n",
    "print(train_mask_mat.shape)\n",
    "print(test_mask_mat.shape)\n",
    "print(val_mask_mat.shape)\n",
    "\n",
    "print(\"number of masked elements\")\n",
    "print(\"train: \" + str(len(np.where(train_mask_mat[:,1]==1)[0])))\n",
    "print(\"test: \" + str(len(np.where(test_mask_mat[:,1]==1)[0])))\n",
    "print(\"val: \" + str(len(np.where(val_mask_mat[:,1]==1)[0])))\n",
    "\n",
    "print(\"\\n positions of ones in masked labels\")\n",
    "print(np.where(train_labels_mat[np.array(train_mask_mat[:,1],dtype=bool)]==1)[1])\n",
    "print(np.where(test_labels_mat[np.array(test_mask_mat[:,1],dtype=bool)]==1)[1])\n",
    "print(np.where(val_labels_mat[np.array(val_mask_mat[:,1],dtype=bool)]==1)[1])\n",
    "\n",
    "print(\"\\n ones outside the mask\")\n",
    "print(np.where(train_labels_mat[np.logical_not(np.array(train_mask_mat[:,1],dtype=bool))]==1))\n",
    "print(np.where(test_labels_mat[np.logical_not(np.array(test_mask_mat[:,1],dtype=bool))]==1))\n",
    "print(np.where(val_labels_mat[np.logical_not(np.array(val_mask_mat[:,1],dtype=bool))]==1))\n",
    "\n",
    "#sum_global_nodes = np.sum(adj_mat.toarray()[:,np.array(train_mask_mat[:,1],dtype=bool)], axis = 1) + np.sum(adj_mat.toarray()[:,np.array(test_mask_mat[:,1],dtype=bool)], axis = 1) + np.sum(adj_mat.toarray()[:,np.array(val_mask_mat[:,1],dtype=bool)], axis = 1)\n",
    "#print(adj_mat.shape[1] - sum(sum_global_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robertb/python/GCN_text_classif/gcn/utils.py:57: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    }
   ],
   "source": [
    "#delete all flags before declaration new one\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('mode', '', 'kernel') # for line by line mode\n",
    "tf.app.flags.DEFINE_string('port', '', 'kernel') # for line by line mode\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel') # for jupyter notebook\n",
    "\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.') #0.01\n",
    "flags.DEFINE_integer('epochs', 2000, 'Number of epochs to train.') #200\n",
    "flags.DEFINE_integer('hidden1', 64, 'Number of units in hidden layer 1.') #16\n",
    "flags.DEFINE_float('dropout', 0.8, 'Dropout rate (1 - keep probability).') #0.5\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 2000, 'Tolerance for early stopping (# of epochs).') #10\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features_mat)\n",
    "support = [preprocess_adj(adj_mat)]\n",
    "num_supports = 1\n",
    "model_func = GCN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/robertb/python/GCN_text_classif/gcn/models.py:95: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/python/GCN_text_classif/gcn/models.py:40: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/inits.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/layers.py:82: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/layers.py:26: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/layers.py:33: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/layers.py:170: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/robertb/python/GCN_text_classif/gcn/models.py:51: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/python/GCN_text_classif/gcn/models.py:51: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/robertb/.pyenv/versions/graph/lib/python3.6/site-packages/gcn-1.0-py3.6.egg/gcn/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, train_labels_mat.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 30.33150 train_acc= 0.22721 val_loss= 39.41757 val_acc= 0.72237 time= 10.71388\n",
      "Epoch: 0002 train_loss= 16.81936 train_acc= 0.20924 val_loss= 37.68326 val_acc= 0.70823 time= 10.01661\n",
      "Epoch: 0003 train_loss= 26.68737 train_acc= 0.22593 val_loss= 36.66272 val_acc= 0.69280 time= 10.01657\n",
      "Epoch: 0004 train_loss= 28.54598 train_acc= 0.21823 val_loss= 35.95023 val_acc= 0.66838 time= 10.07508\n",
      "Epoch: 0005 train_loss= 20.57083 train_acc= 0.22721 val_loss= 35.04095 val_acc= 0.64910 time= 10.03505\n",
      "Epoch: 0006 train_loss= 23.62959 train_acc= 0.21566 val_loss= 34.01050 val_acc= 0.62982 time= 10.04319\n",
      "Epoch: 0007 train_loss= 25.53231 train_acc= 0.19384 val_loss= 32.79931 val_acc= 0.62725 time= 10.14809\n",
      "Epoch: 0008 train_loss= 23.18822 train_acc= 0.23363 val_loss= 31.68569 val_acc= 0.61954 time= 9.98431\n",
      "Epoch: 0009 train_loss= 21.49719 train_acc= 0.21053 val_loss= 30.61210 val_acc= 0.61440 time= 10.07334\n",
      "Epoch: 0010 train_loss= 33.64248 train_acc= 0.19384 val_loss= 29.52277 val_acc= 0.60797 time= 10.03616\n",
      "Epoch: 0011 train_loss= 23.22820 train_acc= 0.21694 val_loss= 28.42147 val_acc= 0.59769 time= 10.01634\n",
      "Epoch: 0012 train_loss= 22.84588 train_acc= 0.21438 val_loss= 27.26761 val_acc= 0.59126 time= 10.00059\n",
      "Epoch: 0013 train_loss= 28.15431 train_acc= 0.23877 val_loss= 26.04314 val_acc= 0.59126 time= 10.09325\n",
      "Epoch: 0014 train_loss= 23.95174 train_acc= 0.19641 val_loss= 24.88573 val_acc= 0.58869 time= 10.00339\n",
      "Epoch: 0015 train_loss= 19.33304 train_acc= 0.20026 val_loss= 23.81691 val_acc= 0.58226 time= 10.01113\n",
      "Epoch: 0016 train_loss= 23.35598 train_acc= 0.20924 val_loss= 22.91148 val_acc= 0.57326 time= 10.04036\n",
      "Epoch: 0017 train_loss= 21.56052 train_acc= 0.22336 val_loss= 23.14679 val_acc= 0.54884 time= 10.03529\n",
      "Epoch: 0018 train_loss= 25.67305 train_acc= 0.19641 val_loss= 23.50206 val_acc= 0.54242 time= 10.02281\n",
      "Epoch: 0019 train_loss= 25.09290 train_acc= 0.22593 val_loss= 23.89862 val_acc= 0.53470 time= 10.00485\n",
      "Epoch: 0020 train_loss= 27.39264 train_acc= 0.19641 val_loss= 24.37520 val_acc= 0.51542 time= 10.07591\n",
      "Epoch: 0021 train_loss= 19.82828 train_acc= 0.23363 val_loss= 24.81748 val_acc= 0.50129 time= 10.06912\n",
      "Epoch: 0022 train_loss= 24.76236 train_acc= 0.20668 val_loss= 25.23985 val_acc= 0.49486 time= 10.04222\n",
      "Epoch: 0023 train_loss= 20.91928 train_acc= 0.20924 val_loss= 25.52662 val_acc= 0.48329 time= 10.52230\n",
      "Epoch: 0024 train_loss= 24.69373 train_acc= 0.20668 val_loss= 25.85886 val_acc= 0.47686 time= 10.31705\n",
      "Epoch: 0025 train_loss= 19.53985 train_acc= 0.20539 val_loss= 26.13939 val_acc= 0.47429 time= 10.14904\n",
      "Epoch: 0026 train_loss= 18.48641 train_acc= 0.18742 val_loss= 26.42439 val_acc= 0.47044 time= 10.14827\n",
      "Epoch: 0027 train_loss= 15.10655 train_acc= 0.20282 val_loss= 26.62718 val_acc= 0.46530 time= 10.03803\n",
      "Epoch: 0028 train_loss= 19.13650 train_acc= 0.21438 val_loss= 26.81292 val_acc= 0.46144 time= 10.03286\n",
      "Epoch: 0029 train_loss= 20.53760 train_acc= 0.20924 val_loss= 27.09163 val_acc= 0.46272 time= 10.05236\n",
      "Epoch: 0030 train_loss= 16.88383 train_acc= 0.21438 val_loss= 27.36971 val_acc= 0.46144 time= 10.02645\n",
      "Epoch: 0031 train_loss= 18.97995 train_acc= 0.20154 val_loss= 27.59387 val_acc= 0.45758 time= 10.00752\n",
      "Epoch: 0032 train_loss= 26.49120 train_acc= 0.21053 val_loss= 27.97974 val_acc= 0.46144 time= 10.09838\n",
      "Epoch: 0033 train_loss= 16.85279 train_acc= 0.21566 val_loss= 28.28302 val_acc= 0.45630 time= 10.05917\n",
      "Epoch: 0034 train_loss= 16.06248 train_acc= 0.23107 val_loss= 28.61886 val_acc= 0.45630 time= 10.03841\n",
      "Epoch: 0035 train_loss= 25.61025 train_acc= 0.22721 val_loss= 29.02945 val_acc= 0.45887 time= 10.03393\n",
      "Epoch: 0036 train_loss= 18.96479 train_acc= 0.23235 val_loss= 29.44853 val_acc= 0.46272 time= 9.99450\n",
      "Epoch: 0037 train_loss= 21.08729 train_acc= 0.21566 val_loss= 29.87613 val_acc= 0.46401 time= 10.04864\n",
      "Epoch: 0038 train_loss= 19.66954 train_acc= 0.23363 val_loss= 30.27986 val_acc= 0.46401 time= 10.14446\n",
      "Epoch: 0039 train_loss= 15.63927 train_acc= 0.20282 val_loss= 30.65070 val_acc= 0.46530 time= 10.00677\n",
      "Epoch: 0040 train_loss= 23.74373 train_acc= 0.20539 val_loss= 30.95825 val_acc= 0.46658 time= 10.10820\n",
      "Epoch: 0041 train_loss= 22.06090 train_acc= 0.20796 val_loss= 31.24516 val_acc= 0.47172 time= 10.04138\n",
      "Epoch: 0042 train_loss= 20.84033 train_acc= 0.19897 val_loss= 31.50195 val_acc= 0.46915 time= 10.05857\n",
      "Epoch: 0043 train_loss= 20.00509 train_acc= 0.22208 val_loss= 31.72740 val_acc= 0.46915 time= 10.00118\n",
      "Epoch: 0044 train_loss= 21.09463 train_acc= 0.20539 val_loss= 31.95000 val_acc= 0.46787 time= 10.04361\n",
      "Epoch: 0045 train_loss= 13.06258 train_acc= 0.23877 val_loss= 32.24564 val_acc= 0.46787 time= 10.14852\n",
      "Epoch: 0046 train_loss= 16.39295 train_acc= 0.19384 val_loss= 32.44986 val_acc= 0.46272 time= 10.02312\n",
      "Epoch: 0047 train_loss= 20.66700 train_acc= 0.22593 val_loss= 32.61364 val_acc= 0.45887 time= 10.04543\n",
      "Epoch: 0048 train_loss= 26.06840 train_acc= 0.22208 val_loss= 32.74863 val_acc= 0.45244 time= 9.99281\n",
      "Epoch: 0049 train_loss= 21.36515 train_acc= 0.22978 val_loss= 32.78788 val_acc= 0.43959 time= 10.00398\n",
      "Epoch: 0050 train_loss= 25.37960 train_acc= 0.19512 val_loss= 32.87888 val_acc= 0.43316 time= 10.08238\n",
      "Epoch: 0051 train_loss= 18.17984 train_acc= 0.20154 val_loss= 32.91633 val_acc= 0.42416 time= 10.14312\n",
      "Epoch: 0052 train_loss= 13.80497 train_acc= 0.22465 val_loss= 32.88339 val_acc= 0.42159 time= 10.03973\n",
      "Epoch: 0053 train_loss= 18.85738 train_acc= 0.20282 val_loss= 32.85091 val_acc= 0.41388 time= 10.69932\n",
      "Epoch: 0054 train_loss= 26.69802 train_acc= 0.20924 val_loss= 32.85963 val_acc= 0.40488 time= 10.22530\n",
      "Epoch: 0055 train_loss= 20.86179 train_acc= 0.19769 val_loss= 32.95164 val_acc= 0.39332 time= 10.06773\n",
      "Epoch: 0056 train_loss= 24.97447 train_acc= 0.20796 val_loss= 32.96809 val_acc= 0.38175 time= 10.04134\n",
      "Epoch: 0057 train_loss= 18.95733 train_acc= 0.20796 val_loss= 32.98352 val_acc= 0.37275 time= 10.25158\n",
      "Epoch: 0058 train_loss= 13.67663 train_acc= 0.22721 val_loss= 33.06282 val_acc= 0.37018 time= 10.08877\n",
      "Epoch: 0059 train_loss= 16.25856 train_acc= 0.19127 val_loss= 33.12662 val_acc= 0.36632 time= 10.05403\n",
      "Epoch: 0060 train_loss= 21.85145 train_acc= 0.21309 val_loss= 33.15312 val_acc= 0.35733 time= 10.03620\n",
      "Epoch: 0061 train_loss= 15.79061 train_acc= 0.22593 val_loss= 33.24054 val_acc= 0.35604 time= 10.01944\n",
      "Epoch: 0062 train_loss= 16.88350 train_acc= 0.21309 val_loss= 33.33949 val_acc= 0.36118 time= 10.07626\n",
      "Epoch: 0063 train_loss= 22.61569 train_acc= 0.20924 val_loss= 33.37252 val_acc= 0.35861 time= 10.09590\n",
      "Epoch: 0064 train_loss= 17.18325 train_acc= 0.22593 val_loss= 33.44285 val_acc= 0.36118 time= 10.04443\n",
      "Epoch: 0065 train_loss= 18.56265 train_acc= 0.22850 val_loss= 33.45361 val_acc= 0.36118 time= 10.02231\n",
      "Epoch: 0066 train_loss= 22.77436 train_acc= 0.19769 val_loss= 33.35721 val_acc= 0.35604 time= 10.03176\n",
      "Epoch: 0067 train_loss= 13.76147 train_acc= 0.25546 val_loss= 33.25648 val_acc= 0.35476 time= 10.04496\n",
      "Epoch: 0068 train_loss= 16.28947 train_acc= 0.21309 val_loss= 33.16203 val_acc= 0.35347 time= 10.00350\n",
      "Epoch: 0069 train_loss= 14.18797 train_acc= 0.21438 val_loss= 33.04854 val_acc= 0.35090 time= 10.06564\n",
      "Epoch: 0070 train_loss= 22.71703 train_acc= 0.22080 val_loss= 32.84707 val_acc= 0.34833 time= 10.10605\n",
      "Epoch: 0071 train_loss= 20.62721 train_acc= 0.21951 val_loss= 32.67652 val_acc= 0.35090 time= 10.01873\n",
      "Epoch: 0072 train_loss= 15.47788 train_acc= 0.21309 val_loss= 32.42221 val_acc= 0.35219 time= 10.01072\n",
      "Epoch: 0073 train_loss= 19.90364 train_acc= 0.20282 val_loss= 32.20514 val_acc= 0.35347 time= 10.05196\n",
      "Epoch: 0074 train_loss= 19.08827 train_acc= 0.19255 val_loss= 31.94729 val_acc= 0.34961 time= 10.01344\n",
      "Epoch: 0075 train_loss= 16.06900 train_acc= 0.20282 val_loss= 31.65778 val_acc= 0.34833 time= 10.00655\n",
      "Epoch: 0076 train_loss= 14.79319 train_acc= 0.21823 val_loss= 31.42137 val_acc= 0.34319 time= 10.22597\n",
      "Epoch: 0077 train_loss= 23.02143 train_acc= 0.21181 val_loss= 31.26193 val_acc= 0.33933 time= 10.03018\n",
      "Epoch: 0078 train_loss= 18.22747 train_acc= 0.20668 val_loss= 31.15307 val_acc= 0.34062 time= 10.03681\n",
      "Epoch: 0079 train_loss= 19.87485 train_acc= 0.20924 val_loss= 31.08811 val_acc= 0.33805 time= 10.04766\n",
      "Epoch: 0080 train_loss= 21.20543 train_acc= 0.22208 val_loss= 31.04863 val_acc= 0.33676 time= 10.06339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0081 train_loss= 19.02888 train_acc= 0.22080 val_loss= 30.96159 val_acc= 0.34190 time= 10.09306\n",
      "Epoch: 0082 train_loss= 20.47910 train_acc= 0.22721 val_loss= 30.85707 val_acc= 0.34319 time= 10.16634\n",
      "Epoch: 0083 train_loss= 12.13223 train_acc= 0.21823 val_loss= 30.75383 val_acc= 0.34190 time= 10.56029\n",
      "Epoch: 0084 train_loss= 19.33172 train_acc= 0.20924 val_loss= 30.67305 val_acc= 0.34319 time= 10.25206\n",
      "Epoch: 0085 train_loss= 13.63630 train_acc= 0.19255 val_loss= 30.61614 val_acc= 0.34319 time= 10.09942\n",
      "Epoch: 0086 train_loss= 18.40340 train_acc= 0.21053 val_loss= 30.58290 val_acc= 0.34319 time= 10.02798\n",
      "Epoch: 0087 train_loss= 16.76731 train_acc= 0.18870 val_loss= 30.56355 val_acc= 0.34190 time= 10.00977\n",
      "Epoch: 0088 train_loss= 15.22830 train_acc= 0.22465 val_loss= 30.48252 val_acc= 0.33933 time= 10.10460\n",
      "Epoch: 0089 train_loss= 21.75092 train_acc= 0.21053 val_loss= 30.35401 val_acc= 0.33548 time= 10.02415\n",
      "Epoch: 0090 train_loss= 16.74750 train_acc= 0.21823 val_loss= 30.27246 val_acc= 0.33548 time= 10.02171\n",
      "Epoch: 0091 train_loss= 13.79950 train_acc= 0.23107 val_loss= 30.16435 val_acc= 0.33933 time= 10.02065\n",
      "Epoch: 0092 train_loss= 14.29775 train_acc= 0.24904 val_loss= 30.03250 val_acc= 0.33548 time= 10.03710\n",
      "Epoch: 0093 train_loss= 17.00531 train_acc= 0.21566 val_loss= 29.89946 val_acc= 0.33548 time= 10.06236\n",
      "Epoch: 0094 train_loss= 15.35412 train_acc= 0.24262 val_loss= 29.74567 val_acc= 0.33548 time= 10.05380\n",
      "Epoch: 0095 train_loss= 17.62586 train_acc= 0.21053 val_loss= 29.55219 val_acc= 0.33676 time= 10.07720\n",
      "Epoch: 0096 train_loss= 18.07541 train_acc= 0.21566 val_loss= 29.32309 val_acc= 0.33805 time= 10.05093\n",
      "Epoch: 0097 train_loss= 12.57620 train_acc= 0.20796 val_loss= 29.14723 val_acc= 0.34190 time= 10.00868\n",
      "Epoch: 0098 train_loss= 19.25006 train_acc= 0.18742 val_loss= 28.97467 val_acc= 0.34062 time= 10.05899\n",
      "Epoch: 0099 train_loss= 11.96042 train_acc= 0.22208 val_loss= 28.88815 val_acc= 0.34576 time= 10.02219\n",
      "Epoch: 0100 train_loss= 20.17628 train_acc= 0.19255 val_loss= 28.81657 val_acc= 0.34961 time= 10.02837\n",
      "Epoch: 0101 train_loss= 14.44303 train_acc= 0.21181 val_loss= 28.85328 val_acc= 0.35219 time= 10.11784\n",
      "Epoch: 0102 train_loss= 17.16253 train_acc= 0.22850 val_loss= 28.86549 val_acc= 0.35347 time= 10.01888\n",
      "Epoch: 0103 train_loss= 15.27627 train_acc= 0.23748 val_loss= 28.85413 val_acc= 0.35861 time= 10.02569\n",
      "Epoch: 0104 train_loss= 15.35476 train_acc= 0.20411 val_loss= 28.76229 val_acc= 0.35861 time= 10.05479\n",
      "Epoch: 0105 train_loss= 16.12392 train_acc= 0.20668 val_loss= 28.72330 val_acc= 0.35733 time= 10.03948\n",
      "Epoch: 0106 train_loss= 15.19277 train_acc= 0.22721 val_loss= 28.64223 val_acc= 0.36118 time= 10.02599\n",
      "Epoch: 0107 train_loss= 12.98249 train_acc= 0.21694 val_loss= 28.56274 val_acc= 0.36375 time= 10.16768\n",
      "Epoch: 0108 train_loss= 13.55622 train_acc= 0.21951 val_loss= 28.49736 val_acc= 0.36889 time= 10.02071\n",
      "Epoch: 0109 train_loss= 16.34513 train_acc= 0.21951 val_loss= 28.41840 val_acc= 0.36761 time= 10.00988\n",
      "Epoch: 0110 train_loss= 9.86151 train_acc= 0.24390 val_loss= 28.34021 val_acc= 0.36632 time= 10.02844\n",
      "Epoch: 0111 train_loss= 23.53774 train_acc= 0.21309 val_loss= 28.17878 val_acc= 0.36632 time= 10.07909\n",
      "Epoch: 0112 train_loss= 20.97197 train_acc= 0.21951 val_loss= 27.99385 val_acc= 0.36889 time= 10.03947\n",
      "Epoch: 0113 train_loss= 17.13240 train_acc= 0.20026 val_loss= 27.78705 val_acc= 0.37018 time= 10.61937\n",
      "Epoch: 0114 train_loss= 21.14526 train_acc= 0.20668 val_loss= 27.52470 val_acc= 0.36632 time= 10.13617\n",
      "Epoch: 0115 train_loss= 13.48030 train_acc= 0.22593 val_loss= 27.26147 val_acc= 0.36504 time= 10.18225\n",
      "Epoch: 0116 train_loss= 14.88432 train_acc= 0.21823 val_loss= 27.05886 val_acc= 0.36118 time= 10.06822\n",
      "Epoch: 0117 train_loss= 18.62966 train_acc= 0.20411 val_loss= 26.86804 val_acc= 0.35476 time= 10.08753\n",
      "Epoch: 0118 train_loss= 13.54227 train_acc= 0.22336 val_loss= 26.69934 val_acc= 0.34833 time= 10.07209\n",
      "Epoch: 0119 train_loss= 14.77479 train_acc= 0.21823 val_loss= 26.56824 val_acc= 0.34447 time= 10.05496\n",
      "Epoch: 0120 train_loss= 14.88729 train_acc= 0.21566 val_loss= 26.47731 val_acc= 0.34062 time= 10.41638\n",
      "Epoch: 0121 train_loss= 17.50662 train_acc= 0.21438 val_loss= 26.41264 val_acc= 0.33290 time= 10.43650\n",
      "Epoch: 0122 train_loss= 14.87835 train_acc= 0.21694 val_loss= 26.29306 val_acc= 0.33290 time= 10.00982\n",
      "Epoch: 0123 train_loss= 13.42509 train_acc= 0.24262 val_loss= 26.15578 val_acc= 0.32648 time= 10.02981\n",
      "Epoch: 0124 train_loss= 18.73548 train_acc= 0.22080 val_loss= 25.89095 val_acc= 0.32648 time= 10.03618\n",
      "Epoch: 0125 train_loss= 15.12293 train_acc= 0.21823 val_loss= 25.67790 val_acc= 0.32262 time= 10.00762\n",
      "Epoch: 0126 train_loss= 10.21852 train_acc= 0.20924 val_loss= 25.46547 val_acc= 0.32134 time= 10.10588\n",
      "Epoch: 0127 train_loss= 12.42346 train_acc= 0.20796 val_loss= 25.31334 val_acc= 0.31877 time= 10.10924\n",
      "Epoch: 0128 train_loss= 17.17189 train_acc= 0.23620 val_loss= 25.16397 val_acc= 0.32134 time= 10.14555\n",
      "Epoch: 0129 train_loss= 14.91900 train_acc= 0.23492 val_loss= 25.02213 val_acc= 0.31877 time= 10.11958\n",
      "Epoch: 0130 train_loss= 11.71852 train_acc= 0.22208 val_loss= 24.89743 val_acc= 0.32648 time= 10.08828\n",
      "Epoch: 0131 train_loss= 13.34215 train_acc= 0.23363 val_loss= 24.80370 val_acc= 0.32776 time= 10.04652\n",
      "Epoch: 0132 train_loss= 14.15359 train_acc= 0.21053 val_loss= 24.67985 val_acc= 0.32905 time= 10.19161\n",
      "Epoch: 0133 train_loss= 12.42009 train_acc= 0.22593 val_loss= 24.54455 val_acc= 0.33290 time= 10.07243\n",
      "Epoch: 0134 train_loss= 13.34191 train_acc= 0.20154 val_loss= 24.39533 val_acc= 0.32519 time= 10.07717\n",
      "Epoch: 0135 train_loss= 14.10464 train_acc= 0.22721 val_loss= 24.24030 val_acc= 0.32134 time= 10.09782\n",
      "Epoch: 0136 train_loss= 11.40422 train_acc= 0.22850 val_loss= 24.06708 val_acc= 0.32005 time= 10.07432\n",
      "Epoch: 0137 train_loss= 14.68306 train_acc= 0.23235 val_loss= 23.97888 val_acc= 0.31491 time= 10.07459\n",
      "Epoch: 0138 train_loss= 15.41291 train_acc= 0.23620 val_loss= 23.86271 val_acc= 0.30848 time= 10.14388\n",
      "Epoch: 0139 train_loss= 14.21029 train_acc= 0.20154 val_loss= 23.75949 val_acc= 0.30463 time= 10.03789\n",
      "Epoch: 0140 train_loss= 16.13368 train_acc= 0.22593 val_loss= 23.70741 val_acc= 0.30463 time= 10.08401\n",
      "Epoch: 0141 train_loss= 11.70360 train_acc= 0.22465 val_loss= 23.66511 val_acc= 0.30463 time= 10.08605\n",
      "Epoch: 0142 train_loss= 16.03068 train_acc= 0.22336 val_loss= 23.62171 val_acc= 0.30463 time= 10.02516\n",
      "Epoch: 0143 train_loss= 10.16848 train_acc= 0.21053 val_loss= 23.58752 val_acc= 0.30206 time= 10.61310\n",
      "Epoch: 0144 train_loss= 15.75734 train_acc= 0.21694 val_loss= 23.58349 val_acc= 0.29820 time= 10.26367\n",
      "Epoch: 0145 train_loss= 15.79313 train_acc= 0.20154 val_loss= 23.56407 val_acc= 0.29692 time= 10.06652\n",
      "Epoch: 0146 train_loss= 11.81378 train_acc= 0.22593 val_loss= 23.50588 val_acc= 0.29692 time= 10.03820\n",
      "Epoch: 0147 train_loss= 9.78328 train_acc= 0.25032 val_loss= 23.42124 val_acc= 0.29820 time= 10.09005\n",
      "Epoch: 0148 train_loss= 11.70792 train_acc= 0.19769 val_loss= 23.36078 val_acc= 0.30077 time= 10.02435\n",
      "Epoch: 0149 train_loss= 11.08524 train_acc= 0.23620 val_loss= 23.27391 val_acc= 0.30077 time= 10.07103\n",
      "Epoch: 0150 train_loss= 14.36255 train_acc= 0.22593 val_loss= 23.11367 val_acc= 0.30463 time= 10.07280\n",
      "Epoch: 0151 train_loss= 12.28698 train_acc= 0.22850 val_loss= 22.95193 val_acc= 0.30848 time= 10.13738\n",
      "Epoch: 0152 train_loss= 11.69849 train_acc= 0.24134 val_loss= 22.79456 val_acc= 0.31362 time= 10.12795\n",
      "Epoch: 0153 train_loss= 16.26058 train_acc= 0.23107 val_loss= 22.64399 val_acc= 0.31877 time= 10.05526\n",
      "Epoch: 0154 train_loss= 14.98880 train_acc= 0.24519 val_loss= 22.50880 val_acc= 0.32648 time= 10.03804\n",
      "Epoch: 0155 train_loss= 14.44229 train_acc= 0.19641 val_loss= 22.37570 val_acc= 0.32905 time= 10.05406\n",
      "Epoch: 0156 train_loss= 9.22316 train_acc= 0.22850 val_loss= 22.26644 val_acc= 0.32905 time= 10.04756\n",
      "Epoch: 0157 train_loss= 10.91508 train_acc= 0.20924 val_loss= 22.19081 val_acc= 0.33033 time= 10.23881\n",
      "Epoch: 0158 train_loss= 12.63656 train_acc= 0.23235 val_loss= 22.12085 val_acc= 0.33162 time= 10.04064\n",
      "Epoch: 0159 train_loss= 15.57572 train_acc= 0.23363 val_loss= 22.05674 val_acc= 0.33162 time= 10.03500\n",
      "Epoch: 0160 train_loss= 15.37927 train_acc= 0.23107 val_loss= 21.98686 val_acc= 0.33290 time= 10.05357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0161 train_loss= 10.67792 train_acc= 0.24647 val_loss= 21.90744 val_acc= 0.33162 time= 10.01205\n",
      "Epoch: 0162 train_loss= 15.27615 train_acc= 0.24005 val_loss= 21.80357 val_acc= 0.33548 time= 10.04854\n",
      "Epoch: 0163 train_loss= 11.57002 train_acc= 0.20924 val_loss= 21.73407 val_acc= 0.33290 time= 10.13722\n",
      "Epoch: 0164 train_loss= 9.30929 train_acc= 0.24647 val_loss= 21.67608 val_acc= 0.33033 time= 10.09720\n",
      "Epoch: 0165 train_loss= 15.22788 train_acc= 0.22721 val_loss= 21.63949 val_acc= 0.32391 time= 10.04852\n",
      "Epoch: 0166 train_loss= 13.88878 train_acc= 0.21053 val_loss= 21.59272 val_acc= 0.32134 time= 10.00591\n",
      "Epoch: 0167 train_loss= 8.65047 train_acc= 0.22080 val_loss= 21.55916 val_acc= 0.32262 time= 10.02684\n",
      "Epoch: 0168 train_loss= 17.35087 train_acc= 0.21694 val_loss= 21.51568 val_acc= 0.32391 time= 9.99444\n",
      "Epoch: 0169 train_loss= 11.91391 train_acc= 0.21181 val_loss= 21.51903 val_acc= 0.32519 time= 10.26238\n",
      "Epoch: 0170 train_loss= 10.18093 train_acc= 0.24775 val_loss= 21.51010 val_acc= 0.32519 time= 10.09935\n",
      "Epoch: 0171 train_loss= 14.22613 train_acc= 0.22080 val_loss= 21.52193 val_acc= 0.32134 time= 10.05346\n",
      "Epoch: 0172 train_loss= 10.01592 train_acc= 0.24519 val_loss= 21.51331 val_acc= 0.31491 time= 10.03670\n",
      "Epoch: 0173 train_loss= 11.83237 train_acc= 0.22336 val_loss= 21.53469 val_acc= 0.30848 time= 10.88150\n",
      "Epoch: 0174 train_loss= 11.28229 train_acc= 0.21823 val_loss= 21.50652 val_acc= 0.30334 time= 10.09477\n",
      "Epoch: 0175 train_loss= 8.41964 train_acc= 0.22080 val_loss= 21.45482 val_acc= 0.29692 time= 10.07324\n",
      "Epoch: 0176 train_loss= 11.83759 train_acc= 0.21309 val_loss= 21.40510 val_acc= 0.29692 time= 10.23350\n",
      "Epoch: 0177 train_loss= 12.10268 train_acc= 0.23748 val_loss= 21.37078 val_acc= 0.29049 time= 10.02527\n",
      "Epoch: 0178 train_loss= 12.73852 train_acc= 0.23235 val_loss= 21.36208 val_acc= 0.28535 time= 10.02164\n",
      "Epoch: 0179 train_loss= 8.86394 train_acc= 0.23107 val_loss= 21.33218 val_acc= 0.28406 time= 10.06773\n",
      "Epoch: 0180 train_loss= 9.18308 train_acc= 0.21309 val_loss= 21.31129 val_acc= 0.28406 time= 10.01254\n",
      "Epoch: 0181 train_loss= 16.62104 train_acc= 0.22465 val_loss= 21.30559 val_acc= 0.28406 time= 10.05915\n",
      "Epoch: 0182 train_loss= 10.34829 train_acc= 0.25160 val_loss= 21.26537 val_acc= 0.28406 time= 10.18491\n",
      "Epoch: 0183 train_loss= 10.43856 train_acc= 0.23363 val_loss= 21.23868 val_acc= 0.28535 time= 10.08796\n",
      "Epoch: 0184 train_loss= 10.17181 train_acc= 0.23235 val_loss= 21.24804 val_acc= 0.28406 time= 10.05499\n",
      "Epoch: 0185 train_loss= 11.00554 train_acc= 0.18742 val_loss= 21.23927 val_acc= 0.28149 time= 10.02377\n",
      "Epoch: 0186 train_loss= 14.45132 train_acc= 0.24262 val_loss= 21.23185 val_acc= 0.28278 time= 10.05296\n",
      "Epoch: 0187 train_loss= 9.19504 train_acc= 0.25802 val_loss= 21.21529 val_acc= 0.28535 time= 10.08141\n",
      "Epoch: 0188 train_loss= 9.48880 train_acc= 0.20411 val_loss= 21.19933 val_acc= 0.28406 time= 10.24461\n",
      "Epoch: 0189 train_loss= 14.39093 train_acc= 0.19769 val_loss= 21.21194 val_acc= 0.28406 time= 10.03684\n",
      "Epoch: 0190 train_loss= 13.09236 train_acc= 0.24262 val_loss= 21.20038 val_acc= 0.28278 time= 10.04845\n",
      "Epoch: 0191 train_loss= 11.39286 train_acc= 0.22850 val_loss= 21.18510 val_acc= 0.28278 time= 10.00304\n",
      "Epoch: 0192 train_loss= 8.49208 train_acc= 0.22593 val_loss= 21.16502 val_acc= 0.28663 time= 10.05098\n",
      "Epoch: 0193 train_loss= 11.22661 train_acc= 0.18614 val_loss= 21.12634 val_acc= 0.28535 time= 10.01326\n",
      "Epoch: 0194 train_loss= 8.03755 train_acc= 0.21566 val_loss= 21.06721 val_acc= 0.28149 time= 10.10929\n",
      "Epoch: 0195 train_loss= 10.97972 train_acc= 0.21053 val_loss= 21.00082 val_acc= 0.28149 time= 10.05867\n",
      "Epoch: 0196 train_loss= 9.89676 train_acc= 0.21566 val_loss= 20.94323 val_acc= 0.27892 time= 10.05203\n",
      "Epoch: 0197 train_loss= 8.90624 train_acc= 0.23235 val_loss= 20.88552 val_acc= 0.27506 time= 10.03912\n",
      "Epoch: 0198 train_loss= 14.23276 train_acc= 0.22850 val_loss= 20.81860 val_acc= 0.26864 time= 10.05721\n",
      "Epoch: 0199 train_loss= 12.63840 train_acc= 0.22978 val_loss= 20.75334 val_acc= 0.26350 time= 10.07235\n",
      "Epoch: 0200 train_loss= 10.65180 train_acc= 0.24904 val_loss= 20.69456 val_acc= 0.25835 time= 10.04731\n",
      "Epoch: 0201 train_loss= 9.20395 train_acc= 0.23492 val_loss= 20.62001 val_acc= 0.25450 time= 10.15635\n",
      "Epoch: 0202 train_loss= 12.69575 train_acc= 0.23877 val_loss= 20.53116 val_acc= 0.25064 time= 10.03735\n",
      "Epoch: 0203 train_loss= 10.21630 train_acc= 0.21438 val_loss= 20.40803 val_acc= 0.25064 time= 10.88857\n",
      "Epoch: 0204 train_loss= 8.12434 train_acc= 0.21438 val_loss= 20.31204 val_acc= 0.24807 time= 10.06971\n",
      "Epoch: 0205 train_loss= 9.82102 train_acc= 0.23748 val_loss= 20.21585 val_acc= 0.25064 time= 10.07346\n",
      "Epoch: 0206 train_loss= 10.06243 train_acc= 0.19897 val_loss= 20.13146 val_acc= 0.24936 time= 10.04789\n",
      "Epoch: 0207 train_loss= 8.92599 train_acc= 0.22336 val_loss= 20.06336 val_acc= 0.24550 time= 10.17333\n",
      "Epoch: 0208 train_loss= 11.36775 train_acc= 0.22721 val_loss= 20.01283 val_acc= 0.24679 time= 10.04279\n",
      "Epoch: 0209 train_loss= 8.40534 train_acc= 0.21823 val_loss= 19.96980 val_acc= 0.24293 time= 10.00799\n",
      "Epoch: 0210 train_loss= 11.82512 train_acc= 0.21566 val_loss= 19.90026 val_acc= 0.25450 time= 10.03186\n",
      "Epoch: 0211 train_loss= 9.19944 train_acc= 0.24519 val_loss= 19.83183 val_acc= 0.26093 time= 10.05684\n",
      "Epoch: 0212 train_loss= 9.59580 train_acc= 0.23363 val_loss= 19.77369 val_acc= 0.26478 time= 10.07825\n",
      "Epoch: 0213 train_loss= 11.77322 train_acc= 0.22850 val_loss= 19.70456 val_acc= 0.26735 time= 10.13677\n",
      "Epoch: 0214 train_loss= 8.23608 train_acc= 0.21694 val_loss= 19.63178 val_acc= 0.26992 time= 10.02641\n",
      "Epoch: 0215 train_loss= 10.93770 train_acc= 0.22850 val_loss= 19.54103 val_acc= 0.27121 time= 10.06599\n",
      "Epoch: 0216 train_loss= 8.53551 train_acc= 0.21053 val_loss= 19.44455 val_acc= 0.27378 time= 10.01723\n",
      "Epoch: 0217 train_loss= 8.37811 train_acc= 0.23620 val_loss= 19.35812 val_acc= 0.27892 time= 10.03538\n",
      "Epoch: 0218 train_loss= 12.65876 train_acc= 0.20796 val_loss= 19.25509 val_acc= 0.28663 time= 10.02112\n",
      "Epoch: 0219 train_loss= 8.63991 train_acc= 0.21951 val_loss= 19.17130 val_acc= 0.29049 time= 10.18267\n",
      "Epoch: 0220 train_loss= 10.04348 train_acc= 0.19897 val_loss= 19.09536 val_acc= 0.29820 time= 10.03861\n",
      "Epoch: 0221 train_loss= 13.07038 train_acc= 0.24262 val_loss= 19.00856 val_acc= 0.30206 time= 10.01927\n",
      "Epoch: 0222 train_loss= 9.71654 train_acc= 0.22336 val_loss= 18.93170 val_acc= 0.31491 time= 10.03439\n",
      "Epoch: 0223 train_loss= 8.92857 train_acc= 0.23235 val_loss= 18.84569 val_acc= 0.32648 time= 10.10064\n",
      "Epoch: 0224 train_loss= 7.13227 train_acc= 0.22721 val_loss= 18.74002 val_acc= 0.34062 time= 10.05877\n",
      "Epoch: 0225 train_loss= 10.80717 train_acc= 0.23620 val_loss= 18.63518 val_acc= 0.34319 time= 10.04830\n",
      "Epoch: 0226 train_loss= 8.98600 train_acc= 0.23363 val_loss= 18.52230 val_acc= 0.34833 time= 10.06680\n",
      "Epoch: 0227 train_loss= 9.17124 train_acc= 0.22208 val_loss= 18.42159 val_acc= 0.35090 time= 10.05916\n",
      "Epoch: 0228 train_loss= 6.92709 train_acc= 0.20924 val_loss= 18.33807 val_acc= 0.35219 time= 10.02602\n",
      "Epoch: 0229 train_loss= 10.02904 train_acc= 0.22080 val_loss= 18.25411 val_acc= 0.35090 time= 10.03453\n",
      "Epoch: 0230 train_loss= 8.76827 train_acc= 0.24005 val_loss= 18.16479 val_acc= 0.35476 time= 10.02647\n",
      "Epoch: 0231 train_loss= 9.70280 train_acc= 0.22721 val_loss= 18.10431 val_acc= 0.35733 time= 10.09016\n",
      "Epoch: 0232 train_loss= 13.47476 train_acc= 0.22208 val_loss= 18.02650 val_acc= 0.36375 time= 10.10169\n",
      "Epoch: 0233 train_loss= 9.45678 train_acc= 0.21694 val_loss= 17.95393 val_acc= 0.36889 time= 10.79672\n",
      "Epoch: 0234 train_loss= 8.97330 train_acc= 0.21053 val_loss= 17.88391 val_acc= 0.37661 time= 10.04496\n",
      "Epoch: 0235 train_loss= 10.21977 train_acc= 0.20282 val_loss= 17.81598 val_acc= 0.37404 time= 10.09983\n",
      "Epoch: 0236 train_loss= 8.21457 train_acc= 0.23748 val_loss= 17.76132 val_acc= 0.37789 time= 10.04218\n",
      "Epoch: 0237 train_loss= 8.08186 train_acc= 0.24775 val_loss= 17.70998 val_acc= 0.37789 time= 10.10902\n",
      "Epoch: 0238 train_loss= 8.42550 train_acc= 0.23363 val_loss= 17.66340 val_acc= 0.37789 time= 10.24683\n",
      "Epoch: 0239 train_loss= 8.30478 train_acc= 0.22593 val_loss= 17.60434 val_acc= 0.37661 time= 10.02570\n",
      "Epoch: 0240 train_loss= 6.41674 train_acc= 0.26187 val_loss= 17.55067 val_acc= 0.37789 time= 10.07216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0241 train_loss= 9.41452 train_acc= 0.23363 val_loss= 17.50740 val_acc= 0.38303 time= 10.10352\n",
      "Epoch: 0242 train_loss= 7.88681 train_acc= 0.24775 val_loss= 17.48282 val_acc= 0.38303 time= 10.10715\n",
      "Epoch: 0243 train_loss= 9.22976 train_acc= 0.21566 val_loss= 17.43946 val_acc= 0.38432 time= 10.08175\n",
      "Epoch: 0244 train_loss= 9.06951 train_acc= 0.23620 val_loss= 17.41250 val_acc= 0.38560 time= 10.11469\n",
      "Epoch: 0245 train_loss= 8.34277 train_acc= 0.21309 val_loss= 17.38198 val_acc= 0.38560 time= 10.01817\n",
      "Epoch: 0246 train_loss= 10.15687 train_acc= 0.24519 val_loss= 17.37591 val_acc= 0.39075 time= 10.01461\n",
      "Epoch: 0247 train_loss= 7.10354 train_acc= 0.22978 val_loss= 17.37362 val_acc= 0.39075 time= 10.08671\n",
      "Epoch: 0248 train_loss= 9.65806 train_acc= 0.25802 val_loss= 17.35454 val_acc= 0.39203 time= 10.04643\n",
      "Epoch: 0249 train_loss= 5.31145 train_acc= 0.23235 val_loss= 17.33066 val_acc= 0.39075 time= 10.02977\n",
      "Epoch: 0250 train_loss= 7.19428 train_acc= 0.26829 val_loss= 17.30233 val_acc= 0.38946 time= 10.09886\n",
      "Epoch: 0251 train_loss= 8.39866 train_acc= 0.25546 val_loss= 17.28163 val_acc= 0.38046 time= 10.12728\n",
      "Epoch: 0252 train_loss= 7.64272 train_acc= 0.23363 val_loss= 17.27137 val_acc= 0.37532 time= 10.07701\n",
      "Epoch: 0253 train_loss= 7.87135 train_acc= 0.23363 val_loss= 17.26196 val_acc= 0.37018 time= 10.05085\n",
      "Epoch: 0254 train_loss= 10.27627 train_acc= 0.24519 val_loss= 17.25071 val_acc= 0.36632 time= 10.08563\n",
      "Epoch: 0255 train_loss= 8.27254 train_acc= 0.25289 val_loss= 17.22401 val_acc= 0.35990 time= 10.06807\n",
      "Epoch: 0256 train_loss= 10.62110 train_acc= 0.21694 val_loss= 17.20232 val_acc= 0.35347 time= 10.07482\n",
      "Epoch: 0257 train_loss= 9.76221 train_acc= 0.22850 val_loss= 17.14116 val_acc= 0.34190 time= 10.13735\n",
      "Epoch: 0258 train_loss= 7.05395 train_acc= 0.20924 val_loss= 17.05355 val_acc= 0.33548 time= 10.10740\n",
      "Epoch: 0259 train_loss= 10.89289 train_acc= 0.23363 val_loss= 16.95670 val_acc= 0.33290 time= 10.26596\n",
      "Epoch: 0260 train_loss= 10.56625 train_acc= 0.22850 val_loss= 16.85572 val_acc= 0.33162 time= 10.11366\n",
      "Epoch: 0261 train_loss= 8.69494 train_acc= 0.23877 val_loss= 16.75721 val_acc= 0.32905 time= 10.08019\n",
      "Epoch: 0262 train_loss= 13.02204 train_acc= 0.21951 val_loss= 16.65714 val_acc= 0.32519 time= 10.84225\n",
      "Epoch: 0263 train_loss= 9.65751 train_acc= 0.23492 val_loss= 16.56132 val_acc= 0.31877 time= 10.24778\n",
      "Epoch: 0264 train_loss= 9.39984 train_acc= 0.21694 val_loss= 16.47090 val_acc= 0.31748 time= 10.01819\n",
      "Epoch: 0265 train_loss= 5.86712 train_acc= 0.21951 val_loss= 16.36800 val_acc= 0.31620 time= 10.02866\n",
      "Epoch: 0266 train_loss= 8.34680 train_acc= 0.21694 val_loss= 16.25902 val_acc= 0.31105 time= 10.12161\n",
      "Epoch: 0267 train_loss= 8.07157 train_acc= 0.24134 val_loss= 16.15190 val_acc= 0.30206 time= 10.02749\n",
      "Epoch: 0268 train_loss= 9.16756 train_acc= 0.23235 val_loss= 16.05602 val_acc= 0.29563 time= 10.00459\n",
      "Epoch: 0269 train_loss= 6.95076 train_acc= 0.21823 val_loss= 15.95269 val_acc= 0.29177 time= 10.24884\n",
      "Epoch: 0270 train_loss= 8.37675 train_acc= 0.24262 val_loss= 15.88020 val_acc= 0.28663 time= 10.06745\n",
      "Epoch: 0271 train_loss= 7.96884 train_acc= 0.19384 val_loss= 15.81167 val_acc= 0.28149 time= 10.13313\n",
      "Epoch: 0272 train_loss= 9.01558 train_acc= 0.24904 val_loss= 15.75001 val_acc= 0.27892 time= 10.11731\n",
      "Epoch: 0273 train_loss= 7.24666 train_acc= 0.24262 val_loss= 15.69253 val_acc= 0.27892 time= 10.08443\n",
      "Epoch: 0274 train_loss= 10.94867 train_acc= 0.20796 val_loss= 15.62467 val_acc= 0.27763 time= 10.09235\n",
      "Epoch: 0275 train_loss= 10.40398 train_acc= 0.22593 val_loss= 15.56089 val_acc= 0.27892 time= 10.15920\n",
      "Epoch: 0276 train_loss= 9.64091 train_acc= 0.22721 val_loss= 15.48774 val_acc= 0.28021 time= 10.07689\n",
      "Epoch: 0277 train_loss= 7.65327 train_acc= 0.25032 val_loss= 15.42872 val_acc= 0.28278 time= 10.09626\n",
      "Epoch: 0278 train_loss= 10.91750 train_acc= 0.21694 val_loss= 15.38442 val_acc= 0.28278 time= 10.12561\n",
      "Epoch: 0279 train_loss= 9.38813 train_acc= 0.21951 val_loss= 15.35308 val_acc= 0.28149 time= 10.09934\n",
      "Epoch: 0280 train_loss= 9.11429 train_acc= 0.20924 val_loss= 15.31653 val_acc= 0.27763 time= 10.00574\n",
      "Epoch: 0281 train_loss= 7.87298 train_acc= 0.23877 val_loss= 15.28687 val_acc= 0.27635 time= 10.01027\n",
      "Epoch: 0282 train_loss= 7.97610 train_acc= 0.24262 val_loss= 15.27033 val_acc= 0.27378 time= 10.13199\n",
      "Epoch: 0283 train_loss= 8.14473 train_acc= 0.23107 val_loss= 15.23406 val_acc= 0.27378 time= 10.12696\n",
      "Epoch: 0284 train_loss= 7.56353 train_acc= 0.21053 val_loss= 15.19657 val_acc= 0.27635 time= 10.08851\n",
      "Epoch: 0285 train_loss= 8.13876 train_acc= 0.20668 val_loss= 15.16624 val_acc= 0.28021 time= 10.03012\n",
      "Epoch: 0286 train_loss= 7.81318 train_acc= 0.21823 val_loss= 15.12693 val_acc= 0.28021 time= 10.03293\n",
      "Epoch: 0287 train_loss= 7.58261 train_acc= 0.24519 val_loss= 15.07802 val_acc= 0.27892 time= 10.01765\n",
      "Epoch: 0288 train_loss= 7.52802 train_acc= 0.23235 val_loss= 15.02507 val_acc= 0.28535 time= 10.14629\n",
      "Epoch: 0289 train_loss= 8.35372 train_acc= 0.21309 val_loss= 14.96292 val_acc= 0.28663 time= 10.08479\n",
      "Epoch: 0290 train_loss= 8.58132 train_acc= 0.22978 val_loss= 14.90427 val_acc= 0.29049 time= 10.08173\n",
      "Epoch: 0291 train_loss= 7.19594 train_acc= 0.23107 val_loss= 14.84786 val_acc= 0.29306 time= 10.04200\n",
      "Epoch: 0292 train_loss= 8.36800 train_acc= 0.21694 val_loss= 14.79908 val_acc= 0.29563 time= 10.67151\n",
      "Epoch: 0293 train_loss= 7.91056 train_acc= 0.24904 val_loss= 14.75396 val_acc= 0.30206 time= 10.07816\n",
      "Epoch: 0294 train_loss= 6.97300 train_acc= 0.23877 val_loss= 14.69054 val_acc= 0.30463 time= 10.13256\n",
      "Epoch: 0295 train_loss= 7.80616 train_acc= 0.22465 val_loss= 14.62246 val_acc= 0.30977 time= 10.08785\n",
      "Epoch: 0296 train_loss= 9.78010 train_acc= 0.24134 val_loss= 14.56380 val_acc= 0.31234 time= 10.06010\n",
      "Epoch: 0297 train_loss= 8.70177 train_acc= 0.22721 val_loss= 14.50655 val_acc= 0.31362 time= 10.07083\n",
      "Epoch: 0298 train_loss= 7.06174 train_acc= 0.27214 val_loss= 14.44552 val_acc= 0.31362 time= 10.02027\n",
      "Epoch: 0299 train_loss= 7.11037 train_acc= 0.21309 val_loss= 14.38805 val_acc= 0.31362 time= 10.08134\n",
      "Epoch: 0300 train_loss= 8.87769 train_acc= 0.23492 val_loss= 14.31733 val_acc= 0.31620 time= 10.20660\n",
      "Epoch: 0301 train_loss= 8.31536 train_acc= 0.24262 val_loss= 14.24724 val_acc= 0.32262 time= 10.08162\n",
      "Epoch: 0302 train_loss= 8.59209 train_acc= 0.23235 val_loss= 14.17895 val_acc= 0.33290 time= 10.10706\n",
      "Epoch: 0303 train_loss= 6.81804 train_acc= 0.21951 val_loss= 14.12463 val_acc= 0.34319 time= 10.06822\n",
      "Epoch: 0304 train_loss= 10.63665 train_acc= 0.23107 val_loss= 14.07776 val_acc= 0.34961 time= 10.01356\n",
      "Epoch: 0305 train_loss= 7.29729 train_acc= 0.24262 val_loss= 14.03937 val_acc= 0.35219 time= 10.08650\n",
      "Epoch: 0306 train_loss= 9.12944 train_acc= 0.24005 val_loss= 13.99664 val_acc= 0.35604 time= 10.07122\n",
      "Epoch: 0307 train_loss= 7.63810 train_acc= 0.24005 val_loss= 13.96362 val_acc= 0.35990 time= 10.20601\n",
      "Epoch: 0308 train_loss= 7.67427 train_acc= 0.22850 val_loss= 13.93329 val_acc= 0.35733 time= 10.02130\n",
      "Epoch: 0309 train_loss= 8.72642 train_acc= 0.21438 val_loss= 13.90866 val_acc= 0.35347 time= 10.00663\n",
      "Epoch: 0310 train_loss= 6.70390 train_acc= 0.25802 val_loss= 13.88190 val_acc= 0.35090 time= 10.03818\n",
      "Epoch: 0311 train_loss= 9.64160 train_acc= 0.23107 val_loss= 13.85544 val_acc= 0.34961 time= 10.05552\n",
      "Epoch: 0312 train_loss= 5.79224 train_acc= 0.22336 val_loss= 13.83453 val_acc= 0.34319 time= 10.04127\n",
      "Epoch: 0313 train_loss= 8.72936 train_acc= 0.23620 val_loss= 13.80194 val_acc= 0.33933 time= 10.15759\n",
      "Epoch: 0314 train_loss= 7.61589 train_acc= 0.23748 val_loss= 13.77172 val_acc= 0.33676 time= 10.05967\n",
      "Epoch: 0315 train_loss= 8.02522 train_acc= 0.24519 val_loss= 13.73997 val_acc= 0.33676 time= 10.03921\n",
      "Epoch: 0316 train_loss= 6.65438 train_acc= 0.24134 val_loss= 13.71743 val_acc= 0.33290 time= 10.06167\n",
      "Epoch: 0317 train_loss= 9.31952 train_acc= 0.23235 val_loss= 13.68520 val_acc= 0.32905 time= 10.06930\n",
      "Epoch: 0318 train_loss= 6.51281 train_acc= 0.22721 val_loss= 13.65164 val_acc= 0.32776 time= 10.05210\n",
      "Epoch: 0319 train_loss= 7.03030 train_acc= 0.24647 val_loss= 13.61412 val_acc= 0.32776 time= 10.16461\n",
      "Epoch: 0320 train_loss= 8.37927 train_acc= 0.25674 val_loss= 13.57012 val_acc= 0.33162 time= 10.06447\n",
      "Epoch: 0321 train_loss= 7.04790 train_acc= 0.23748 val_loss= 13.52330 val_acc= 0.32905 time= 10.03698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0322 train_loss= 6.17212 train_acc= 0.21951 val_loss= 13.49052 val_acc= 0.32905 time= 11.03098\n",
      "Epoch: 0323 train_loss= 7.59732 train_acc= 0.22080 val_loss= 13.46160 val_acc= 0.33033 time= 10.10971\n",
      "Epoch: 0324 train_loss= 5.29685 train_acc= 0.25417 val_loss= 13.43024 val_acc= 0.32905 time= 10.03883\n",
      "Epoch: 0325 train_loss= 6.13692 train_acc= 0.26059 val_loss= 13.39267 val_acc= 0.33162 time= 10.15017\n",
      "Epoch: 0326 train_loss= 8.13469 train_acc= 0.23235 val_loss= 13.37054 val_acc= 0.33290 time= 10.13595\n",
      "Epoch: 0327 train_loss= 6.43751 train_acc= 0.22208 val_loss= 13.34554 val_acc= 0.33290 time= 10.05798\n",
      "Epoch: 0328 train_loss= 6.70601 train_acc= 0.23363 val_loss= 13.32617 val_acc= 0.33162 time= 10.02800\n",
      "Epoch: 0329 train_loss= 7.49256 train_acc= 0.22850 val_loss= 13.30678 val_acc= 0.33290 time= 10.05106\n",
      "Epoch: 0330 train_loss= 7.13865 train_acc= 0.24134 val_loss= 13.27764 val_acc= 0.33290 time= 10.10111\n",
      "Epoch: 0331 train_loss= 5.86257 train_acc= 0.21181 val_loss= 13.25172 val_acc= 0.33676 time= 10.13007\n",
      "Epoch: 0332 train_loss= 4.76577 train_acc= 0.22978 val_loss= 13.22613 val_acc= 0.34319 time= 10.06900\n",
      "Epoch: 0333 train_loss= 8.59277 train_acc= 0.24262 val_loss= 13.19324 val_acc= 0.34447 time= 10.06102\n",
      "Epoch: 0334 train_loss= 5.31630 train_acc= 0.24005 val_loss= 13.16349 val_acc= 0.34704 time= 10.00980\n",
      "Epoch: 0335 train_loss= 7.30576 train_acc= 0.25674 val_loss= 13.11940 val_acc= 0.35219 time= 10.02354\n",
      "Epoch: 0336 train_loss= 6.15770 train_acc= 0.24904 val_loss= 13.07887 val_acc= 0.35861 time= 10.01355\n",
      "Epoch: 0337 train_loss= 7.62687 train_acc= 0.24390 val_loss= 13.04369 val_acc= 0.36247 time= 10.05037\n",
      "Epoch: 0338 train_loss= 7.38149 train_acc= 0.23492 val_loss= 13.02072 val_acc= 0.36761 time= 10.11332\n",
      "Epoch: 0339 train_loss= 9.33733 train_acc= 0.25546 val_loss= 12.99618 val_acc= 0.37532 time= 10.04066\n",
      "Epoch: 0340 train_loss= 5.50662 train_acc= 0.24390 val_loss= 12.97874 val_acc= 0.38432 time= 10.01500\n",
      "Epoch: 0341 train_loss= 7.83959 train_acc= 0.19384 val_loss= 12.96435 val_acc= 0.38946 time= 10.04895\n",
      "Epoch: 0342 train_loss= 6.12621 train_acc= 0.22080 val_loss= 12.95227 val_acc= 0.39974 time= 10.12262\n",
      "Epoch: 0343 train_loss= 7.55939 train_acc= 0.22721 val_loss= 12.93118 val_acc= 0.40874 time= 10.08062\n",
      "Epoch: 0344 train_loss= 6.11000 train_acc= 0.23492 val_loss= 12.90671 val_acc= 0.41388 time= 10.13862\n",
      "Epoch: 0345 train_loss= 6.27287 train_acc= 0.21951 val_loss= 12.88316 val_acc= 0.42031 time= 10.07596\n",
      "Epoch: 0346 train_loss= 5.69686 train_acc= 0.24519 val_loss= 12.85906 val_acc= 0.41902 time= 10.02523\n",
      "Epoch: 0347 train_loss= 6.73533 train_acc= 0.24262 val_loss= 12.83298 val_acc= 0.41902 time= 10.01428\n",
      "Epoch: 0348 train_loss= 6.54876 train_acc= 0.23620 val_loss= 12.80348 val_acc= 0.41774 time= 10.03934\n",
      "Epoch: 0349 train_loss= 6.15798 train_acc= 0.23363 val_loss= 12.77608 val_acc= 0.41131 time= 10.04705\n",
      "Epoch: 0350 train_loss= 5.93362 train_acc= 0.23877 val_loss= 12.75424 val_acc= 0.40103 time= 10.20159\n",
      "Epoch: 0351 train_loss= 10.25350 train_acc= 0.24519 val_loss= 12.73658 val_acc= 0.39332 time= 10.07419\n",
      "Epoch: 0352 train_loss= 5.26297 train_acc= 0.25417 val_loss= 12.71840 val_acc= 0.38560 time= 10.68464\n",
      "Epoch: 0353 train_loss= 7.83028 train_acc= 0.22850 val_loss= 12.69568 val_acc= 0.37532 time= 10.08068\n",
      "Epoch: 0354 train_loss= 6.53375 train_acc= 0.24775 val_loss= 12.66910 val_acc= 0.37018 time= 10.07474\n",
      "Epoch: 0355 train_loss= 4.70359 train_acc= 0.24519 val_loss= 12.64119 val_acc= 0.36761 time= 10.08526\n",
      "Epoch: 0356 train_loss= 5.37650 train_acc= 0.21823 val_loss= 12.60859 val_acc= 0.36247 time= 10.14638\n",
      "Epoch: 0357 train_loss= 6.14752 train_acc= 0.23363 val_loss= 12.57464 val_acc= 0.35990 time= 10.06004\n",
      "Epoch: 0358 train_loss= 5.28620 train_acc= 0.21823 val_loss= 12.53472 val_acc= 0.35733 time= 10.05877\n",
      "Epoch: 0359 train_loss= 6.31301 train_acc= 0.20668 val_loss= 12.49720 val_acc= 0.35219 time= 10.02614\n",
      "Epoch: 0360 train_loss= 6.99781 train_acc= 0.24134 val_loss= 12.46728 val_acc= 0.34576 time= 10.04429\n",
      "Epoch: 0361 train_loss= 6.38862 train_acc= 0.25931 val_loss= 12.44222 val_acc= 0.34190 time= 10.08250\n",
      "Epoch: 0362 train_loss= 5.75431 train_acc= 0.17458 val_loss= 12.42024 val_acc= 0.34062 time= 10.05362\n",
      "Epoch: 0363 train_loss= 4.64697 train_acc= 0.24005 val_loss= 12.40211 val_acc= 0.33933 time= 10.15040\n",
      "Epoch: 0364 train_loss= 5.65860 train_acc= 0.22721 val_loss= 12.38809 val_acc= 0.33805 time= 10.03564\n",
      "Epoch: 0365 train_loss= 5.53342 train_acc= 0.22721 val_loss= 12.36597 val_acc= 0.33676 time= 10.03891\n",
      "Epoch: 0366 train_loss= 5.09516 train_acc= 0.24262 val_loss= 12.34925 val_acc= 0.33548 time= 10.07848\n",
      "Epoch: 0367 train_loss= 6.05375 train_acc= 0.27856 val_loss= 12.33470 val_acc= 0.33033 time= 10.06562\n",
      "Epoch: 0368 train_loss= 6.60725 train_acc= 0.22850 val_loss= 12.31707 val_acc= 0.32519 time= 10.03305\n",
      "Epoch: 0369 train_loss= 8.23994 train_acc= 0.23748 val_loss= 12.30003 val_acc= 0.32262 time= 10.09431\n",
      "Epoch: 0370 train_loss= 5.23756 train_acc= 0.24262 val_loss= 12.27527 val_acc= 0.31877 time= 10.08285\n",
      "Epoch: 0371 train_loss= 5.26519 train_acc= 0.23235 val_loss= 12.24758 val_acc= 0.32005 time= 10.04155\n",
      "Epoch: 0372 train_loss= 5.52103 train_acc= 0.25931 val_loss= 12.21421 val_acc= 0.31877 time= 10.01572\n",
      "Epoch: 0373 train_loss= 7.87217 train_acc= 0.22336 val_loss= 12.18323 val_acc= 0.31748 time= 10.06985\n",
      "Epoch: 0374 train_loss= 4.45279 train_acc= 0.19897 val_loss= 12.15304 val_acc= 0.31748 time= 10.10390\n",
      "Epoch: 0375 train_loss= 6.80766 train_acc= 0.23748 val_loss= 12.12440 val_acc= 0.31491 time= 10.15326\n",
      "Epoch: 0376 train_loss= 6.85412 train_acc= 0.22336 val_loss= 12.09409 val_acc= 0.31105 time= 10.08571\n",
      "Epoch: 0377 train_loss= 6.21572 train_acc= 0.24005 val_loss= 12.06138 val_acc= 0.30977 time= 10.07689\n",
      "Epoch: 0378 train_loss= 6.33764 train_acc= 0.24519 val_loss= 12.02737 val_acc= 0.30977 time= 10.17740\n",
      "Epoch: 0379 train_loss= 6.60135 train_acc= 0.25546 val_loss= 11.98948 val_acc= 0.30463 time= 10.17148\n",
      "Epoch: 0380 train_loss= 6.08154 train_acc= 0.24775 val_loss= 11.95996 val_acc= 0.29820 time= 10.07545\n",
      "Epoch: 0381 train_loss= 5.78411 train_acc= 0.22080 val_loss= 11.92806 val_acc= 0.29692 time= 10.17656\n",
      "Epoch: 0382 train_loss= 5.60609 train_acc= 0.24134 val_loss= 11.89704 val_acc= 0.29563 time= 10.82941\n",
      "Epoch: 0383 train_loss= 5.40311 train_acc= 0.22721 val_loss= 11.86949 val_acc= 0.29692 time= 10.07154\n",
      "Epoch: 0384 train_loss= 5.54492 train_acc= 0.20924 val_loss= 11.84149 val_acc= 0.29820 time= 10.02525\n",
      "Epoch: 0385 train_loss= 5.93247 train_acc= 0.22080 val_loss= 11.80776 val_acc= 0.29820 time= 10.03277\n",
      "Epoch: 0386 train_loss= 4.78726 train_acc= 0.22721 val_loss= 11.77055 val_acc= 0.30206 time= 10.04456\n",
      "Epoch: 0387 train_loss= 4.58073 train_acc= 0.23877 val_loss= 11.73419 val_acc= 0.30591 time= 10.01687\n",
      "Epoch: 0388 train_loss= 6.67394 train_acc= 0.24647 val_loss= 11.70651 val_acc= 0.31105 time= 10.13074\n",
      "Epoch: 0389 train_loss= 4.35757 train_acc= 0.25032 val_loss= 11.67863 val_acc= 0.31234 time= 10.07435\n",
      "Epoch: 0390 train_loss= 7.44851 train_acc= 0.23748 val_loss= 11.64963 val_acc= 0.31234 time= 10.08917\n",
      "Epoch: 0391 train_loss= 5.52130 train_acc= 0.23877 val_loss= 11.62424 val_acc= 0.31234 time= 10.07476\n",
      "Epoch: 0392 train_loss= 6.43377 train_acc= 0.21566 val_loss= 11.60449 val_acc= 0.31234 time= 10.03567\n",
      "Epoch: 0393 train_loss= 5.31171 train_acc= 0.23748 val_loss= 11.58132 val_acc= 0.31620 time= 10.04649\n",
      "Epoch: 0394 train_loss= 5.46262 train_acc= 0.23235 val_loss= 11.56491 val_acc= 0.31620 time= 10.14691\n",
      "Epoch: 0395 train_loss= 5.55538 train_acc= 0.23492 val_loss= 11.54854 val_acc= 0.31362 time= 10.01801\n",
      "Epoch: 0396 train_loss= 6.19145 train_acc= 0.23877 val_loss= 11.53948 val_acc= 0.31362 time= 10.07904\n",
      "Epoch: 0397 train_loss= 6.41035 train_acc= 0.22721 val_loss= 11.53350 val_acc= 0.31234 time= 10.12864\n",
      "Epoch: 0398 train_loss= 6.35650 train_acc= 0.23363 val_loss= 11.52561 val_acc= 0.30848 time= 10.03840\n",
      "Epoch: 0399 train_loss= 6.40346 train_acc= 0.26701 val_loss= 11.51745 val_acc= 0.30720 time= 10.04622\n",
      "Epoch: 0400 train_loss= 4.13644 train_acc= 0.23492 val_loss= 11.49677 val_acc= 0.30591 time= 10.14069\n",
      "Epoch: 0401 train_loss= 5.40341 train_acc= 0.27086 val_loss= 11.47291 val_acc= 0.30591 time= 10.13087\n",
      "Epoch: 0402 train_loss= 6.15724 train_acc= 0.23620 val_loss= 11.44371 val_acc= 0.30848 time= 10.07968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0403 train_loss= 6.01022 train_acc= 0.22336 val_loss= 11.41377 val_acc= 0.30848 time= 10.07595\n",
      "Epoch: 0404 train_loss= 4.85069 train_acc= 0.22721 val_loss= 11.38272 val_acc= 0.30848 time= 10.06347\n",
      "Epoch: 0405 train_loss= 4.20779 train_acc= 0.22850 val_loss= 11.35916 val_acc= 0.30977 time= 10.05415\n",
      "Epoch: 0406 train_loss= 6.03339 train_acc= 0.21181 val_loss= 11.32777 val_acc= 0.31105 time= 10.18140\n",
      "Epoch: 0407 train_loss= 5.40622 train_acc= 0.23363 val_loss= 11.29851 val_acc= 0.31234 time= 10.04509\n",
      "Epoch: 0408 train_loss= 6.66147 train_acc= 0.22593 val_loss= 11.26594 val_acc= 0.31491 time= 10.03931\n",
      "Epoch: 0409 train_loss= 5.67104 train_acc= 0.23363 val_loss= 11.23388 val_acc= 0.31748 time= 10.08884\n",
      "Epoch: 0410 train_loss= 5.56684 train_acc= 0.26573 val_loss= 11.20254 val_acc= 0.32134 time= 10.03471\n",
      "Epoch: 0411 train_loss= 5.40521 train_acc= 0.24134 val_loss= 11.17568 val_acc= 0.32134 time= 10.14120\n",
      "Epoch: 0412 train_loss= 6.21449 train_acc= 0.23363 val_loss= 11.14515 val_acc= 0.32391 time= 10.71662\n",
      "Epoch: 0413 train_loss= 5.51210 train_acc= 0.22593 val_loss= 11.11550 val_acc= 0.32648 time= 10.16297\n",
      "Epoch: 0414 train_loss= 4.22623 train_acc= 0.23363 val_loss= 11.09374 val_acc= 0.32648 time= 10.10323\n",
      "Epoch: 0415 train_loss= 5.41559 train_acc= 0.21694 val_loss= 11.07019 val_acc= 0.32262 time= 10.12054\n",
      "Epoch: 0416 train_loss= 5.60211 train_acc= 0.23492 val_loss= 11.05108 val_acc= 0.32262 time= 10.17364\n",
      "Epoch: 0417 train_loss= 4.97578 train_acc= 0.24647 val_loss= 11.03159 val_acc= 0.32648 time= 10.09435\n",
      "Epoch: 0418 train_loss= 4.86875 train_acc= 0.21053 val_loss= 11.01812 val_acc= 0.32648 time= 10.02360\n",
      "Epoch: 0419 train_loss= 4.35829 train_acc= 0.25289 val_loss= 11.00655 val_acc= 0.32648 time= 10.18958\n",
      "Epoch: 0420 train_loss= 5.32315 train_acc= 0.23235 val_loss= 10.99037 val_acc= 0.32391 time= 10.03345\n",
      "Epoch: 0421 train_loss= 4.52975 train_acc= 0.24390 val_loss= 10.97391 val_acc= 0.32005 time= 10.06912\n",
      "Epoch: 0422 train_loss= 4.90689 train_acc= 0.20668 val_loss= 10.95658 val_acc= 0.32134 time= 10.03176\n",
      "Epoch: 0423 train_loss= 5.28684 train_acc= 0.20668 val_loss= 10.93844 val_acc= 0.32134 time= 10.02293\n",
      "Epoch: 0424 train_loss= 4.28500 train_acc= 0.23748 val_loss= 10.92389 val_acc= 0.32134 time= 10.03923\n",
      "Epoch: 0425 train_loss= 7.06228 train_acc= 0.21181 val_loss= 10.91173 val_acc= 0.32391 time= 10.14626\n",
      "Epoch: 0426 train_loss= 5.98583 train_acc= 0.27471 val_loss= 10.90215 val_acc= 0.32519 time= 10.03405\n",
      "Epoch: 0427 train_loss= 6.96823 train_acc= 0.21823 val_loss= 10.88726 val_acc= 0.32519 time= 10.03614\n",
      "Epoch: 0428 train_loss= 5.10565 train_acc= 0.24647 val_loss= 10.86943 val_acc= 0.32519 time= 10.02481\n",
      "Epoch: 0429 train_loss= 5.90258 train_acc= 0.22721 val_loss= 10.85613 val_acc= 0.32648 time= 10.04821\n",
      "Epoch: 0430 train_loss= 5.51188 train_acc= 0.22978 val_loss= 10.84422 val_acc= 0.32776 time= 10.07859\n",
      "Epoch: 0431 train_loss= 4.25455 train_acc= 0.24390 val_loss= 10.83308 val_acc= 0.33419 time= 10.26779\n",
      "Epoch: 0432 train_loss= 4.20304 train_acc= 0.23492 val_loss= 10.82005 val_acc= 0.33933 time= 10.03961\n",
      "Epoch: 0433 train_loss= 4.64272 train_acc= 0.21823 val_loss= 10.80641 val_acc= 0.34190 time= 10.01386\n",
      "Epoch: 0434 train_loss= 5.37891 train_acc= 0.20668 val_loss= 10.79483 val_acc= 0.34319 time= 10.01307\n",
      "Epoch: 0435 train_loss= 5.47572 train_acc= 0.23748 val_loss= 10.77320 val_acc= 0.34704 time= 10.02247\n",
      "Epoch: 0436 train_loss= 5.20293 train_acc= 0.26958 val_loss= 10.75387 val_acc= 0.34961 time= 10.03415\n",
      "Epoch: 0437 train_loss= 4.83337 train_acc= 0.20924 val_loss= 10.73511 val_acc= 0.34961 time= 10.08298\n",
      "Epoch: 0438 train_loss= 4.01449 train_acc= 0.23363 val_loss= 10.72576 val_acc= 0.35090 time= 10.20504\n",
      "Epoch: 0439 train_loss= 4.12927 train_acc= 0.24390 val_loss= 10.71739 val_acc= 0.36118 time= 10.02158\n",
      "Epoch: 0440 train_loss= 4.68428 train_acc= 0.23877 val_loss= 10.71317 val_acc= 0.36761 time= 10.03032\n",
      "Epoch: 0441 train_loss= 4.73011 train_acc= 0.25546 val_loss= 10.70335 val_acc= 0.37147 time= 10.67602\n",
      "Epoch: 0442 train_loss= 4.21337 train_acc= 0.22978 val_loss= 10.69097 val_acc= 0.37532 time= 10.31139\n",
      "Epoch: 0443 train_loss= 4.85876 train_acc= 0.24519 val_loss= 10.67989 val_acc= 0.37918 time= 10.04622\n",
      "Epoch: 0444 train_loss= 5.11424 train_acc= 0.22721 val_loss= 10.67112 val_acc= 0.39075 time= 10.12170\n",
      "Epoch: 0445 train_loss= 5.38102 train_acc= 0.25674 val_loss= 10.66769 val_acc= 0.39589 time= 10.08643\n",
      "Epoch: 0446 train_loss= 5.91957 train_acc= 0.22465 val_loss= 10.66275 val_acc= 0.40360 time= 10.05167\n",
      "Epoch: 0447 train_loss= 3.43184 train_acc= 0.22208 val_loss= 10.65944 val_acc= 0.40360 time= 10.06516\n",
      "Epoch: 0448 train_loss= 4.22411 train_acc= 0.21823 val_loss= 10.65550 val_acc= 0.40360 time= 10.03467\n",
      "Epoch: 0449 train_loss= 5.40874 train_acc= 0.22336 val_loss= 10.65182 val_acc= 0.40231 time= 10.08740\n",
      "Epoch: 0450 train_loss= 4.69964 train_acc= 0.24134 val_loss= 10.65149 val_acc= 0.39846 time= 10.16027\n",
      "Epoch: 0451 train_loss= 4.55160 train_acc= 0.26187 val_loss= 10.64834 val_acc= 0.39460 time= 10.04566\n",
      "Epoch: 0452 train_loss= 4.45751 train_acc= 0.23620 val_loss= 10.64540 val_acc= 0.38560 time= 10.02426\n",
      "Epoch: 0453 train_loss= 3.93581 train_acc= 0.26573 val_loss= 10.64315 val_acc= 0.37918 time= 10.04857\n",
      "Epoch: 0454 train_loss= 6.44636 train_acc= 0.22465 val_loss= 10.64132 val_acc= 0.36889 time= 10.02756\n",
      "Epoch: 0455 train_loss= 4.27213 train_acc= 0.25417 val_loss= 10.63999 val_acc= 0.35990 time= 10.03652\n",
      "Epoch: 0456 train_loss= 5.40503 train_acc= 0.24519 val_loss= 10.62614 val_acc= 0.34704 time= 10.11477\n",
      "Epoch: 0457 train_loss= 4.46516 train_acc= 0.23620 val_loss= 10.61532 val_acc= 0.33676 time= 10.03811\n",
      "Epoch: 0458 train_loss= 3.55256 train_acc= 0.24262 val_loss= 10.60810 val_acc= 0.33290 time= 10.02182\n",
      "Epoch: 0459 train_loss= 4.62571 train_acc= 0.24262 val_loss= 10.60653 val_acc= 0.32519 time= 10.01648\n",
      "Epoch: 0460 train_loss= 4.27098 train_acc= 0.24390 val_loss= 10.60611 val_acc= 0.32134 time= 10.03828\n",
      "Epoch: 0461 train_loss= 6.15690 train_acc= 0.24904 val_loss= 10.60454 val_acc= 0.31877 time= 10.12062\n",
      "Epoch: 0462 train_loss= 4.19139 train_acc= 0.23107 val_loss= 10.60058 val_acc= 0.31748 time= 10.12368\n",
      "Epoch: 0463 train_loss= 3.93412 train_acc= 0.24390 val_loss= 10.60742 val_acc= 0.31748 time= 10.07294\n",
      "Epoch: 0464 train_loss= 4.14746 train_acc= 0.20924 val_loss= 10.60690 val_acc= 0.31620 time= 10.04919\n",
      "Epoch: 0465 train_loss= 3.96203 train_acc= 0.23620 val_loss= 10.60049 val_acc= 0.31491 time= 10.05117\n",
      "Epoch: 0466 train_loss= 4.82175 train_acc= 0.25160 val_loss= 10.59405 val_acc= 0.31234 time= 10.03307\n",
      "Epoch: 0467 train_loss= 3.68897 train_acc= 0.23877 val_loss= 10.58979 val_acc= 0.30977 time= 10.12133\n",
      "Epoch: 0468 train_loss= 3.28694 train_acc= 0.22721 val_loss= 10.58392 val_acc= 0.30720 time= 10.06652\n",
      "Epoch: 0469 train_loss= 4.56549 train_acc= 0.25032 val_loss= 10.58588 val_acc= 0.30334 time= 10.20818\n",
      "Epoch: 0470 train_loss= 4.52509 train_acc= 0.25802 val_loss= 10.58456 val_acc= 0.30077 time= 10.09730\n",
      "Epoch: 0471 train_loss= 5.87045 train_acc= 0.22593 val_loss= 10.57997 val_acc= 0.29820 time= 10.72912\n",
      "Epoch: 0472 train_loss= 3.70348 train_acc= 0.23877 val_loss= 10.57308 val_acc= 0.29692 time= 10.06148\n",
      "Epoch: 0473 train_loss= 4.14334 train_acc= 0.23877 val_loss= 10.56600 val_acc= 0.29306 time= 10.10799\n",
      "Epoch: 0474 train_loss= 3.61564 train_acc= 0.23620 val_loss= 10.55983 val_acc= 0.29306 time= 10.06131\n",
      "Epoch: 0475 train_loss= 3.88154 train_acc= 0.22593 val_loss= 10.54970 val_acc= 0.29306 time= 10.14829\n",
      "Epoch: 0476 train_loss= 4.05148 train_acc= 0.23363 val_loss= 10.53117 val_acc= 0.29306 time= 10.05130\n",
      "Epoch: 0477 train_loss= 6.00998 train_acc= 0.23235 val_loss= 10.51075 val_acc= 0.29563 time= 10.73804\n",
      "Epoch: 0478 train_loss= 4.69290 train_acc= 0.22465 val_loss= 10.48179 val_acc= 0.29563 time= 10.05938\n",
      "Epoch: 0479 train_loss= 4.83597 train_acc= 0.22080 val_loss= 10.45366 val_acc= 0.30206 time= 10.02528\n",
      "Epoch: 0480 train_loss= 4.60865 train_acc= 0.24519 val_loss= 10.42578 val_acc= 0.30206 time= 10.04441\n",
      "Epoch: 0481 train_loss= 4.56736 train_acc= 0.24647 val_loss= 10.39826 val_acc= 0.30591 time= 10.14238\n",
      "Epoch: 0482 train_loss= 3.30902 train_acc= 0.23877 val_loss= 10.37228 val_acc= 0.31105 time= 10.20368\n",
      "Epoch: 0483 train_loss= 5.64714 train_acc= 0.24134 val_loss= 10.34711 val_acc= 0.31620 time= 10.56754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0484 train_loss= 3.84775 train_acc= 0.24775 val_loss= 10.32338 val_acc= 0.32005 time= 10.77939\n",
      "Epoch: 0485 train_loss= 4.40721 train_acc= 0.25417 val_loss= 10.30548 val_acc= 0.32648 time= 11.00741\n",
      "Epoch: 0486 train_loss= 3.64997 train_acc= 0.24390 val_loss= 10.28928 val_acc= 0.33033 time= 11.82871\n",
      "Epoch: 0487 train_loss= 4.14216 train_acc= 0.22593 val_loss= 10.27365 val_acc= 0.33290 time= 11.15288\n",
      "Epoch: 0488 train_loss= 4.58071 train_acc= 0.24904 val_loss= 10.25434 val_acc= 0.33548 time= 10.26356\n",
      "Epoch: 0489 train_loss= 3.87467 train_acc= 0.21053 val_loss= 10.23793 val_acc= 0.33548 time= 10.51335\n",
      "Epoch: 0490 train_loss= 4.23529 train_acc= 0.25417 val_loss= 10.21700 val_acc= 0.33548 time= 10.16824\n",
      "Epoch: 0491 train_loss= 4.78875 train_acc= 0.25802 val_loss= 10.19701 val_acc= 0.33548 time= 10.09219\n",
      "Epoch: 0492 train_loss= 4.67677 train_acc= 0.25160 val_loss= 10.16925 val_acc= 0.33548 time= 10.29424\n",
      "Epoch: 0493 train_loss= 3.94735 train_acc= 0.25546 val_loss= 10.14841 val_acc= 0.34447 time= 10.13864\n",
      "Epoch: 0494 train_loss= 4.11445 train_acc= 0.23877 val_loss= 10.13542 val_acc= 0.35347 time= 10.05969\n",
      "Epoch: 0495 train_loss= 4.43872 train_acc= 0.22593 val_loss= 10.12708 val_acc= 0.36247 time= 10.05989\n",
      "Epoch: 0496 train_loss= 3.42647 train_acc= 0.23877 val_loss= 10.11900 val_acc= 0.36761 time= 10.16523\n",
      "Epoch: 0497 train_loss= 3.53351 train_acc= 0.23492 val_loss= 10.11234 val_acc= 0.37018 time= 10.27010\n",
      "Epoch: 0498 train_loss= 5.05337 train_acc= 0.22850 val_loss= 10.10428 val_acc= 0.37147 time= 11.07734\n",
      "Epoch: 0499 train_loss= 4.14295 train_acc= 0.24005 val_loss= 10.09884 val_acc= 0.37404 time= 10.53184\n",
      "Epoch: 0500 train_loss= 3.22836 train_acc= 0.27343 val_loss= 10.09442 val_acc= 0.37275 time= 10.79511\n",
      "Epoch: 0501 train_loss= 3.76837 train_acc= 0.25546 val_loss= 10.08963 val_acc= 0.37275 time= 10.20842\n",
      "Epoch: 0502 train_loss= 3.84944 train_acc= 0.26059 val_loss= 10.08426 val_acc= 0.37275 time= 10.05620\n",
      "Epoch: 0503 train_loss= 5.30758 train_acc= 0.24775 val_loss= 10.07495 val_acc= 0.37275 time= 10.09187\n",
      "Epoch: 0504 train_loss= 4.27139 train_acc= 0.24390 val_loss= 10.06383 val_acc= 0.37275 time= 10.10638\n",
      "Epoch: 0505 train_loss= 4.87229 train_acc= 0.24647 val_loss= 10.05087 val_acc= 0.37147 time= 10.23768\n",
      "Epoch: 0506 train_loss= 4.16568 train_acc= 0.26059 val_loss= 10.03538 val_acc= 0.36247 time= 11.15880\n",
      "Epoch: 0507 train_loss= 3.64284 train_acc= 0.26573 val_loss= 10.01992 val_acc= 0.35219 time= 10.37403\n",
      "Epoch: 0508 train_loss= 3.16896 train_acc= 0.23363 val_loss= 10.00531 val_acc= 0.34833 time= 10.38646\n",
      "Epoch: 0509 train_loss= 3.04740 train_acc= 0.25802 val_loss= 9.99299 val_acc= 0.34447 time= 10.48899\n",
      "Epoch: 0510 train_loss= 4.55999 train_acc= 0.24904 val_loss= 9.98373 val_acc= 0.33676 time= 10.24086\n",
      "Epoch: 0511 train_loss= 3.93047 train_acc= 0.24134 val_loss= 9.97533 val_acc= 0.33676 time= 10.25573\n",
      "Epoch: 0512 train_loss= 3.48054 train_acc= 0.26829 val_loss= 9.96380 val_acc= 0.33676 time= 11.10463\n",
      "Epoch: 0513 train_loss= 4.05240 train_acc= 0.23235 val_loss= 9.95520 val_acc= 0.33676 time= 11.94828\n",
      "Epoch: 0514 train_loss= 3.57192 train_acc= 0.26573 val_loss= 9.94597 val_acc= 0.33676 time= 11.88558\n",
      "Epoch: 0515 train_loss= 3.52883 train_acc= 0.23107 val_loss= 9.94233 val_acc= 0.33676 time= 10.66294\n",
      "Epoch: 0516 train_loss= 3.74124 train_acc= 0.23620 val_loss= 9.93638 val_acc= 0.33676 time= 10.75460\n",
      "Epoch: 0517 train_loss= 3.22908 train_acc= 0.22593 val_loss= 9.93280 val_acc= 0.33676 time= 10.68127\n",
      "Epoch: 0518 train_loss= 3.05692 train_acc= 0.26059 val_loss= 9.92277 val_acc= 0.33805 time= 10.55858\n",
      "Epoch: 0519 train_loss= 4.00979 train_acc= 0.24134 val_loss= 9.91819 val_acc= 0.33805 time= 10.64940\n",
      "Epoch: 0520 train_loss= 3.13147 train_acc= 0.24519 val_loss= 9.91352 val_acc= 0.33676 time= 10.64439\n",
      "Epoch: 0521 train_loss= 3.08428 train_acc= 0.24390 val_loss= 9.90836 val_acc= 0.33676 time= 10.60708\n",
      "Epoch: 0522 train_loss= 2.97357 train_acc= 0.24390 val_loss= 9.90179 val_acc= 0.33676 time= 10.64815\n",
      "Epoch: 0523 train_loss= 4.63995 train_acc= 0.24904 val_loss= 9.89376 val_acc= 0.33548 time= 10.69927\n",
      "Epoch: 0524 train_loss= 4.11640 train_acc= 0.24262 val_loss= 9.88604 val_acc= 0.33162 time= 10.94739\n",
      "Epoch: 0525 train_loss= 3.84637 train_acc= 0.25546 val_loss= 9.87941 val_acc= 0.33419 time= 10.78827\n",
      "Epoch: 0526 train_loss= 3.75011 train_acc= 0.23235 val_loss= 9.87082 val_acc= 0.33033 time= 10.53722\n",
      "Epoch: 0527 train_loss= 3.53714 train_acc= 0.25289 val_loss= 9.86292 val_acc= 0.33033 time= 10.67333\n",
      "Epoch: 0528 train_loss= 3.91223 train_acc= 0.25160 val_loss= 9.86139 val_acc= 0.33033 time= 10.66597\n",
      "Epoch: 0529 train_loss= 3.46286 train_acc= 0.21566 val_loss= 9.85845 val_acc= 0.32134 time= 11.53005\n",
      "Epoch: 0530 train_loss= 3.29892 train_acc= 0.22978 val_loss= 9.85222 val_acc= 0.31491 time= 10.58538\n",
      "Epoch: 0531 train_loss= 4.32130 train_acc= 0.26958 val_loss= 9.85196 val_acc= 0.31234 time= 10.58812\n",
      "Epoch: 0532 train_loss= 3.34578 train_acc= 0.24262 val_loss= 9.84979 val_acc= 0.30463 time= 10.56710\n",
      "Epoch: 0533 train_loss= 3.93137 train_acc= 0.26187 val_loss= 9.84416 val_acc= 0.29820 time= 10.70524\n",
      "Epoch: 0534 train_loss= 4.55427 train_acc= 0.22465 val_loss= 9.83700 val_acc= 0.29049 time= 10.50682\n",
      "Epoch: 0535 train_loss= 4.00432 train_acc= 0.25417 val_loss= 9.82921 val_acc= 0.28663 time= 10.67872\n",
      "Epoch: 0536 train_loss= 3.57024 train_acc= 0.26059 val_loss= 9.82241 val_acc= 0.28406 time= 10.51512\n",
      "Epoch: 0537 train_loss= 3.42689 train_acc= 0.22336 val_loss= 9.81380 val_acc= 0.28535 time= 10.53860\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "cost_val = []\n",
    "\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, train_labels_mat, train_mask_mat[:,1], placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, val_mask_mat, val_mask_mat[:,1], placeholders, sess, model)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, test_labels_mat, test_mask_mat[:,1], placeholders, sess, model)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
