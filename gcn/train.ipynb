{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "from models import GCN, MLP\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "\n",
    "def load_data(dataset_str, max_sample):\n",
    "    data_path = '../data/'\n",
    "    files = [f for f in os.listdir(data_path) if f.startswith(dataset_str)]\n",
    "    files = np.random.choice(files,max_sample,False) \n",
    "    \n",
    "    adj_lst = list()\n",
    "    features_lst = list()\n",
    "    labels_lst = list()\n",
    "    n_nodes_lst = list()\n",
    "    \n",
    "    # load and concatenate all graph data\n",
    "    for i in range(files.size):\n",
    "        file = open(data_path + files[i],'rb')\n",
    "        \n",
    "        adj = pkl.load(file)\n",
    "        feature = pkl.load(file)\n",
    "        label = pkl.load(file)\n",
    "        n_nodes = [i+1]*feature.shape[0]\n",
    "\n",
    "        adj_lst.append(adj)\n",
    "        features_lst.append(feature)\n",
    "        labels_lst.append(label)\n",
    "        n_nodes_lst.append(n_nodes)\n",
    "        \n",
    "        file.close()\n",
    "    \n",
    "    # create labels matrix\n",
    "    labels_mat =  np.vstack([[labels_lst[i]]*len(n_nodes_lst[i]) for i in range(len(labels_lst))])\n",
    "    \n",
    "    # create feature matrix\n",
    "    features_mat =  np.vstack(features_lst)\n",
    "    features_mat = sp.csr_matrix(features_mat)\n",
    "    \n",
    "    # adj block matrix\n",
    "    nodes_before = 0\n",
    "    nodes_after = features_mat.shape[0] \n",
    "    for i in range(len(adj_lst)):\n",
    "        n_nodes = len(n_nodes_lst[i])\n",
    "        nodes_after -= n_nodes\n",
    "        if nodes_before>0:\n",
    "            left_mat = sp.hstack([np.zeros((n_nodes, nodes_before)),adj_lst[i]])\n",
    "        else :\n",
    "            left_mat = adj_lst[i]\n",
    "        if nodes_after>0:\n",
    "          \n",
    "            adj_lst[i] = sp.hstack([left_mat,np.zeros((n_nodes, nodes_after))])\n",
    "        else:\n",
    "            adj_lst[i] = left_mat\n",
    "            \n",
    "        nodes_before += n_nodes\n",
    "    adj_mat = sp.vstack(adj_lst)\n",
    "    \n",
    "    # prepare masks (train/val/test sets)\n",
    "    train_ratio = 0.8\n",
    "    val_ratio = 0.2\n",
    "    train_mask = np.zeros(len(files))\n",
    "    val_mask = np.zeros(len(files))\n",
    "    test_mask = np.zeros(len(files))\n",
    "    \n",
    "    train_mask_ind = np.random.choice(range(len(files)), int(len(files)*train_ratio), False)\n",
    "    val_mask_ind = np.random.choice(train_mask_ind, int(len(train_mask_ind)*val_ratio), False)\n",
    "    \n",
    "    test_mask_ind = set(range(len(files))).difference(set(train_mask_ind))\n",
    "    train_mask_ind = set(train_mask_ind).difference(set(val_mask_ind))\n",
    "    \n",
    "    for ind in train_mask_ind:\n",
    "        train_mask[ind] = 1\n",
    "    \n",
    "    for ind in val_mask_ind:\n",
    "        val_mask[ind] = 1\n",
    "    \n",
    "    for ind in test_mask_ind:\n",
    "        test_mask[ind] = 1\n",
    "    \n",
    "    test_mask_mat =  np.vstack([[[test_mask[i]]*5]*len(n_nodes_lst[i]) for i in range(len(test_mask))])\n",
    "    val_mask_mat =   np.vstack([[[val_mask[i]]*5]*len(n_nodes_lst[i]) for i in range(len(val_mask))])\n",
    "    train_mask_mat =  np.vstack([[[train_mask[i]]*5]*len(n_nodes_lst[i]) for i in range(len(train_mask))])\n",
    "    \n",
    "    # check if the result is correct\n",
    "    sum_check = test_mask_mat+val_mask_mat+train_mask_mat\n",
    "    if (np.max(sum_check)>1):\n",
    "        sys.exit()\n",
    "    elif (np.any(sum_check==0)):\n",
    "        sys.exit()\n",
    "    \n",
    "    # use mask for labels masking\n",
    "    train_labels_mat = np.multiply(labels_mat, train_mask_mat)\n",
    "    test_labels_mat = np.multiply(labels_mat, test_mask_mat)\n",
    "    val_labels_mat = np.multiply(labels_mat, val_mask_mat)\n",
    "    \n",
    "    # check if the result is correct\n",
    "    sum_check = train_labels_mat+test_labels_mat+val_labels_mat\n",
    "    if (np.sum(sum_check)!= test_labels_mat.shape[0]):\n",
    "        sys.exit()    \n",
    "    \n",
    "    return adj_mat, features_mat, train_labels_mat, test_labels_mat, val_labels_mat, train_mask_mat, test_mask_mat, val_mask_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get paths to available grahs metadata\n",
    "adj_mat, features_mat, train_labels_mat, test_labels_mat, val_labels_mat, train_mask_mat, test_mask_mat, val_mask_mat = load_data(\"txt_graph2216_21012020\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3923, 3923)\n",
      "(3923, 100)\n",
      "(3923, 5)\n",
      "(3923, 5)\n",
      "(3923, 5)\n",
      "(3923, 5)\n",
      "(3923, 5)\n",
      "(3923, 5)\n"
     ]
    }
   ],
   "source": [
    "print(adj_mat.shape)\n",
    "print(features_mat.shape)\n",
    "print(train_labels_mat.shape)\n",
    "print(test_labels_mat.shape)\n",
    "print(val_labels_mat.shape)\n",
    "print(train_mask_mat.shape)\n",
    "print(test_mask_mat.shape)\n",
    "print(val_mask_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'tocoo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-33774902612f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Some preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_adj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mnum_supports\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/GCN_text_classif/gcn/utils.py\u001b[0m in \u001b[0;36mpreprocess_features\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mr_inv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mr_mat_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_inv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_mat_inv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/GCN_text_classif/gcn/utils.py\u001b[0m in \u001b[0;36msparse_to_tuple\u001b[0;34m(sparse_mx)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0msparse_mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0msparse_mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/GCN_text_classif/gcn/utils.py\u001b[0m in \u001b[0;36mto_tuple\u001b[0;34m(mx)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m\"\"\"Convert sparse matrix to tuple representation.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix_coo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mmx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'tocoo'"
     ]
    }
   ],
   "source": [
    "#delete all flags before declaration new one\n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('mode', '', 'kernel') # for line by line mode\n",
    "tf.app.flags.DEFINE_string('port', '', 'kernel') # for line by line mode\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel') # for jupyter notebook\n",
    "\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features_mat)\n",
    "support = [preprocess_adj(adj_mat)]\n",
    "num_supports = 1\n",
    "model_func = GCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "cost_val = []\n",
    "\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
